title,date,paragraphId,text
Attention Is All You Need,2017-06-12,1e460845-beb6-47fb-805c-59bf66557b2b,"Provided proper attribution is provided, Google hereby grants permission to"
Attention Is All You Need,2017-06-12,467bc396-da9a-4a06-99f7-6256078b8cb8,reproduce the tables and figures in this paper solely for use in journalistic or
Attention Is All You Need,2017-06-12,b1ac7576-14fa-4821-aa86-679dd4f84fe7,scholarly works.
Attention Is All You Need,2017-06-12,a0a4f389-c0fa-4e68-a133-37cacc8949ee,Attention Is All You Need
Attention Is All You Need,2017-06-12,df95a974-900c-4de7-b923-61e3ca35990e,Ashish Vaswani∗
Attention Is All You Need,2017-06-12,eb0674c3-5e1b-42da-9e85-1e5369a03d3d,Google Brain
Attention Is All You Need,2017-06-12,8a74523f-75bc-4b53-b22d-9a43ced1ea0d,avaswani@google.comNoam Shazeer∗
Attention Is All You Need,2017-06-12,6e59f3fc-3c86-417e-a732-4989a8dcdf9f,Google Brain
Attention Is All You Need,2017-06-12,263b9e8a-8a4f-47e8-b807-b2c85833860a,noam@google.comNiki Parmar∗
Attention Is All You Need,2017-06-12,c369ba61-0db3-48b7-a912-eacfce22a174,Google Research
Attention Is All You Need,2017-06-12,f2699fe3-54f9-487a-8348-9921a9a8e56f,nikip@google.comJakob Uszkoreit∗
Attention Is All You Need,2017-06-12,6c4db0e7-774e-498a-aeaa-fb8fac467909,Google Research
Attention Is All You Need,2017-06-12,faaab18d-5961-4f28-80a4-b8f1445c47da,usz@google.com
Attention Is All You Need,2017-06-12,f44e70e3-6b9f-48c7-babf-43ec5d7bd99d,Llion Jones∗
Attention Is All You Need,2017-06-12,fde631c4-f71e-46eb-92b8-13ba128a700b,Google Research
Attention Is All You Need,2017-06-12,fe0f2e7c-9e92-4208-82a4-dbad0fe56e56,llion@google.comAidan N. Gomez∗ †
Attention Is All You Need,2017-06-12,8dcae615-73a6-4393-a6a7-9ab9a32e3cc4,University of Toronto
Attention Is All You Need,2017-06-12,3baf912b-ce49-4606-b51f-f5bdeab421b2,aidan@cs.toronto.eduŁukasz Kaiser∗
Attention Is All You Need,2017-06-12,d9a0dc82-78f4-4962-a9d0-2a5dd64a67bb,Google Brain
Attention Is All You Need,2017-06-12,65228814-3273-46f1-b648-04c0f7979636,lukaszkaiser@google.com
Attention Is All You Need,2017-06-12,79acd3fc-d1a3-4550-85b7-fa5b015ba614,Illia Polosukhin∗ ‡
Attention Is All You Need,2017-06-12,a23db0c5-3fa2-492f-a93d-024a5b2a9a76,illia.polosukhin@gmail.com
Attention Is All You Need,2017-06-12,3b57e400-cb11-4b2b-9a1e-96af0e0f198f,Abstract
Attention Is All You Need,2017-06-12,6eb3a3b4-652a-494e-a282-1e9a5d71e4e2,The dominant sequence transduction models are based on complex recurrent or
Attention Is All You Need,2017-06-12,37544ec3-f0cf-4b4a-8ce6-7870d6ff4dcc,convolutional neural networks that include an encoder and a decoder. The best
Attention Is All You Need,2017-06-12,c20db093-593d-4193-8be6-da056c044313,performing models also connect the encoder and decoder through an attention
Attention Is All You Need,2017-06-12,4e95a81e-04c4-4781-b442-8fdc5bc4e7f5,"mechanism. We propose a new simple network architecture, the Transformer,"
Attention Is All You Need,2017-06-12,db628a53-3f0c-425a-99dc-36b8d921250a,"based solely on attention mechanisms, dispensing with recurrence and convolutions"
Attention Is All You Need,2017-06-12,4d524b03-6454-4c2d-bbb5-22df9ffeaf52,entirely. Experiments on two machine translation tasks show these models to
Attention Is All You Need,2017-06-12,92c14c52-6bac-4078-abc4-0e862bea4134,be superior in quality while being more parallelizable and requiring significantly
Attention Is All You Need,2017-06-12,c348e42c-4979-4000-befd-07aff61baa71,less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
Attention Is All You Need,2017-06-12,358aafe5-72ab-4bce-a41b-52259805fd6e,"to-German translation task, improving over the existing best results, including"
Attention Is All You Need,2017-06-12,0b5d00ad-3c22-4947-8412-b37487c6b95e,"ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,"
Attention Is All You Need,2017-06-12,19e870cc-6237-4dc1-9035-6d1e85d7d7b0,our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
Attention Is All You Need,2017-06-12,b6b8eadb-a9d3-43f3-a901-c38a05ed03ff,"training for 3.5 days on eight GPUs, a small fraction of the training costs of the"
Attention Is All You Need,2017-06-12,dcfbac6e-8adb-41a2-a171-4e33c99617ba,best models from the literature. We show that the Transformer generalizes well to
Attention Is All You Need,2017-06-12,ec954bc5-c676-4636-95aa-43d3561bc9e1,other tasks by applying it successfully to English constituency parsing both with
Attention Is All You Need,2017-06-12,d88f1eea-ef3e-41b5-96fe-3233bb041ff1,large and limited training data.
Attention Is All You Need,2017-06-12,51ef8d44-f640-49e4-b4e6-919b8b704297,∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
Attention Is All You Need,2017-06-12,bfbd40b8-83f7-4cab-a308-545121385392,"the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and"
Attention Is All You Need,2017-06-12,4ecd6214-8f13-46d6-a16b-832e56dc1ff5,"has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head"
Attention Is All You Need,2017-06-12,3f488f01-d38e-4ef5-b57a-c4572608008e,attention and the parameter-free position representation and became the other person involved in nearly every
Attention Is All You Need,2017-06-12,84ece450-6bba-426b-a0c9-267c11c1e769,"detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and"
Attention Is All You Need,2017-06-12,74c0ddc5-6eb7-4b30-b884-6b0eaa66d9ed,"tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and"
Attention Is All You Need,2017-06-12,721fe49d-2824-4376-be64-566784f0a533,efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
Attention Is All You Need,2017-06-12,512a13f0-7531-4214-8dd6-5c5bfbe29a05,"implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating"
Attention Is All You Need,2017-06-12,b21a1a07-8145-45ba-a139-252bdc7de686,our research.
Attention Is All You Need,2017-06-12,e337eee6-9590-4ee8-b772-d511c98c3da6,†Work performed while at Google Brain.
Attention Is All You Need,2017-06-12,47d1ecbc-af8e-4f72-9a35-b9dcbc140ae0,‡Work performed while at Google Research.
Attention Is All You Need,2017-06-12,7d650f1d-ef0f-42c3-a11d-bea08cb5e8ea,"31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023"
Attention Is All You Need,2017-06-12,25ccf8ac-c005-49ea-b147-e3533bf1bdf9,1 Introduction
Attention Is All You Need,2017-06-12,8cb1c7e7-9264-434f-b744-f6848891c481,"Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks"
Attention Is All You Need,2017-06-12,1974e389-47b3-4f3a-b2c9-b74261f62494,"in particular, have been firmly established as state of the art approaches in sequence modeling and"
Attention Is All You Need,2017-06-12,cbc6dc08-6ba3-4a29-89b8-7e1ad8c9bee6,"transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous"
Attention Is All You Need,2017-06-12,d489b2de-a75f-438a-a135-d10904a60dd5,efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
Attention Is All You Need,2017-06-12,88d05871-bf09-4dc7-aefb-ff594e927e5b,"architectures [38, 24, 15]."
Attention Is All You Need,2017-06-12,e1d493af-84ff-431d-8608-1e6db24a3bb5,Recurrent models typically factor computation along the symbol positions of the input and output
Attention Is All You Need,2017-06-12,be4bf39e-753c-4262-b8a5-fe68b77d8edf,"sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden"
Attention Is All You Need,2017-06-12,1e01a1ad-a00c-47e7-8067-f6a0bb104831,"states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently"
Attention Is All You Need,2017-06-12,f5d69c9f-d18d-496e-b859-cef48e3a1a77,"sequential nature precludes parallelization within training examples, which becomes critical at longer"
Attention Is All You Need,2017-06-12,69af502e-34bd-4084-96a8-11a754fa0b35,"sequence lengths, as memory constraints limit batching across examples. Recent work has achieved"
Attention Is All You Need,2017-06-12,032e87d3-190e-4ce1-96f5-e00411a07987,significant improvements in computational efficiency through factorization tricks [ 21] and conditional
Attention Is All You Need,2017-06-12,40206f27-748e-4ed1-b0b0-2ab07ce8e4ff,"computation [ 32], while also improving model performance in case of the latter. The fundamental"
Attention Is All You Need,2017-06-12,50d1e307-4637-412e-9b76-90d4fc986336,"constraint of sequential computation, however, remains."
Attention Is All You Need,2017-06-12,5068cc3c-2afe-4d79-8434-d2555a10fad3,Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
Attention Is All You Need,2017-06-12,6ff51d4d-64b8-4014-a639-40005f3321b8,"tion models in various tasks, allowing modeling of dependencies without regard to their distance in"
Attention Is All You Need,2017-06-12,bac32cf4-3972-42c2-aa58-9bae048ebb1c,"the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms"
Attention Is All You Need,2017-06-12,7cdc7a47-4d31-4eba-b93f-5143c3ff9b27,are used in conjunction with a recurrent network.
Attention Is All You Need,2017-06-12,de5fa59a-a763-444d-885d-7a216dbad58c,"In this work we propose the Transformer, a model architecture eschewing recurrence and instead"
Attention Is All You Need,2017-06-12,36156c8d-2f44-44a8-8f2b-092492ad3748,relying entirely on an attention mechanism to draw global dependencies between input and output.
Attention Is All You Need,2017-06-12,e5210919-208e-4201-bb5e-b56791dec013,The Transformer allows for significantly more parallelization and can reach a new state of the art in
Attention Is All You Need,2017-06-12,430cc55b-1162-40ad-8bed-a15427ce34b9,translation quality after being trained for as little as twelve hours on eight P100 GPUs.
Attention Is All You Need,2017-06-12,1fe42531-e9fa-4f76-a3eb-0f72b4524aa7,2 Background
Attention Is All You Need,2017-06-12,e6443fcc-8f01-4e16-ab90-7660008feff5,The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
Attention Is All You Need,2017-06-12,f7d84e8f-c58d-47b6-a5cf-f2c59e46d8bc,"[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building"
Attention Is All You Need,2017-06-12,ae62993b-0a06-4242-a9a0-ec3171fdefcf,"block, computing hidden representations in parallel for all input and output positions. In these models,"
Attention Is All You Need,2017-06-12,82a7d2d6-f969-4a1f-94af-7b6a15b9345a,the number of operations required to relate signals from two arbitrary input or output positions grows
Attention Is All You Need,2017-06-12,9f6131e7-228e-4f84-b49a-6c06b0962bfc,"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes"
Attention Is All You Need,2017-06-12,0413eb04-13bb-41ba-aff2-9e4558df731d,it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is
Attention Is All You Need,2017-06-12,eca947c4-5b29-4e81-a690-862433ea771d,"reduced to a constant number of operations, albeit at the cost of reduced effective resolution due"
Attention Is All You Need,2017-06-12,055b5384-3218-4e11-b817-40b2d566f83a,"to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as"
Attention Is All You Need,2017-06-12,92bf1ec3-3bd5-4bd8-a9bd-0b62a228ab9b,described in section 3.2.
Attention Is All You Need,2017-06-12,d4938b91-24a7-4b59-bcca-74091de0b7d5,"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions"
Attention Is All You Need,2017-06-12,54ad7fcc-021f-416c-ac4d-389914c72680,of a single sequence in order to compute a representation of the sequence. Self-attention has been
Attention Is All You Need,2017-06-12,7996d5fd-832c-42cc-9c28-6e09b0a351e7,"used successfully in a variety of tasks including reading comprehension, abstractive summarization,"
Attention Is All You Need,2017-06-12,50f221be-0b94-4c5d-ada9-61ace6035278,"textual entailment and learning task-independent sentence representations [4, 27, 28, 22]."
Attention Is All You Need,2017-06-12,00c1f2f6-18b3-4194-b8a3-582d95999a9a,End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
Attention Is All You Need,2017-06-12,96236f7c-b7b8-43c6-8eec-c63ca5000a8e,aligned recurrence and have been shown to perform well on simple-language question answering and
Attention Is All You Need,2017-06-12,1fc3fda5-8d19-46d8-baae-6c984d32b8cd,language modeling tasks [34].
Attention Is All You Need,2017-06-12,191182e1-c4a8-4e0e-b115-a517406ecdc0,"To the best of our knowledge, however, the Transformer is the first transduction model relying"
Attention Is All You Need,2017-06-12,72d5f6be-12df-47dd-9978-4bd48e72034c,entirely on self-attention to compute representations of its input and output without using sequence-
Attention Is All You Need,2017-06-12,77fb5955-49fe-47bd-a2a7-3216e335c625,"aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate"
Attention Is All You Need,2017-06-12,fe1fb18f-611f-47b3-ae10-ae2340c2455a,"self-attention and discuss its advantages over models such as [17, 18] and [9]."
Attention Is All You Need,2017-06-12,45b4d8e8-556f-4d5c-bdb2-6e340af6c4d9,3 Model Architecture
Attention Is All You Need,2017-06-12,3c276434-2d7f-45f4-b0ff-bce9473a191c,"Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]."
Attention Is All You Need,2017-06-12,3fb57258-29b7-4012-a52b-877ac24bec9e,"Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence"
Attention Is All You Need,2017-06-12,c971cda8-7df0-4a48-97e5-bab5736904f6,"of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output"
Attention Is All You Need,2017-06-12,ef967faf-4663-4488-89c7-0d1ed02fa6b6,"sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive"
Attention Is All You Need,2017-06-12,78a35885-962c-4da0-9635-856c95f2f3c8,"[10], consuming the previously generated symbols as additional input when generating the next."
Attention Is All You Need,2017-06-12,aae671a2-92b8-42bc-b00a-2b1759360da6,2
Attention Is All You Need,2017-06-12,188d97db-670e-4481-ae76-0a6a88835db9,Figure 1: The Transformer - model architecture.
Attention Is All You Need,2017-06-12,80c3a5c6-6496-44a5-bea3-d0d5b508779f,"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully"
Attention Is All You Need,2017-06-12,7864cf4b-3681-4388-8afc-55c324fd5e3d,"connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,"
Attention Is All You Need,2017-06-12,576a82a3-d298-4261-b502-75ed52e19051,respectively.
Attention Is All You Need,2017-06-12,eb035cb5-a1b3-4a49-84b7-77c4503ef3af,3.1 Encoder and Decoder Stacks
Attention Is All You Need,2017-06-12,3fa006af-149c-4244-b0ff-5f30bf654892,Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two
Attention Is All You Need,2017-06-12,37154177-4e64-46ec-88c2-47fc93babd53,"sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-"
Attention Is All You Need,2017-06-12,ecbda388-92b5-479a-bbbd-5430a5f79f27,wise fully connected feed-forward network. We employ a residual connection [ 11] around each of
Attention Is All You Need,2017-06-12,7979a227-7f5b-408b-878a-c2abc2b5adde,"the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is"
Attention Is All You Need,2017-06-12,f6541373-653d-47e8-9971-f26276a673b5,"LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer"
Attention Is All You Need,2017-06-12,a10c402f-86d8-4c56-81ee-2334650e08a0,"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding"
Attention Is All You Need,2017-06-12,82e1f448-da43-4095-93da-38d80063d35d,"layers, produce outputs of dimension dmodel = 512 ."
Attention Is All You Need,2017-06-12,51528cdf-ce31-4b48-9126-a9af6f2d9582,Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two
Attention Is All You Need,2017-06-12,ac057db0-a9a7-4502-aa1d-7dbaf87559a7,"sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head"
Attention Is All You Need,2017-06-12,c714331d-e69e-45c0-9269-1fe8380546d0,"attention over the output of the encoder stack. Similar to the encoder, we employ residual connections"
Attention Is All You Need,2017-06-12,e34457cf-a5d4-4169-a074-54c6903fc61b,"around each of the sub-layers, followed by layer normalization. We also modify the self-attention"
Attention Is All You Need,2017-06-12,81c629a4-6ea5-4d17-a6de-8aec7a381317,sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
Attention Is All You Need,2017-06-12,e753b8b1-66eb-4db9-b8c3-91444854782c,"masking, combined with fact that the output embeddings are offset by one position, ensures that the"
Attention Is All You Need,2017-06-12,b06c77bf-89f1-4a92-985c-d55ced39b25c,predictions for position ican depend only on the known outputs at positions less than i.
Attention Is All You Need,2017-06-12,88392f0c-8d5f-4768-b3d9-1729f35de981,3.2 Attention
Attention Is All You Need,2017-06-12,243b2a31-a709-4feb-8655-0051515cc471,"An attention function can be described as mapping a query and a set of key-value pairs to an output,"
Attention Is All You Need,2017-06-12,de38cfd9-0872-47a1-8b19-7b7195ca70c9,"where the query, keys, values, and output are all vectors. The output is computed as a weighted sum"
Attention Is All You Need,2017-06-12,811e549b-70c4-4a01-b79d-6f6a9ccb3a23,3
Attention Is All You Need,2017-06-12,d2e0976a-d63e-4b33-ab3f-a596bf24f482,Scaled Dot-Product Attention
Attention Is All You Need,2017-06-12,acc847de-b7d7-4887-8ef1-d8df41c0d3e5,Multi-Head Attention
Attention Is All You Need,2017-06-12,ee286f7b-6899-4c4f-93d9-52a45aaacd40,Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
Attention Is All You Need,2017-06-12,1c78659e-2c67-4149-98d3-c21fc591fed7,attention layers running in parallel.
Attention Is All You Need,2017-06-12,cf7b7ea9-b855-4dc5-910d-a4eeed070ae1,"of the values, where the weight assigned to each value is computed by a compatibility function of the"
Attention Is All You Need,2017-06-12,8266aa16-79df-4a00-86bb-806de4c68409,query with the corresponding key.
Attention Is All You Need,2017-06-12,b1f8c83b-f3a7-4090-9500-c2ddbc48b7ca,3.2.1 Scaled Dot-Product Attention
Attention Is All You Need,2017-06-12,74125202-18b4-4e81-9409-dd9271457b0c,"We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of"
Attention Is All You Need,2017-06-12,a9e5b4e4-1513-4758-b6f4-70c0a29f6bce,"queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the"
Attention Is All You Need,2017-06-12,b247156f-863f-434a-a81b-832eeb53beb2,"query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the"
Attention Is All You Need,2017-06-12,1b051a33-1bca-4767-8bbe-6545ea1c3311,values.
Attention Is All You Need,2017-06-12,fa7f26ed-5ee8-43eb-ae3e-f6f85648f202,"In practice, we compute the attention function on a set of queries simultaneously, packed together"
Attention Is All You Need,2017-06-12,5bc414ff-8aab-4a09-9285-abf42dd82adb,into a matrix Q. The keys and values are also packed together into matrices KandV. We compute
Attention Is All You Need,2017-06-12,b4bf8e7e-1fcf-4439-8eb0-d5e89af64e2a,the matrix of outputs as:
Attention Is All You Need,2017-06-12,5632cc6c-42c2-468a-abb1-8e5932711cf1,"Attention( Q, K, V ) = softmax(QKT"
Attention Is All You Need,2017-06-12,c9a9e4dd-f841-4332-8358-d91a5c9f8c6e,√dk)V (1)
Attention Is All You Need,2017-06-12,66c9ae61-56dc-4c63-8745-c62ea39ac1df,"The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-"
Attention Is All You Need,2017-06-12,4eb81d69-9c06-4015-b9d9-2f065433c2d2,"plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor"
Attention Is All You Need,2017-06-12,b187f24f-520c-4561-aac7-1f176a646cea,of1√dk. Additive attention computes the compatibility function using a feed-forward network with
Attention Is All You Need,2017-06-12,8dce6f47-5e34-497b-8f43-57f51303d61a,"a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is"
Attention Is All You Need,2017-06-12,f7238ffe-dae4-4b85-a2ca-5ea676dcc2b8,"much faster and more space-efficient in practice, since it can be implemented using highly optimized"
Attention Is All You Need,2017-06-12,ff062bc6-052a-48d6-9236-ca33ba2bcbb3,matrix multiplication code.
Attention Is All You Need,2017-06-12,459d42ed-2b83-40ae-ac4c-513b137dff39,"While for small values of dkthe two mechanisms perform similarly, additive attention outperforms"
Attention Is All You Need,2017-06-12,dc29eb7d-e61b-478e-928f-49bbe0290443,dot product attention without scaling for larger values of dk[3]. We suspect that for large values of
Attention Is All You Need,2017-06-12,7631d8b6-8619-4e16-8676-6010e0b6284e,"dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has"
Attention Is All You Need,2017-06-12,99d27154-9305-4a90-bd0f-94d0fb67de2b,"extremely small gradients4. To counteract this effect, we scale the dot products by1√dk."
Attention Is All You Need,2017-06-12,c272b2d8-23b2-4b48-bbce-5e7f2fc63ec3,3.2.2 Multi-Head Attention
Attention Is All You Need,2017-06-12,894ae2db-ee4c-4c8e-8093-accb17c26ce4,"Instead of performing a single attention function with dmodel-dimensional keys, values and queries,"
Attention Is All You Need,2017-06-12,0a3b3d17-31ba-435f-acc9-c45cccdcb70c,"we found it beneficial to linearly project the queries, keys and values htimes with different, learned"
Attention Is All You Need,2017-06-12,149d77d4-4873-47e5-b651-0f5e9f5025a7,"linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of"
Attention Is All You Need,2017-06-12,0c93d050-46bd-4d10-a6b1-c7ae8dd52562,"queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional"
Attention Is All You Need,2017-06-12,ee6e13c2-3de4-4e72-ad6a-391c4fd0183d,"4To illustrate why the dot products get large, assume that the components of qandkare independent random"
Attention Is All You Need,2017-06-12,a8509e1b-c7d8-4ab0-9833-b0c090e1d359,"variables with mean 0and variance 1. Then their dot product, q·k=Pdk"
Attention Is All You Need,2017-06-12,09e4e37a-50d9-49c3-95a4-9de4baa8e03d,"i=1qiki, has mean 0and variance dk."
Attention Is All You Need,2017-06-12,6abc0c04-7add-42c7-a448-e9952930049e,4
Attention Is All You Need,2017-06-12,d20f603b-cbe5-45ec-bd51-f824cbb1fdc9,"output values. These are concatenated and once again projected, resulting in the final values, as"
Attention Is All You Need,2017-06-12,beb174c1-efc4-4210-952a-ab9c143f28ae,depicted in Figure 2.
Attention Is All You Need,2017-06-12,071264cb-2d8e-4e6b-b4d8-3e703b106325,Multi-head attention allows the model to jointly attend to information from different representation
Attention Is All You Need,2017-06-12,668d8492-527b-4346-b34c-561cd8eb8722,"subspaces at different positions. With a single attention head, averaging inhibits this."
Attention Is All You Need,2017-06-12,d71e9930-8439-4701-9e7a-fd7381e8ee71,"MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO"
Attention Is All You Need,2017-06-12,fc66a993-2782-4781-aee5-d66a4c2ebc0a,where head i= Attention( QWQ
Attention Is All You Need,2017-06-12,a7854173-3faf-458b-81f0-2ea2eee7a62a,"i, KWK"
Attention Is All You Need,2017-06-12,d0f2bc6b-6353-46bf-8d65-5e59ad82ced4,"i, V WV"
Attention Is All You Need,2017-06-12,2f61d183-78b2-41ff-b7eb-8e3ae6593137,i)
Attention Is All You Need,2017-06-12,348282bd-7ebf-4c5c-912d-29968709e02c,Where the projections are parameter matrices WQ
Attention Is All You Need,2017-06-12,8d1c25be-7fa7-4ac9-b21b-b5677b1550b5,"i∈Rdmodel×dk,WK"
Attention Is All You Need,2017-06-12,1411294e-a987-4914-8670-ff3f80e7fef8,"i∈Rdmodel×dk,WV"
Attention Is All You Need,2017-06-12,6e47a747-d6dc-4232-9c05-64df15ca7af8,i∈Rdmodel×dv
Attention Is All You Need,2017-06-12,90856c78-933d-4b7a-bdc0-094adc285fd3,andWO∈Rhdv×dmodel.
Attention Is All You Need,2017-06-12,82aad7cb-0eec-4889-a732-6d4980dbcc32,"In this work we employ h= 8 parallel attention layers, or heads. For each of these we use"
Attention Is All You Need,2017-06-12,5b2f4553-a529-4b8f-9542-6f5691babffc,"dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost"
Attention Is All You Need,2017-06-12,98e43619-8ad2-4028-81c6-9e3d1ef1b9d7,is similar to that of single-head attention with full dimensionality.
Attention Is All You Need,2017-06-12,1741944e-1e7b-4be5-8336-b629ee0b56d5,3.2.3 Applications of Attention in our Model
Attention Is All You Need,2017-06-12,602dfdad-b7f1-460b-af61-4659938443ca,The Transformer uses multi-head attention in three different ways:
Attention Is All You Need,2017-06-12,3f4d948f-d4c2-4caf-9342-fa9baf22719d,"•In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,"
Attention Is All You Need,2017-06-12,4a2752ac-98d7-4e94-a58a-373f3fdce007,and the memory keys and values come from the output of the encoder. This allows every
Attention Is All You Need,2017-06-12,73d79772-0df1-4584-a67d-3e56240e54cc,position in the decoder to attend over all positions in the input sequence. This mimics the
Attention Is All You Need,2017-06-12,3ba9c1d9-9e53-4365-b5c8-971ccb886300,typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
Attention Is All You Need,2017-06-12,9942d986-0de1-4f2b-99ab-4979c0b34689,"[38, 2, 9]."
Attention Is All You Need,2017-06-12,bdb6272c-c191-4f99-9e91-88b1096a0200,"•The encoder contains self-attention layers. In a self-attention layer all of the keys, values"
Attention Is All You Need,2017-06-12,1c000f83-afb9-4a50-a132-f6fb716aca52,"and queries come from the same place, in this case, the output of the previous layer in the"
Attention Is All You Need,2017-06-12,554b1caa-b752-4bbc-aee4-52a4ae8a83e6,encoder. Each position in the encoder can attend to all positions in the previous layer of the
Attention Is All You Need,2017-06-12,4e656871-b666-4bac-b239-32a9afeb627b,encoder.
Attention Is All You Need,2017-06-12,0c97e48d-0006-4126-be69-e85e772371f6,"•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to"
Attention Is All You Need,2017-06-12,4c3f7c66-1939-4689-a5f7-b3e5b285333c,all positions in the decoder up to and including that position. We need to prevent leftward
Attention Is All You Need,2017-06-12,289404a9-c4e5-49ec-93c4-36de28c9685b,information flow in the decoder to preserve the auto-regressive property. We implement this
Attention Is All You Need,2017-06-12,7baef1dd-e9da-4867-b1d6-a0b8df7a2190,inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
Attention Is All You Need,2017-06-12,f35af3cb-eafc-4cae-9754-c017e9d9344f,of the softmax which correspond to illegal connections. See Figure 2.
Attention Is All You Need,2017-06-12,3d7987ff-28db-4347-b088-36320c826a6d,3.3 Position-wise Feed-Forward Networks
Attention Is All You Need,2017-06-12,65c790bf-cb0d-4b43-92da-7f0444e618b5,"In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully"
Attention Is All You Need,2017-06-12,86940f9c-83ca-4d26-a7b8-2c1ad048673e,"connected feed-forward network, which is applied to each position separately and identically. This"
Attention Is All You Need,2017-06-12,d025e13b-5de0-4844-a4c3-2ae2929ce9e0,consists of two linear transformations with a ReLU activation in between.
Attention Is All You Need,2017-06-12,569a87d5-4bd0-466c-833d-3b9e05e91f5e,"FFN( x) = max(0 , xW 1+b1)W2+b2 (2)"
Attention Is All You Need,2017-06-12,c2c4ac7f-ae36-4919-9310-c2838acbf087,"While the linear transformations are the same across different positions, they use different parameters"
Attention Is All You Need,2017-06-12,58bf9ab8-3cf2-4db1-b5e1-e2c2086f0d0e,from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
Attention Is All You Need,2017-06-12,a52e5dff-a118-4028-ac6c-9e178a017310,"The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality"
Attention Is All You Need,2017-06-12,f9417792-9148-444d-844f-e089f034a402,dff= 2048 .
Attention Is All You Need,2017-06-12,3c6420e9-75fb-45fb-96c0-0fde43fe6de5,3.4 Embeddings and Softmax
Attention Is All You Need,2017-06-12,79ac07ce-24dd-4562-bb8a-478c9f32276c,"Similarly to other sequence transduction models, we use learned embeddings to convert the input"
Attention Is All You Need,2017-06-12,18938472-d1d0-4991-8893-2fd6e701cae7,tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-
Attention Is All You Need,2017-06-12,14209770-3ff1-4746-a1d5-af2a07f3fac3,mation and softmax function to convert the decoder output to predicted next-token probabilities. In
Attention Is All You Need,2017-06-12,6adff29e-60e8-482c-95eb-d7fa1fa0b66f,"our model, we share the same weight matrix between the two embedding layers and the pre-softmax"
Attention Is All You Need,2017-06-12,17405e34-19f3-4552-abeb-065ec572ed62,"linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel."
Attention Is All You Need,2017-06-12,5e06fe16-9ccb-4358-9ca9-eef49bdf21fa,5
Attention Is All You Need,2017-06-12,10a53a17-552a-47b7-ae20-a125d1fe5f97,"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations"
Attention Is All You Need,2017-06-12,f6cbbf74-fed9-435b-b100-6701a264a179,"for different layer types. nis the sequence length, dis the representation dimension, kis the kernel"
Attention Is All You Need,2017-06-12,4abc16c7-aa4f-44f2-837d-fb9c136eb586,size of convolutions and rthe size of the neighborhood in restricted self-attention.
Attention Is All You Need,2017-06-12,122fe9b8-a23d-4f88-a8b3-30bd07ef2286,Layer Type Complexity per Layer Sequential Maximum Path Length
Attention Is All You Need,2017-06-12,48c14277-f3e5-4d25-be86-db82e4417d59,Operations
Attention Is All You Need,2017-06-12,8c285612-2b72-4f0d-b5b7-3c9e8fdaec59,Self-Attention O(n2·d) O(1) O(1)
Attention Is All You Need,2017-06-12,79f76730-1bdd-44a2-8527-f24d2b84a60d,Recurrent O(n·d2) O(n) O(n)
Attention Is All You Need,2017-06-12,e44f87d5-a468-4b06-9aec-7230620bc67c,Convolutional O(k·n·d2) O(1) O(logk(n))
Attention Is All You Need,2017-06-12,df6a6b10-7d7d-4e3b-be6d-74ef13b35924,Self-Attention (restricted) O(r·n·d) O(1) O(n/r)
Attention Is All You Need,2017-06-12,944a4689-4119-42f6-a1c5-eb59e1fcdf9f,3.5 Positional Encoding
Attention Is All You Need,2017-06-12,930d8ee7-423e-43a3-842e-6d4d289a2c52,"Since our model contains no recurrence and no convolution, in order for the model to make use of the"
Attention Is All You Need,2017-06-12,3df887bc-8056-4f41-abfc-1eb88fc07a1d,"order of the sequence, we must inject some information about the relative or absolute position of the"
Attention Is All You Need,2017-06-12,b5a96f08-8a91-457e-b444-fc4e7184bfe1,"tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the"
Attention Is All You Need,2017-06-12,170c7fd2-f999-475d-af85-f6d3f63dadad,bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel
Attention Is All You Need,2017-06-12,9ade6976-baa0-4355-a5ab-f590d1dfe73c,"as the embeddings, so that the two can be summed. There are many choices of positional encodings,"
Attention Is All You Need,2017-06-12,821bd52b-ae1a-4f20-9d2c-436a118e6791,learned and fixed [9].
Attention Is All You Need,2017-06-12,7b2d68ff-a9dc-4f75-9da5-a50bc7631046,"In this work, we use sine and cosine functions of different frequencies:"
Attention Is All You Need,2017-06-12,f8c932b5-cc79-4597-a866-fe4dbb52622a,"PE(pos,2i)=sin(pos/100002i/d model)"
Attention Is All You Need,2017-06-12,e28eb63c-fdfd-4f5a-95fe-73badc514966,"PE(pos,2i+1)=cos(pos/100002i/d model)"
Attention Is All You Need,2017-06-12,c5ab0b4d-d589-4e68-b9c2-0efedb1216b0,"where posis the position and iis the dimension. That is, each dimension of the positional encoding"
Attention Is All You Need,2017-06-12,be6a39fa-de98-4e93-a662-5b76d3504d81,corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We
Attention Is All You Need,2017-06-12,6c67b630-5bbd-43af-85f6-329a97056837,chose this function because we hypothesized it would allow the model to easily learn to attend by
Attention Is All You Need,2017-06-12,8be5a523-359a-4b99-97b5-610905cb5f3a,"relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of"
Attention Is All You Need,2017-06-12,df0b6c75-ad2f-4dd8-8374-2e664ce632f7,PEpos.
Attention Is All You Need,2017-06-12,b507eb40-35ab-4686-97c3-93d57b8afef5,"We also experimented with using learned positional embeddings [ 9] instead, and found that the two"
Attention Is All You Need,2017-06-12,e9159032-480a-4987-bbe1-777b39bb7a23,versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
Attention Is All You Need,2017-06-12,302ada93-db46-4ade-82f6-d276ddb7809c,because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
Attention Is All You Need,2017-06-12,7b26f344-77cb-4190-b134-58cf942b7a4e,during training.
Attention Is All You Need,2017-06-12,4f25523a-47c8-4a37-914e-a9a984058196,4 Why Self-Attention
Attention Is All You Need,2017-06-12,a9643e90-6836-4063-b433-e9c04e52c5f5,In this section we compare various aspects of self-attention layers to the recurrent and convolu-
Attention Is All You Need,2017-06-12,63fe3aca-fbc1-4029-a515-86ede02c709a,tional layers commonly used for mapping one variable-length sequence of symbol representations
Attention Is All You Need,2017-06-12,60ca77f3-b2d2-4c1c-971d-8346798b29b1,"(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden"
Attention Is All You Need,2017-06-12,5cabe9c2-5a7d-47bc-99d6-1e294b60cbcd,layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
Attention Is All You Need,2017-06-12,595b3276-5170-4b04-819c-cb2a476d8dd1,consider three desiderata.
Attention Is All You Need,2017-06-12,717a8e9e-6785-44f2-bcdb-37e61936d8d9,One is the total computational complexity per layer. Another is the amount of computation that can
Attention Is All You Need,2017-06-12,0a99c9ae-e4a3-4125-b2ea-5678ef99877e,"be parallelized, as measured by the minimum number of sequential operations required."
Attention Is All You Need,2017-06-12,bf9c4012-8200-4ef0-83c7-ab68cfa86fad,The third is the path length between long-range dependencies in the network. Learning long-range
Attention Is All You Need,2017-06-12,0c053443-dddc-4c83-a538-21f463412a9a,dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
Attention Is All You Need,2017-06-12,3073b01a-457c-4239-a6cc-1609bac4ec5f,ability to learn such dependencies is the length of the paths forward and backward signals have to
Attention Is All You Need,2017-06-12,9ab7905d-34e1-4aba-81b9-33f15e5f7b37,traverse in the network. The shorter these paths between any combination of positions in the input
Attention Is All You Need,2017-06-12,216161b9-2e47-421a-a74b-4c9fe615b512,"and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare"
Attention Is All You Need,2017-06-12,22152f69-6e5c-4895-89f5-32d923cf1ec6,the maximum path length between any two input and output positions in networks composed of the
Attention Is All You Need,2017-06-12,2df54a22-8ea0-4219-89b8-e9fbbc2b3006,different layer types.
Attention Is All You Need,2017-06-12,b8eaaf1e-fc99-4e5d-816c-e31ff0f6f678,"As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially"
Attention Is All You Need,2017-06-12,545e60c2-5991-45e6-9624-ab0e3abe6f44,"executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of"
Attention Is All You Need,2017-06-12,65e8c553-df7a-44a4-92ef-192411e74407,"computational complexity, self-attention layers are faster than recurrent layers when the sequence"
Attention Is All You Need,2017-06-12,99b8a948-75e1-4628-bdd9-5e5d026bd207,6
Attention Is All You Need,2017-06-12,20c1dca5-b0ff-4aa2-b8d9-3c90c254a42f,"length nis smaller than the representation dimensionality d, which is most often the case with"
Attention Is All You Need,2017-06-12,c18eabc3-446f-4178-a7f8-4ff94267b382,"sentence representations used by state-of-the-art models in machine translations, such as word-piece"
Attention Is All You Need,2017-06-12,e584dd56-ca6f-4623-a122-cbcd9d6ba1cf,[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving
Attention Is All You Need,2017-06-12,8885f1e0-8ad1-4ead-b4fb-693c0333daf4,"very long sequences, self-attention could be restricted to considering only a neighborhood of size rin"
Attention Is All You Need,2017-06-12,28ca1437-7b93-48bb-9390-a4adfebe629f,the input sequence centered around the respective output position. This would increase the maximum
Attention Is All You Need,2017-06-12,76361664-9fc7-4c25-af02-f7b82596ea67,path length to O(n/r). We plan to investigate this approach further in future work.
Attention Is All You Need,2017-06-12,3aa96769-69d1-4a6f-9c19-17484877d1f0,A single convolutional layer with kernel width k < n does not connect all pairs of input and output
Attention Is All You Need,2017-06-12,3ce56fbc-2dbb-488e-ba3d-968251f41fa8,"positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,"
Attention Is All You Need,2017-06-12,7b62df98-d8ae-4acf-8665-4b5ca28ed485,"orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths"
Attention Is All You Need,2017-06-12,b395e3e3-d833-44fa-97de-bfdafb649039,between any two positions in the network. Convolutional layers are generally more expensive than
Attention Is All You Need,2017-06-12,2fbd6951-96ad-4227-ae64-5aacb52d63ea,"recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity"
Attention Is All You Need,2017-06-12,429ba9e8-fc3a-4198-9789-e5e0896befbb,"considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable"
Attention Is All You Need,2017-06-12,15227de5-fa48-4521-93b7-b94f42c7a6ea,"convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,"
Attention Is All You Need,2017-06-12,af191dbb-c64c-4cdb-a7d3-46be0e5298ec,the approach we take in our model.
Attention Is All You Need,2017-06-12,e225d2d0-e466-4a3c-9400-952f7522d127,"As side benefit, self-attention could yield more interpretable models. We inspect attention distributions"
Attention Is All You Need,2017-06-12,fcb6eae4-8354-440c-8985-d97eb8e3f10b,from our models and present and discuss examples in the appendix. Not only do individual attention
Attention Is All You Need,2017-06-12,aa1a7ea3-0320-430b-a66b-0a52f319bc36,"heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic"
Attention Is All You Need,2017-06-12,f3719d71-4944-4be2-a9d1-c2f9ffe84989,and semantic structure of the sentences.
Attention Is All You Need,2017-06-12,0d3145fc-3dd9-4221-8104-b0f0536c372c,5 Training
Attention Is All You Need,2017-06-12,a8cbac57-d45d-4460-b4b0-2b362e3344d5,This section describes the training regime for our models.
Attention Is All You Need,2017-06-12,6ab40b08-eddb-400c-855f-0dc4e30039d1,5.1 Training Data and Batching
Attention Is All You Need,2017-06-12,3dabb83b-f665-48d2-b492-27cb578d7b89,We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
Attention Is All You Need,2017-06-12,9550e1e7-e375-4541-8f22-5b05d6bb997e,"sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-"
Attention Is All You Need,2017-06-12,38513114-0fa6-48ee-82ac-7e53d2353376,"target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT"
Attention Is All You Need,2017-06-12,eaedaffe-9666-4f93-a59f-ae17f6d45b93,2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
Attention Is All You Need,2017-06-12,dcd2599d-4b4e-4a58-9e5c-1f82758f847b,vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training
Attention Is All You Need,2017-06-12,5529149d-7e19-4e52-8ca3-26d6b3df3b31,batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
Attention Is All You Need,2017-06-12,95fa5b6d-f736-47fb-8422-aea0fb0291c0,target tokens.
Attention Is All You Need,2017-06-12,e3d28666-46ac-4584-82f6-7d9e1093ae54,5.2 Hardware and Schedule
Attention Is All You Need,2017-06-12,6d7c966b-48de-4fac-b9d8-724f0dfedb7d,We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
Attention Is All You Need,2017-06-12,0f207d1f-d7fb-47cd-9eb0-eed9c45afde5,"the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We"
Attention Is All You Need,2017-06-12,7693c1c1-ad08-490a-a574-56a10b3ba980,"trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the"
Attention Is All You Need,2017-06-12,c13d7dba-2fef-4100-90ee-c4064ccdb285,"bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps"
Attention Is All You Need,2017-06-12,23f87d0e-1070-4000-836e-90f8f3f67e4d,(3.5 days).
Attention Is All You Need,2017-06-12,96e3bf71-d34b-430b-a249-b4faaa4ef107,5.3 Optimizer
Attention Is All You Need,2017-06-12,ca2f4371-eceb-445f-8e6f-9539ac6206ee,"We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning"
Attention Is All You Need,2017-06-12,c88d2c86-9ddd-44c4-8db4-ee42c691020c,"rate over the course of training, according to the formula:"
Attention Is All You Need,2017-06-12,b7150b7f-be30-4d48-84e0-a3db9f6eb81e,lrate =d−0.5
Attention Is All You Need,2017-06-12,124063a7-6f6f-4212-882b-df89b575d7e9,"model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)"
Attention Is All You Need,2017-06-12,26ad1a6e-89d0-4ced-94a5-e414a5e9838d,"This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,"
Attention Is All You Need,2017-06-12,8bb21995-33a9-49b6-9de8-8ff3de913085,and decreasing it thereafter proportionally to the inverse square root of the step number. We used
Attention Is All You Need,2017-06-12,e88e00f1-b687-4fe4-80e0-6ed87e1dd2a1,warmup _steps = 4000 .
Attention Is All You Need,2017-06-12,66725183-2259-443c-b293-4145b3e0f0f7,5.4 Regularization
Attention Is All You Need,2017-06-12,6b6506e6-bae1-41eb-9266-23eb06d6d5ee,We employ three types of regularization during training:
Attention Is All You Need,2017-06-12,aeef20d9-7abb-492f-acd0-627e491aaa3b,7
Attention Is All You Need,2017-06-12,1df0c643-679e-443e-90d2-7341dd6a7391,Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
Attention Is All You Need,2017-06-12,23b5821a-eef0-42de-863f-5e0228778f3d,English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
Attention Is All You Need,2017-06-12,0a18564f-2e7a-40a5-8e70-cae361b810f7,ModelBLEU Training Cost (FLOPs)
Attention Is All You Need,2017-06-12,5f134468-f228-4343-8c97-5d0629866667,EN-DE EN-FR EN-DE EN-FR
Attention Is All You Need,2017-06-12,9d14a90b-dd59-49d4-be2c-72b449979bbb,ByteNet [18] 23.75
Attention Is All You Need,2017-06-12,7d94aa0b-63da-4a1a-ac33-954210639309,Deep-Att + PosUnk [39] 39.2 1.0·1020
Attention Is All You Need,2017-06-12,213f9c59-d204-46c7-bded-931215a698bc,GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020
Attention Is All You Need,2017-06-12,41473e67-38d7-4b58-a375-6e31a2cac4fe,ConvS2S [9] 25.16 40.46 9.6·10181.5·1020
Attention Is All You Need,2017-06-12,29a7ce6f-fe29-47fa-84cd-6c23df16dfa9,MoE [32] 26.03 40.56 2.0·10191.2·1020
Attention Is All You Need,2017-06-12,95f17223-e570-4c27-aac1-f8bdeb4db592,Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020
Attention Is All You Need,2017-06-12,4bbc914d-58ef-4902-bab8-aed9fcb3d814,GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021
Attention Is All You Need,2017-06-12,b76bca9b-8ff2-4989-aa0b-d545d01a9b81,ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021
Attention Is All You Need,2017-06-12,6c4e1901-1b3f-42b3-9fa7-5c121389b3ac,Transformer (base model) 27.3 38.1 3.3·1018
Attention Is All You Need,2017-06-12,7c7ad448-c494-4b38-a9d7-2a1787575f19,Transformer (big) 28.4 41.8 2.3·1019
Attention Is All You Need,2017-06-12,eeee3363-03f6-4e41-a52d-d5b53b85eb9a,"Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the"
Attention Is All You Need,2017-06-12,19194d2b-b91f-4231-b934-ff5f9c2d807a,"sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the"
Attention Is All You Need,2017-06-12,4197d62b-46ac-4644-a473-3f4d71fd387b,"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of"
Attention Is All You Need,2017-06-12,01f14456-7d79-4584-a42c-1e4925025190,Pdrop= 0.1.
Attention Is All You Need,2017-06-12,4963d209-3643-40e1-a034-fca5f1802917,"Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This"
Attention Is All You Need,2017-06-12,af888602-1001-484e-a53f-83f90534e85f,"hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
Attention Is All You Need,2017-06-12,67fff77c-d3a2-4c70-a625-dc300b1da34d,6 Results
Attention Is All You Need,2017-06-12,5be964b7-b997-47e1-b7b1-834ce430c504,6.1 Machine Translation
Attention Is All You Need,2017-06-12,9e74e692-9827-4125-ae08-6dc24db90b64,"On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)"
Attention Is All You Need,2017-06-12,a72001ac-661f-42cf-81b0-7ff32eca0f8b,in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
Attention Is All You Need,2017-06-12,795b9567-b7b9-4ea1-aefe-860a32a64ed4,"BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is"
Attention Is All You Need,2017-06-12,4a03f868-7cfe-4ad1-b278-92c5e182bb45,listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model
Attention Is All You Need,2017-06-12,94e208b7-682d-4a0e-9e6f-e10b87ad68a2,"surpasses all previously published models and ensembles, at a fraction of the training cost of any of"
Attention Is All You Need,2017-06-12,e629d165-d164-41ae-aeb4-739cd2dc5de5,the competitive models.
Attention Is All You Need,2017-06-12,fd39dfe0-0b80-4442-98e3-525c67855b46,"On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,"
Attention Is All You Need,2017-06-12,6a666941-d90e-4644-b300-cdf903a44732,"outperforming all of the previously published single models, at less than 1/4the training cost of the"
Attention Is All You Need,2017-06-12,99484b61-609e-4e3e-875d-0462bf7feb9e,previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
Attention Is All You Need,2017-06-12,9e225589-571b-4b17-8320-db7f6bccef5a,"dropout rate Pdrop= 0.1, instead of 0.3."
Attention Is All You Need,2017-06-12,00f7ab14-1288-4575-a9e3-9e3fec0dc1cf,"For the base models, we used a single model obtained by averaging the last 5 checkpoints, which"
Attention Is All You Need,2017-06-12,6735c3db-9d6b-490b-8ed2-01ea65d00243,"were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We"
Attention Is All You Need,2017-06-12,f8287d1d-f2ca-4b05-9dc1-88a56bed1a5b,used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters
Attention Is All You Need,2017-06-12,ce214a3e-3d50-4333-98ef-d58f547d0faf,were chosen after experimentation on the development set. We set the maximum output length during
Attention Is All You Need,2017-06-12,5d93bacb-86ab-448e-9394-ec1dee7bbb84,"inference to input length + 50, but terminate early when possible [38]."
Attention Is All You Need,2017-06-12,8d04515b-6459-4469-825f-c0832be3b95c,Table 2 summarizes our results and compares our translation quality and training costs to other model
Attention Is All You Need,2017-06-12,51474de5-f78f-492d-9d11-6759e0f6ef33,architectures from the literature. We estimate the number of floating point operations used to train a
Attention Is All You Need,2017-06-12,e6d3528d-15b4-41cd-8591-8d1b50f7b045,"model by multiplying the training time, the number of GPUs used, and an estimate of the sustained"
Attention Is All You Need,2017-06-12,bd30c588-7b00-4a8c-88b7-3592c8309824,single-precision floating-point capacity of each GPU5.
Attention Is All You Need,2017-06-12,19cc411e-73fe-4ed2-956b-457841327786,6.2 Model Variations
Attention Is All You Need,2017-06-12,50756353-aa18-4cac-b21a-b0d57cf93f63,"To evaluate the importance of different components of the Transformer, we varied our base model"
Attention Is All You Need,2017-06-12,55c51c09-379d-4485-873b-5bc674bcdbd0,"in different ways, measuring the change in performance on English-to-German translation on the"
Attention Is All You Need,2017-06-12,58a06c09-1d84-4222-b369-f7357a5a9499,"5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."
Attention Is All You Need,2017-06-12,1b75818e-ef85-4c5d-9613-af5bc22a97df,8
Attention Is All You Need,2017-06-12,74ddaf97-5ae5-451e-9ab7-69336756d436,Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
Attention Is All You Need,2017-06-12,d24e7dd7-f380-4363-b714-2ca3ef394347,"model. All metrics are on the English-to-German translation development set, newstest2013. Listed"
Attention Is All You Need,2017-06-12,cd105f4f-e6a5-4f7e-821c-385c6a2605b5,"perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to"
Attention Is All You Need,2017-06-12,40586c92-e194-4b65-bf73-62fb2687fddb,per-word perplexities.
Attention Is All You Need,2017-06-12,7e2c1d14-549d-49b3-b885-a7961c7ed7a3,N d model dff h d k dvPdrop ϵlstrain PPL BLEU params
Attention Is All You Need,2017-06-12,d486a6e8-de17-489b-b471-abbe6d2a1fce,steps (dev) (dev) ×106
Attention Is All You Need,2017-06-12,57d50d35-45d7-4468-8da5-9b389bf9932f,base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
Attention Is All You Need,2017-06-12,72186eb9-49d6-43b0-a59d-82cfb7a4d05a,(A)1 512 512 5.29 24.9
Attention Is All You Need,2017-06-12,fade7040-b5fa-4557-b2b6-86ec9dec7847,4 128 128 5.00 25.5
Attention Is All You Need,2017-06-12,aac211d1-4998-4080-a238-ebab7494ed11,16 32 32 4.91 25.8
Attention Is All You Need,2017-06-12,060d4531-ee43-44d6-a8b8-af09c99f330c,32 16 16 5.01 25.4
Attention Is All You Need,2017-06-12,8b4bbd6d-3482-4b0c-8dcf-42f4e30a8de3,(B)16 5.16 25.1 58
Attention Is All You Need,2017-06-12,8498beb1-a4df-4187-94fb-59c08427e6e4,32 5.01 25.4 60
Attention Is All You Need,2017-06-12,5783e311-316e-4170-a4ff-7a66214a9a62,(C)2 6.11 23.7 36
Attention Is All You Need,2017-06-12,fe0d9ad1-2c81-449e-8f7f-65f55d1755b6,4 5.19 25.3 50
Attention Is All You Need,2017-06-12,2071473f-6383-4303-93f3-145f27c519e1,8 4.88 25.5 80
Attention Is All You Need,2017-06-12,2301b315-3a8b-4a60-b5fb-201f2ebc8e10,256 32 32 5.75 24.5 28
Attention Is All You Need,2017-06-12,2a10050a-ee0f-498c-8fdb-6df10b062b47,1024 128 128 4.66 26.0 168
Attention Is All You Need,2017-06-12,ff3fd5e5-52bf-42d0-a03a-2bcba6b88c92,1024 5.12 25.4 53
Attention Is All You Need,2017-06-12,92000889-a1fc-4551-b91a-01eb9e983b44,4096 4.75 26.2 90
Attention Is All You Need,2017-06-12,6a063dab-dfe6-4067-bfdf-6f15b6d82e57,(D)0.0 5.77 24.6
Attention Is All You Need,2017-06-12,dc93cf65-cf3b-4790-a975-a6313f1b538d,0.2 4.95 25.5
Attention Is All You Need,2017-06-12,c0021f4d-81db-4c24-b122-8c0d318e2102,0.0 4.67 25.3
Attention Is All You Need,2017-06-12,08825e9b-3165-48ac-becc-a71d2fbe72e9,0.2 5.47 25.7
Attention Is All You Need,2017-06-12,70585d1f-e16c-46b1-a287-21bc58801781,(E) positional embedding instead of sinusoids 4.92 25.7
Attention Is All You Need,2017-06-12,eb63c0c2-fe73-4793-be3c-a23f2e621c0a,big 6 1024 4096 16 0.3 300K 4.33 26.4 213
Attention Is All You Need,2017-06-12,6f216c58-becc-4c05-a7df-c47b5f7343ec,"development set, newstest2013. We used beam search as described in the previous section, but no"
Attention Is All You Need,2017-06-12,f3878155-c835-4a61-8a3e-83a64a8fad01,checkpoint averaging. We present these results in Table 3.
Attention Is All You Need,2017-06-12,33863c0f-0046-49de-8165-0e74d3b063ac,"In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,"
Attention Is All You Need,2017-06-12,506f2ab1-a627-4c1b-93e4-837fe32831a5,"keeping the amount of computation constant, as described in Section 3.2.2. While single-head"
Attention Is All You Need,2017-06-12,bb190d71-1008-437e-9df6-fc28ec165349,"attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."
Attention Is All You Need,2017-06-12,7a00451f-7702-4f3f-8d70-545354aee7e1,"In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This"
Attention Is All You Need,2017-06-12,a66cee82-2544-4470-9204-bdf6e49885ee,suggests that determining compatibility is not easy and that a more sophisticated compatibility
Attention Is All You Need,2017-06-12,6f74dfdd-15cb-47b2-9969-95be67f83b2d,"function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,"
Attention Is All You Need,2017-06-12,5af8e6bf-bab6-47fd-8530-5950b25bc352,"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our"
Attention Is All You Need,2017-06-12,dbc0ad6b-fa8f-487e-8c71-04632ec4a706,"sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical"
Attention Is All You Need,2017-06-12,a2e39220-d1ca-4e39-ad6f-f1c003276937,results to the base model.
Attention Is All You Need,2017-06-12,769d8f82-f686-483d-be21-0ec05be7ce4e,6.3 English Constituency Parsing
Attention Is All You Need,2017-06-12,763e2d99-dac3-49b2-b31c-dbc439093c11,To evaluate if the Transformer can generalize to other tasks we performed experiments on English
Attention Is All You Need,2017-06-12,24259040-a3c6-4cec-8e8f-48ce797303fe,constituency parsing. This task presents specific challenges: the output is subject to strong structural
Attention Is All You Need,2017-06-12,9a21545c-87ed-4739-8bbd-49f381ddcd03,"constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence"
Attention Is All You Need,2017-06-12,cde7697b-8b76-4332-b2b5-585ba8897203,models have not been able to attain state-of-the-art results in small-data regimes [37].
Attention Is All You Need,2017-06-12,cb00f9b0-548a-44e0-8b75-e1538be9787d,We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
Attention Is All You Need,2017-06-12,efb97cfc-f8dc-4701-aaed-7d70ab7e6d3f,"Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,"
Attention Is All You Need,2017-06-12,99b7d74b-7514-48aa-9c1e-a55e731767d5,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences
Attention Is All You Need,2017-06-12,0882d993-0c7c-44b4-95c4-c2ee154ea9ab,[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
Attention Is All You Need,2017-06-12,06e0a9d2-c166-49a1-9dd0-1f0d89773385,for the semi-supervised setting.
Attention Is All You Need,2017-06-12,40df92e4-2bc4-4a45-982b-f6eeb13b0d82,"We performed only a small number of experiments to select the dropout, both attention and residual"
Attention Is All You Need,2017-06-12,8a6ccf04-d02a-4a16-a735-4d404101e873,"(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters"
Attention Is All You Need,2017-06-12,c2606d3a-de34-4ed1-9805-ccc48969d891,"remained unchanged from the English-to-German base translation model. During inference, we"
Attention Is All You Need,2017-06-12,bd38985e-8ef2-4e12-8c51-351cfd238fdd,9
Attention Is All You Need,2017-06-12,38aa9984-4db9-4cf8-bdc6-6c2a2eeb7238,Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
Attention Is All You Need,2017-06-12,00af7ff4-5ebe-4481-bd10-7b705bf86a82,of WSJ)
Attention Is All You Need,2017-06-12,5da2a34c-5fa8-46dd-9024-c4ef3fd46356,Parser Training WSJ 23 F1
Attention Is All You Need,2017-06-12,736fb3d8-e8d4-4dc0-a756-a40754a6f4e0,"Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3"
Attention Is All You Need,2017-06-12,1313d51e-eb69-4af1-9845-b5c2171a554f,"Petrov et al. (2006) [29] WSJ only, discriminative 90.4"
Attention Is All You Need,2017-06-12,b1ad53d8-da07-4e84-b64e-65e449aa4d4f,"Zhu et al. (2013) [40] WSJ only, discriminative 90.4"
Attention Is All You Need,2017-06-12,c14d2588-0dfc-405a-a04e-7959ae9d62eb,"Dyer et al. (2016) [8] WSJ only, discriminative 91.7"
Attention Is All You Need,2017-06-12,c503ae0f-d6a4-43bb-aac8-81eea06d4654,"Transformer (4 layers) WSJ only, discriminative 91.3"
Attention Is All You Need,2017-06-12,495cdc90-0eda-47a1-a80d-7b19544eeeea,Zhu et al. (2013) [40] semi-supervised 91.3
Attention Is All You Need,2017-06-12,3399513a-8a43-44f2-aec8-830d47b76919,Huang & Harper (2009) [14] semi-supervised 91.3
Attention Is All You Need,2017-06-12,c65c622c-f78d-41b9-bc58-3914301c603d,McClosky et al. (2006) [26] semi-supervised 92.1
Attention Is All You Need,2017-06-12,596e7c57-f40a-46c6-aa70-b052c40bdede,Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1
Attention Is All You Need,2017-06-12,bbebfb8d-3088-4f26-8510-1b75d7b66cc9,Transformer (4 layers) semi-supervised 92.7
Attention Is All You Need,2017-06-12,8b321891-dd4f-40f1-8d29-5c82b209ae03,Luong et al. (2015) [23] multi-task 93.0
Attention Is All You Need,2017-06-12,a5db5f08-1902-4a9d-a997-559ff33423bb,Dyer et al. (2016) [8] generative 93.3
Attention Is All You Need,2017-06-12,59b5e04d-e241-4fdf-96c1-138f9d9a4ba9,increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3
Attention Is All You Need,2017-06-12,4ef9caf5-430d-4550-9584-87b0c328ebc0,for both WSJ only and the semi-supervised setting.
Attention Is All You Need,2017-06-12,77a9c74e-7330-4d25-88f2-4bb2b05d2b87,Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-
Attention Is All You Need,2017-06-12,15ffb4f1-0e67-4e12-b1c1-c5df0fcf0473,"prisingly well, yielding better results than all previously reported models with the exception of the"
Attention Is All You Need,2017-06-12,4b8e4a5e-558e-42f8-b04b-04e823057422,Recurrent Neural Network Grammar [8].
Attention Is All You Need,2017-06-12,3b113fd0-f353-438a-a773-d5cb26c395a8,"In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-"
Attention Is All You Need,2017-06-12,ad35c818-a5af-4994-8738-e30cdb354ec1,Parser [29] even when training only on the WSJ training set of 40K sentences.
Attention Is All You Need,2017-06-12,ba182fc0-a695-4241-a86a-bd3a94bb5e5b,7 Conclusion
Attention Is All You Need,2017-06-12,c8903b9c-2321-4442-a398-624bb298aa77,"In this work, we presented the Transformer, the first sequence transduction model based entirely on"
Attention Is All You Need,2017-06-12,941c4997-a674-42fe-b560-763a532009ae,"attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with"
Attention Is All You Need,2017-06-12,ed721388-140e-4a42-93de-bc8e867a034f,multi-headed self-attention.
Attention Is All You Need,2017-06-12,9a9c7938-e1c0-4fc3-8155-1c25884fa510,"For translation tasks, the Transformer can be trained significantly faster than architectures based"
Attention Is All You Need,2017-06-12,3241e91a-ff66-4559-890c-8e3a8d35a027,on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
Attention Is All You Need,2017-06-12,f35b9d28-b627-4a47-a461-cf880cce9851,"English-to-French translation tasks, we achieve a new state of the art. In the former task our best"
Attention Is All You Need,2017-06-12,590659fd-69dc-45df-8aea-13a3392ed373,model outperforms even all previously reported ensembles.
Attention Is All You Need,2017-06-12,10c01501-5cf7-4ee1-afda-f2dfd5385035,We are excited about the future of attention-based models and plan to apply them to other tasks. We
Attention Is All You Need,2017-06-12,addcd36a-71cc-4917-a802-ec1bd7721b3e,plan to extend the Transformer to problems involving input and output modalities other than text and
Attention Is All You Need,2017-06-12,8996da42-9adb-451a-b20b-296524e72ca0,"to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs"
Attention Is All You Need,2017-06-12,52ee9edf-edcf-48de-9ec4-52c3f8d5189c,"such as images, audio and video. Making generation less sequential is another research goals of ours."
Attention Is All You Need,2017-06-12,52a3ebed-b303-4e36-af24-3ba4edba9605,The code we used to train and evaluate our models is available at https://github.com/
Attention Is All You Need,2017-06-12,3ba1c91c-763b-4143-8bc6-26c6c5102b78,tensorflow/tensor2tensor .
Attention Is All You Need,2017-06-12,daadbe6a-ef6c-4b28-b0a4-3cec4da65366,Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
Attention Is All You Need,2017-06-12,20b7dccf-ed01-4781-b73e-5624e08ce714,"comments, corrections and inspiration."
Attention Is All You Need,2017-06-12,ee2a0a82-4174-43cd-8c39-546c1b9826fc,References
Attention Is All You Need,2017-06-12,ff913e3d-50b5-4d68-a333-ee69268132fb,"[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint"
Attention Is All You Need,2017-06-12,df46800d-6ff4-4a1e-9e2d-bddb52cc3bb4,"arXiv:1607.06450 , 2016."
Attention Is All You Need,2017-06-12,ea140d3a-3b0a-4a81-8145-f3ec33d23c74,"[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly"
Attention Is All You Need,2017-06-12,bab73bb1-4913-43a9-a5c3-4b41c7961b19,"learning to align and translate. CoRR , abs/1409.0473, 2014."
Attention Is All You Need,2017-06-12,2e9a8c03-a439-4b92-86b4-a3b84030a0f9,"[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural"
Attention Is All You Need,2017-06-12,3e838b8c-5718-44cb-8331-a9b342482bfa,"machine translation architectures. CoRR , abs/1703.03906, 2017."
Attention Is All You Need,2017-06-12,3169ac5d-df74-46e9-b2ed-dcdb64ad04f7,"[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine"
Attention Is All You Need,2017-06-12,4cb9da1a-bc55-4c02-80c7-c9d6aa0e0223,"reading. arXiv preprint arXiv:1601.06733 , 2016."
Attention Is All You Need,2017-06-12,b09973e2-c243-47f4-9100-2cffd766290b,10
Attention Is All You Need,2017-06-12,986d205a-950a-4400-a212-db576525ca02,"[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,"
Attention Is All You Need,2017-06-12,201be049-3fd5-41df-93a3-3514a016b9b0,and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
Attention Is All You Need,2017-06-12,0a4d623f-e6d3-4034-b07a-b11360afcbce,"machine translation. CoRR , abs/1406.1078, 2014."
Attention Is All You Need,2017-06-12,add503f1-7ed7-41f1-b033-0d9295149cda,[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
Attention Is All You Need,2017-06-12,226f6f0c-da0c-40dc-8540-35d550117ba6,"preprint arXiv:1610.02357 , 2016."
Attention Is All You Need,2017-06-12,11727188-8795-4cb3-a4d3-415460f85a2f,"[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation"
Attention Is All You Need,2017-06-12,dcf32a8b-02b2-440c-ac26-65362a6210fd,"of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014."
Attention Is All You Need,2017-06-12,40f12d2e-38f0-41bb-94c6-01cc9d60762a,"[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural"
Attention Is All You Need,2017-06-12,71bfba3d-3966-46b7-a61f-52180ff180e0,"network grammars. In Proc. of NAACL , 2016."
Attention Is All You Need,2017-06-12,1e1f6bc5-53f6-4bb5-8a3a-13b1e226c893,"[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-"
Attention Is All You Need,2017-06-12,aee9e7c5-6503-405c-84a0-b73b46255e37,"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017."
Attention Is All You Need,2017-06-12,af69aee3-8b91-4ed2-9b15-4c3de5763720,[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
Attention Is All You Need,2017-06-12,2d622eb5-566a-4fcf-a526-5aacdfcd55d1,"arXiv:1308.0850 , 2013."
Attention Is All You Need,2017-06-12,45dd598f-a76f-4a91-ae35-6f8ba2df1a2d,"[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-"
Attention Is All You Need,2017-06-12,13f6173f-2bfc-4d94-bcee-23398595ad83,age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Attention Is All You Need,2017-06-12,3ee251a5-e6e5-4310-911e-d001dc54faf1,"Recognition , pages 770–778, 2016."
Attention Is All You Need,2017-06-12,2bb367f5-f616-47ea-92d9-aa1c7142dfbd,"[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in"
Attention Is All You Need,2017-06-12,fc83a167-ba1b-4b3a-8ad9-f82bed2b0462,"recurrent nets: the difficulty of learning long-term dependencies, 2001."
Attention Is All You Need,2017-06-12,e0cdc8a4-9be5-42d2-b40e-d7b5b021f071,"[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,"
Attention Is All You Need,2017-06-12,f63a75f7-9912-4078-91d5-e95fba4736ef,"9(8):1735–1780, 1997."
Attention Is All You Need,2017-06-12,a16e6aab-230d-4c40-95c2-e5777fe87b8e,[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
Attention Is All You Need,2017-06-12,3e0f5758-b724-43f5-a369-b724e41fddb2,across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Attention Is All You Need,2017-06-12,c5ec0bfd-2e40-4066-9b90-6fcff22c4713,"Language Processing , pages 832–841. ACL, August 2009."
Attention Is All You Need,2017-06-12,bd13c65f-9899-4e1d-99db-687ea73a0e6b,"[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring"
Attention Is All You Need,2017-06-12,6bd0b53f-53ee-4b76-906b-73836816f104,"the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016."
Attention Is All You Need,2017-06-12,04b74ce8-38e7-409c-96c1-873adf7591b8,[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural
Attention Is All You Need,2017-06-12,3f026e86-539a-4fc9-805d-651b7efd0e5b,"Information Processing Systems, (NIPS) , 2016."
Attention Is All You Need,2017-06-12,ada7e8f5-7b98-4c27-8c3d-b457037f8be2,[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference
Attention Is All You Need,2017-06-12,957ea4f1-8509-4d76-af9d-aa5fc2bff3f6,"on Learning Representations (ICLR) , 2016."
Attention Is All You Need,2017-06-12,4504f8a8-d053-41c8-9180-1da61ec4b6bc,"[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-"
Attention Is All You Need,2017-06-12,21988d04-e359-4652-9d15-304fb448a68a,"ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,"
Attention Is All You Need,2017-06-12,cac014ea-3a4f-413e-832d-1c5c94d885b3,2017.
Attention Is All You Need,2017-06-12,25ab3208-d462-408d-89c2-a4232962ec91,"[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks."
Attention Is All You Need,2017-06-12,024dbabd-4b5f-4928-910e-c2ee9189ce41,"InInternational Conference on Learning Representations , 2017."
Attention Is All You Need,2017-06-12,92af56d3-94b3-422b-aef7-97f57f54963b,"[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015."
Attention Is All You Need,2017-06-12,e1625139-61e2-4262-b26f-b034218fd522,[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint
Attention Is All You Need,2017-06-12,3a08ce95-0585-4cff-a969-2602706602be,"arXiv:1703.10722 , 2017."
Attention Is All You Need,2017-06-12,2639941a-1d23-4e3c-bcfc-b9d9fd30299f,"[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen"
Attention Is All You Need,2017-06-12,262c50d0-8ed7-4c8c-956b-43d50c6975e9,"Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint"
Attention Is All You Need,2017-06-12,b6ac0fb4-47db-4229-b447-91a5ca55e4f9,"arXiv:1703.03130 , 2017."
Attention Is All You Need,2017-06-12,82909b13-9dfa-4009-a70e-7d088eff24ab,"[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task"
Attention Is All You Need,2017-06-12,61c45f0b-5e5e-4a9d-bc26-a38f3bdc8bd4,"sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015."
Attention Is All You Need,2017-06-12,24501aaa-304b-4550-807e-f5528c95649b,"[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-"
Attention Is All You Need,2017-06-12,157c2079-2f2e-4fd3-a38e-0a73d48f5f50,"based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015."
Attention Is All You Need,2017-06-12,f9b883ea-b4a0-48a7-b5e1-4d5b0eb1aac8,11
Attention Is All You Need,2017-06-12,f5dfe3ba-13d9-4edd-ac56-a0058d994408,"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated"
Attention Is All You Need,2017-06-12,e77b1cdc-9554-47b5-9840-a8b892539313,"corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993."
Attention Is All You Need,2017-06-12,45e7dc46-0c4d-48d4-9d81-6b051ca5cb9b,"[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In"
Attention Is All You Need,2017-06-12,5f5feb2b-2ad2-41c9-b714-c311f91fb7df,"Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,"
Attention Is All You Need,2017-06-12,51b3de86-7244-424c-b970-371b80b7d8df,"pages 152–159. ACL, June 2006."
Attention Is All You Need,2017-06-12,27fa291c-6baa-4dd2-b4a0-9353b35e26fe,"[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention"
Attention Is All You Need,2017-06-12,3c10c6ca-e56e-46f3-939f-f3824f43f016,"model. In Empirical Methods in Natural Language Processing , 2016."
Attention Is All You Need,2017-06-12,305e571e-a13e-429b-b902-bb2b6a8ea59a,"[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive"
Attention Is All You Need,2017-06-12,a4ac409c-ce92-4b64-9ac9-628aa070aef7,"summarization. arXiv preprint arXiv:1705.04304 , 2017."
Attention Is All You Need,2017-06-12,5c456d22-01a3-46c2-bffd-ba8cb01c7b3d,"[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,"
Attention Is All You Need,2017-06-12,2aa333af-f601-4376-a3ce-617766598679,and interpretable tree annotation. In Proceedings of the 21st International Conference on
Attention Is All You Need,2017-06-12,f28d680c-d4b1-4825-b10b-394ff7e5b0d6,"Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July"
Attention Is All You Need,2017-06-12,3740d10b-ee22-48a0-a14a-7dddad1b3edf,2006.
Attention Is All You Need,2017-06-12,a483a3b5-3d55-4f61-9f4d-a82dfd44ac45,[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv
Attention Is All You Need,2017-06-12,65189636-3670-4488-8c14-b5542005d6a8,"preprint arXiv:1608.05859 , 2016."
Attention Is All You Need,2017-06-12,b13207dd-6324-4257-aa7d-e51e957d0c73,"[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words"
Attention Is All You Need,2017-06-12,9a08cbb7-777d-492a-9522-2abe84b283e6,"with subword units. arXiv preprint arXiv:1508.07909 , 2015."
Attention Is All You Need,2017-06-12,80398e07-c36a-4f22-99c8-cf3f1ae1d3c0,"[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,"
Attention Is All You Need,2017-06-12,a17cfb31-f637-46a8-8ee5-981224d58196,and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
Attention Is All You Need,2017-06-12,f2dbbcd7-500a-4c61-8e6d-3be91c67adb9,"layer. arXiv preprint arXiv:1701.06538 , 2017."
Attention Is All You Need,2017-06-12,c487ba84-fa1a-4df3-8068-354dde1e224e,"[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-"
Attention Is All You Need,2017-06-12,d17ce0ac-64dc-4c62-a406-1e284e37d0f0,nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine
Attention Is All You Need,2017-06-12,58adf50e-498c-4286-b78a-178b52e25137,"Learning Research , 15(1):1929–1958, 2014."
Attention Is All You Need,2017-06-12,88508703-0b46-4aab-b431-cb522eed3679,"[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory"
Attention Is All You Need,2017-06-12,f1e0dba7-d583-4d2c-919a-ecb6549ad757,"networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,"
Attention Is All You Need,2017-06-12,392b8a95-3588-44c4-b5c0-6a34dcdcdd6e,"Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,"
Attention Is All You Need,2017-06-12,c7f45482-480a-45e5-a39a-4d733be2266b,"Inc., 2015."
Attention Is All You Need,2017-06-12,1aa2eba7-dff1-4833-91c8-34d8f6282d13,"[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural"
Attention Is All You Need,2017-06-12,b487d5a2-a87f-49ef-8527-ff17893dcbdd,"networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014."
Attention Is All You Need,2017-06-12,02cf586e-292f-4fae-93d7-60a3f4fe4d69,"[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna."
Attention Is All You Need,2017-06-12,d43c4a02-6e8e-484f-8103-cd0b6992829a,"Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015."
Attention Is All You Need,2017-06-12,0626d0cb-1072-4860-b2f8-8af6c3f36d8d,"[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In"
Attention Is All You Need,2017-06-12,00aa6c05-edeb-4125-b4c3-8edd67f4c07b,"Advances in Neural Information Processing Systems , 2015."
Attention Is All You Need,2017-06-12,212ef42b-3dbf-46ba-8eb9-d797731a4b44,"[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang"
Attention Is All You Need,2017-06-12,b199a53e-77a3-4288-99bb-2e508789cc45,"Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine"
Attention Is All You Need,2017-06-12,292e80d0-19a2-4695-ac18-7b85ca323f48,translation system: Bridging the gap between human and machine translation. arXiv preprint
Attention Is All You Need,2017-06-12,cc9f80de-4f94-4c8b-b069-614eee800698,"arXiv:1609.08144 , 2016."
Attention Is All You Need,2017-06-12,7c006426-ab13-42a3-b219-c77e8010196c,"[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with"
Attention Is All You Need,2017-06-12,de36017e-58ab-4212-ac37-5ed420110e70,"fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016."
Attention Is All You Need,2017-06-12,53c13f2e-4000-4a49-b102-256e61e6265e,"[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate"
Attention Is All You Need,2017-06-12,89cab62c-50ba-4757-8933-d56b07affba9,shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume
Attention Is All You Need,2017-06-12,3de6e31f-27dd-49ae-8359-d069f7e7551c,"1: Long Papers) , pages 434–443. ACL, August 2013."
Attention Is All You Need,2017-06-12,7ffa5884-de71-49aa-a8cb-e81f2c0ce443,12
Attention Is All You Need,2017-06-12,78a8cd4b-3d8e-4726-bb17-54abc902929f,Attention Visualizations
Attention Is All You Need,2017-06-12,a5b7a289-ba61-45d2-ac8a-1a63fdf50fc6,Input-Input Layer5
Attention Is All You Need,2017-06-12,bb766774-d0b5-4c4e-8554-47f85fb4ec62,It
Attention Is All You Need,2017-06-12,9d5f2294-2226-4105-9ec2-f7da319a84a7,is
Attention Is All You Need,2017-06-12,0093d375-5b45-4e5b-8c35-de908ba42540,in
Attention Is All You Need,2017-06-12,760d01bf-5ecf-48a8-adc1-b588dadf47ae,this
Attention Is All You Need,2017-06-12,ded896f3-e959-4278-9dc0-ac7603521848,spirit
Attention Is All You Need,2017-06-12,35a24b34-3ab5-487e-9650-00e5e6af47a9,that
Attention Is All You Need,2017-06-12,57a4ca81-6f6b-4754-9c96-43feb4a3796c,a
Attention Is All You Need,2017-06-12,e744fa36-a5c6-4420-8739-01f53cdd3ead,majority
Attention Is All You Need,2017-06-12,4bfa722e-eb53-475d-90f8-f9bbe5721f25,of
Attention Is All You Need,2017-06-12,07422278-2f31-4e59-8948-638f4bb548f2,American
Attention Is All You Need,2017-06-12,956e577e-2196-48bf-b720-b444f5a2b314,governments
Attention Is All You Need,2017-06-12,c476d657-1b84-411b-bd11-3b371076dee1,have
Attention Is All You Need,2017-06-12,50dca9b2-dc4b-41fd-abb5-9413fc6b19e7,passed
Attention Is All You Need,2017-06-12,06a97b4c-c513-4969-86df-8ac047e749fe,new
Attention Is All You Need,2017-06-12,ee5634c1-81b5-4d15-b5ef-2f735e3a13d9,laws
Attention Is All You Need,2017-06-12,86432675-3fef-49ae-b253-fd83091d4888,since
Attention Is All You Need,2017-06-12,67e8902e-628e-4ccc-af9c-185a5332711d,2009
Attention Is All You Need,2017-06-12,a3984695-891d-495a-b8f0-b1c9c9666f24,making
Attention Is All You Need,2017-06-12,207fdc81-2bec-41b2-910c-20d0cf15ca37,the
Attention Is All You Need,2017-06-12,bbbc649f-b7d2-4441-824a-0d337168f555,registration
Attention Is All You Need,2017-06-12,436fd350-fb41-4895-a47c-206aa5f30c2b,or
Attention Is All You Need,2017-06-12,71d55e4e-b75d-4c8c-9f5a-9c73545875b6,voting
Attention Is All You Need,2017-06-12,383ee815-fc9c-4628-ba83-2c3fdef4fb61,process
Attention Is All You Need,2017-06-12,a8b2ad7d-e267-4977-a893-358af741072e,more
Attention Is All You Need,2017-06-12,54404b60-d8eb-42d6-af0f-adddc178ac2d,difficult
Attention Is All You Need,2017-06-12,429b02a0-f142-40cd-ad7a-e5745422a7fc,.
Attention Is All You Need,2017-06-12,9fa8d1a9-fd9f-4ca1-af13-a03d423d559c,<EOS>
Attention Is All You Need,2017-06-12,f490c9ca-a7ea-4b08-98b0-551b148c9d8c,<pad>
Attention Is All You Need,2017-06-12,784c8fd8-ef50-4763-bca3-ecac4daaa2bd,<pad>
Attention Is All You Need,2017-06-12,058b014a-a745-4349-bb1a-7527e39d4908,<pad>
Attention Is All You Need,2017-06-12,c7bd259b-9a74-49e4-9444-44c984265294,<pad>
Attention Is All You Need,2017-06-12,dd556239-fdd1-410c-82b2-a96c51d3370b,<pad>
Attention Is All You Need,2017-06-12,185e251e-db7d-4679-82b5-accd914dea71,<pad>
Attention Is All You Need,2017-06-12,2267834e-d31f-4a7b-950a-38ff1c7c0199,It
Attention Is All You Need,2017-06-12,1ccf0a28-965e-4c7b-8a14-fc5519c1359e,is
Attention Is All You Need,2017-06-12,c4a392d1-1640-474d-add9-b501865fdcc1,in
Attention Is All You Need,2017-06-12,cd892aed-2108-4fa9-b9a6-adc89f34d049,this
Attention Is All You Need,2017-06-12,458f78e3-a2ac-44e2-9807-daf7e68dc8f2,spirit
Attention Is All You Need,2017-06-12,ee4615af-7703-4cf4-999b-3b61d834a3e6,that
Attention Is All You Need,2017-06-12,8e0a801a-a007-4511-a86d-0a456368e07c,a
Attention Is All You Need,2017-06-12,5d84b715-f593-4ed9-b21e-4e382236bfd8,majority
Attention Is All You Need,2017-06-12,44ddff16-5e17-4ff2-9eba-d36481d30be7,of
Attention Is All You Need,2017-06-12,685e1dec-d512-4f13-9239-9c8fb98d205d,American
Attention Is All You Need,2017-06-12,7d4d62ef-d48a-4476-a38c-2534118597a1,governments
Attention Is All You Need,2017-06-12,75e48272-279e-4c91-8bc9-b32ba4a7ba1f,have
Attention Is All You Need,2017-06-12,191505ce-e659-4f4f-8237-e64be3777d30,passed
Attention Is All You Need,2017-06-12,2278ca28-6c2d-4a79-aa48-75debddcd42b,new
Attention Is All You Need,2017-06-12,9c7b3864-a2ca-4e35-b835-b0183461178f,laws
Attention Is All You Need,2017-06-12,3903d2d2-7ba9-462f-b714-861c4e4773e2,since
Attention Is All You Need,2017-06-12,8f3345cf-f777-41be-95dd-ffa7d0e37ad4,2009
Attention Is All You Need,2017-06-12,3e77e9e8-c6c4-450b-92c6-d5d4dd00cf09,making
Attention Is All You Need,2017-06-12,83f5c692-9e74-47b4-b91d-027dae2e4fd1,the
Attention Is All You Need,2017-06-12,6bb5b73f-7ffa-4cdf-80b0-74428b9b3562,registration
Attention Is All You Need,2017-06-12,b6ef5984-91e8-46c9-b3d0-22805a4c08f8,or
Attention Is All You Need,2017-06-12,3be05afa-d633-40d9-acc1-fba24466b54d,voting
Attention Is All You Need,2017-06-12,4fdebd35-dec8-4cc8-b36d-928fd8bd5609,process
Attention Is All You Need,2017-06-12,544d5476-7177-46b2-b13a-5c904c172456,more
Attention Is All You Need,2017-06-12,63f8a994-df37-456d-9e8b-3edbed36a594,difficult
Attention Is All You Need,2017-06-12,f7d04b52-1d3d-4525-9bd2-5b11c919571d,.
Attention Is All You Need,2017-06-12,2356d8b0-a514-4241-854e-0e028a67f870,<EOS>
Attention Is All You Need,2017-06-12,9e8cb53e-8f7e-4b2b-9ed8-1860fb1a1d12,<pad>
Attention Is All You Need,2017-06-12,c6f34ba8-48fc-420d-941e-93ec1b1425b8,<pad>
Attention Is All You Need,2017-06-12,028376b5-a5ae-4ba1-88bf-e6f9eb35482f,<pad>
Attention Is All You Need,2017-06-12,e7add8bf-61dc-41c5-a07d-3e5e67e9c1c5,<pad>
Attention Is All You Need,2017-06-12,cbaf70bb-c710-41ea-82f1-b60766b9cbf1,<pad>
Attention Is All You Need,2017-06-12,324a0bb2-3d40-440b-86f7-0ab212dc8c26,<pad>
Attention Is All You Need,2017-06-12,525e4724-8a27-41bc-a202-5f44e7b7eff3,Figure 3: An example of the attention mechanism following long-distance dependencies in the
Attention Is All You Need,2017-06-12,d64c820f-601d-4ab9-a971-e5e922b9cc9e,encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
Attention Is All You Need,2017-06-12,8f8178f0-0fc5-4ed5-9c86-c0cc4bfed4e2,"the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for"
Attention Is All You Need,2017-06-12,a05a538b-ebca-4cc5-8f33-be6be2f7897e,the word ‘making’. Different colors represent different heads. Best viewed in color.
Attention Is All You Need,2017-06-12,17540e1e-d4b5-47ed-89b8-229431c92788,13
Attention Is All You Need,2017-06-12,eafd353e-7472-4d21-8d10-545498db1cf8,Input-Input Layer5
Attention Is All You Need,2017-06-12,2cd61b16-4e35-4cf5-903c-e32da63097d9,The
Attention Is All You Need,2017-06-12,8ad297a3-6bd9-4e90-8d32-9fac9177f37b,Law
Attention Is All You Need,2017-06-12,6e0dd6c9-7eaa-4cde-a84e-4b6500ffadaa,will
Attention Is All You Need,2017-06-12,7ab26f99-4e5d-4ea3-ba7f-b1d63006abd8,never
Attention Is All You Need,2017-06-12,b96567ac-8d56-4285-8053-08fbc88b0457,be
Attention Is All You Need,2017-06-12,189dc646-75cf-4ceb-be36-9fd91661fbef,perfect
Attention Is All You Need,2017-06-12,786d8186-eb30-4c36-b9c6-39a11bda0f32,","
Attention Is All You Need,2017-06-12,a1dd785e-f297-470e-b4f3-57af0b2fa382,but
Attention Is All You Need,2017-06-12,08728dd2-1974-400a-85c3-89dda8b669ef,its
Attention Is All You Need,2017-06-12,8928779c-ffff-41b0-b852-2d408c3b5eff,application
Attention Is All You Need,2017-06-12,320d0b70-0747-46c1-96e5-c9352db38231,should
Attention Is All You Need,2017-06-12,18420add-25e2-4789-ba2e-535db0d7f30c,be
Attention Is All You Need,2017-06-12,fafb94f9-9aec-4fc3-8f8d-0344ef864c3e,just
Attention Is All You Need,2017-06-12,17824dd9-4ed3-4a5c-8d33-d150d239947b,-
Attention Is All You Need,2017-06-12,ff036eb7-f6ef-4e4e-90b1-f46378f4ebbe,this
Attention Is All You Need,2017-06-12,bcf4544c-8509-472d-af2c-b68fbbb54a3a,is
Attention Is All You Need,2017-06-12,e2026256-7b63-4652-a31f-af74a212f6b8,what
Attention Is All You Need,2017-06-12,3fff984d-ee33-451f-95de-3255cce56273,we
Attention Is All You Need,2017-06-12,72dbcc72-5e00-4270-a893-666a6b4c074c,are
Attention Is All You Need,2017-06-12,0ba446cb-7a23-41e6-931e-dd8233998029,missing
Attention Is All You Need,2017-06-12,a211b317-54aa-4875-9863-e374a53ea9ce,","
Attention Is All You Need,2017-06-12,cabdf2a3-08b5-47ca-81fe-e3e4a528099f,in
Attention Is All You Need,2017-06-12,404e8316-b77e-4f6e-abe6-24f6275cbc4b,my
Attention Is All You Need,2017-06-12,87950b42-c9b4-4781-a1f7-5fe2a3da4ba2,opinion
Attention Is All You Need,2017-06-12,129e56b5-cba6-4053-9190-1d409b98e867,.
Attention Is All You Need,2017-06-12,5b891fdf-45ec-47eb-afb5-d38475c81aae,<EOS>
Attention Is All You Need,2017-06-12,cfa5cde5-437b-4f22-a6e5-ad296a7a448b,<pad>
Attention Is All You Need,2017-06-12,348fa4c1-07b4-4426-a866-a5cbb594c507,The
Attention Is All You Need,2017-06-12,d7ee2618-2a9f-4214-b892-65b23554424c,Law
Attention Is All You Need,2017-06-12,8787b5b7-cca0-46ea-8691-212470d16520,will
Attention Is All You Need,2017-06-12,4f4d97ec-3bbd-40e1-8381-959f179ead99,never
Attention Is All You Need,2017-06-12,a196f9af-946d-4585-a5dc-0f147b0af087,be
Attention Is All You Need,2017-06-12,98e0f7e6-6393-4b5a-8a8c-ce9caea291f3,perfect
Attention Is All You Need,2017-06-12,d1076a40-41f5-498b-9eaa-8ad8ed8e6a92,","
Attention Is All You Need,2017-06-12,5d8f7dd9-e641-4a8f-aca1-07183d97712a,but
Attention Is All You Need,2017-06-12,efe34e70-2e3e-44fd-b8a2-27fb267e5863,its
Attention Is All You Need,2017-06-12,c7318385-ac97-4795-8f75-d33431ed065a,application
Attention Is All You Need,2017-06-12,37c1a07c-9c6a-475c-b30e-b6893f342c98,should
Attention Is All You Need,2017-06-12,bc0775a7-8351-4119-9409-a3b409d53f19,be
Attention Is All You Need,2017-06-12,a1be03d9-cdca-472f-a4ac-0f4ea24170e6,just
Attention Is All You Need,2017-06-12,5fd3c8d3-666a-4e06-853b-b00ec11e084f,-
Attention Is All You Need,2017-06-12,88dd7332-96e6-4c67-8ac7-7c5e93cbb5f7,this
Attention Is All You Need,2017-06-12,978eb46f-0470-47a4-a236-833e75de8c3b,is
Attention Is All You Need,2017-06-12,803dc975-9e72-4bfb-8a33-3497b3215ce7,what
Attention Is All You Need,2017-06-12,aa094ff1-6d91-4d11-889a-001cd355ca35,we
Attention Is All You Need,2017-06-12,b1d45474-9e35-4684-a7fc-6f1fc6ae99f2,are
Attention Is All You Need,2017-06-12,036d0595-aae3-4981-81f5-d90a811cf2ee,missing
Attention Is All You Need,2017-06-12,32e57f51-3b10-4e81-909b-628ebfceacc9,","
Attention Is All You Need,2017-06-12,fd13227b-5767-4c36-b259-cc5302d56be6,in
Attention Is All You Need,2017-06-12,9873bc37-0ff6-46e1-9362-92247763f070,my
Attention Is All You Need,2017-06-12,13811f83-4ac3-4822-bf29-67161ab85187,opinion
Attention Is All You Need,2017-06-12,4dbdc0d2-dc6b-4568-9c13-ab1a29d3791b,.
Attention Is All You Need,2017-06-12,db1a5977-9147-4585-931d-c441ced787f3,<EOS>
Attention Is All You Need,2017-06-12,6f3784ad-ac0e-4b61-b4c4-4500aa881829,<pad>
Attention Is All You Need,2017-06-12,5c7698aa-dbe7-468c-b181-f199a29a98f8,Input-Input Layer5
Attention Is All You Need,2017-06-12,78a233fb-08d1-409c-a744-6c34910d5faf,The
Attention Is All You Need,2017-06-12,7eb83703-af68-4143-9d44-5b225d65f28c,Law
Attention Is All You Need,2017-06-12,ee32ed77-d8a3-42bb-84fb-b71d6c6c3786,will
Attention Is All You Need,2017-06-12,668b9906-f23e-4cb2-a043-5f811873bea8,never
Attention Is All You Need,2017-06-12,e83e6646-9516-411b-8fb8-fa1403fc4f1e,be
Attention Is All You Need,2017-06-12,967cbb6d-0231-4fad-985d-f8697d34a544,perfect
Attention Is All You Need,2017-06-12,853b2b35-dc7a-4582-adcb-5c7fe74e43b8,","
Attention Is All You Need,2017-06-12,8170e27f-b1d8-4e63-9222-ab9cbdc0b005,but
Attention Is All You Need,2017-06-12,707a1349-872d-407a-903b-cfa85a078243,its
Attention Is All You Need,2017-06-12,5fa93616-9b85-46f6-b725-015f8d2fc322,application
Attention Is All You Need,2017-06-12,29290d8d-f96d-4470-b85f-fedb0e64c822,should
Attention Is All You Need,2017-06-12,5de73415-4a7f-44cd-b6dc-c4a9c9fcfebe,be
Attention Is All You Need,2017-06-12,cc2e8b95-f64e-42c6-bf47-b79005ccfa5e,just
Attention Is All You Need,2017-06-12,1c037403-19bc-4dd8-9200-aced0f085c07,-
Attention Is All You Need,2017-06-12,721d5316-4f67-4a8a-8cbc-1bbc1c120865,this
Attention Is All You Need,2017-06-12,827a4c14-89c8-40c1-95ae-7113b30ae4c9,is
Attention Is All You Need,2017-06-12,e84e6962-4919-4585-927c-a4b27d456eee,what
Attention Is All You Need,2017-06-12,ed96e2e4-ab62-4557-a6f8-09288fa1c8a7,we
Attention Is All You Need,2017-06-12,e275bdc9-b27c-45d6-a28e-539f03382e54,are
Attention Is All You Need,2017-06-12,a4f4517e-b808-44fe-9d0b-080cb85b21c3,missing
Attention Is All You Need,2017-06-12,16010731-dade-49ca-b586-eaef2b37cc1c,","
Attention Is All You Need,2017-06-12,3a462bbc-86c0-47be-95a9-a40cef7e1f18,in
Attention Is All You Need,2017-06-12,b0c4ced5-ab44-4e89-96cc-7ed0aac63f3a,my
Attention Is All You Need,2017-06-12,7c537898-b738-42bf-9f34-2f88e8d0adf4,opinion
Attention Is All You Need,2017-06-12,864fbfda-a066-43cb-82b0-6660d0dc2c1c,.
Attention Is All You Need,2017-06-12,0367cc00-bfe0-4379-9847-11a7299a352e,<EOS>
Attention Is All You Need,2017-06-12,2c52f67b-4091-47ba-bc93-aeef1cdc8e86,<pad>
Attention Is All You Need,2017-06-12,1336cc44-bee5-47b8-91d1-53040f57f26b,The
Attention Is All You Need,2017-06-12,b6d3719f-c0e9-4b86-8f4c-bda28152587d,Law
Attention Is All You Need,2017-06-12,e3b4afa3-0884-414e-a9c1-eb81dff051f9,will
Attention Is All You Need,2017-06-12,da485be5-b092-4555-9803-a2fe9ed9d06b,never
Attention Is All You Need,2017-06-12,a99f2823-bbd0-49d4-9b9c-fb49ac1339d6,be
Attention Is All You Need,2017-06-12,f582300c-37e7-4a56-bd37-f9b8eb7473f4,perfect
Attention Is All You Need,2017-06-12,4166b16a-97bc-452e-81ce-446c7175c9f1,","
Attention Is All You Need,2017-06-12,9efeda00-d498-4115-bab1-f9cf4f148d6e,but
Attention Is All You Need,2017-06-12,bac19713-4103-4f90-a761-bc9ac9605388,its
Attention Is All You Need,2017-06-12,5f190fa3-d806-4d22-837c-76db9537c8c2,application
Attention Is All You Need,2017-06-12,eff1c50a-bb9e-4dcc-958d-c59a53c5c27d,should
Attention Is All You Need,2017-06-12,d1a4d58d-e67a-45ed-b94e-eb6b29d3e7ee,be
Attention Is All You Need,2017-06-12,a42158a1-704a-44be-82ff-b305058d24f4,just
Attention Is All You Need,2017-06-12,a68a7d15-e7b6-4981-9963-e931447b331b,-
Attention Is All You Need,2017-06-12,9bc1516c-1596-4230-bc69-0cd858b2df06,this
Attention Is All You Need,2017-06-12,43bd44aa-8e08-4aec-873f-b9049b0bdba1,is
Attention Is All You Need,2017-06-12,a3dffc67-4e92-478b-9d1b-1d1297dfab85,what
Attention Is All You Need,2017-06-12,de881082-d7d4-421f-90f9-58c080f65094,we
Attention Is All You Need,2017-06-12,79049ba4-65a2-45d6-b0df-c38d5b766205,are
Attention Is All You Need,2017-06-12,74500dc2-3a7a-4a2d-a16b-bc8b0ed3e161,missing
Attention Is All You Need,2017-06-12,1ea9ada9-3c11-4eb3-95fb-bdf7bfcd94eb,","
Attention Is All You Need,2017-06-12,a7a2ab3c-9f30-4c7e-b6fa-5949b9770ce8,in
Attention Is All You Need,2017-06-12,4d0e6aa7-2e52-4cd8-a947-bf8ac729e5de,my
Attention Is All You Need,2017-06-12,13977912-9d14-42fd-9e8f-328a95e258f4,opinion
Attention Is All You Need,2017-06-12,88535671-2348-4110-8f8e-cdc373857c5a,.
Attention Is All You Need,2017-06-12,f482a3b3-22b7-47f2-82ef-21efb07a5015,<EOS>
Attention Is All You Need,2017-06-12,f32b0caa-9c68-414d-962b-556157b70c34,"<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:"
Attention Is All You Need,2017-06-12,8fa326e0-043b-4925-911b-9b0abc8877aa,Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5
Attention Is All You Need,2017-06-12,37fa71ce-e9a3-4c92-8ca9-e46ff30bad8f,and 6. Note that the attentions are very sharp for this word.
Attention Is All You Need,2017-06-12,22ed2848-4034-47d7-bc86-682406b4c15f,14
Attention Is All You Need,2017-06-12,05cb9a8f-9812-46b2-9e84-b9cd5378c65d,Input-Input Layer5
Attention Is All You Need,2017-06-12,1b4d01c1-8ed5-45ce-a314-e1cf71589888,The
Attention Is All You Need,2017-06-12,1fc9b5ac-150a-4f73-af9a-bb56afc4f0cd,Law
Attention Is All You Need,2017-06-12,9dfac8b4-53d2-4793-be67-6294715b9516,will
Attention Is All You Need,2017-06-12,042255f4-d2ab-46e2-93d7-431c1cc4b47e,never
Attention Is All You Need,2017-06-12,2ba48430-6490-4f67-91ef-efd99ee5135e,be
Attention Is All You Need,2017-06-12,d5fb7f9c-16a1-4b11-8c92-945689071047,perfect
Attention Is All You Need,2017-06-12,4c22a92f-dc22-4858-a221-161436154212,","
Attention Is All You Need,2017-06-12,17f2f7cb-eba2-4165-80b6-39861826bbfe,but
Attention Is All You Need,2017-06-12,fc42c371-0ebc-4695-bb7c-e7aa7f3c189f,its
Attention Is All You Need,2017-06-12,cff07ebd-c29a-499b-8a6a-1c7e07fc6dba,application
Attention Is All You Need,2017-06-12,26ecb69c-c124-4f33-bde1-3a32ac05863c,should
Attention Is All You Need,2017-06-12,284bfbc4-afb8-4a8e-bb96-56db2b6e4e65,be
Attention Is All You Need,2017-06-12,8eefaf92-e25c-454c-b26b-c079b436465f,just
Attention Is All You Need,2017-06-12,4daee93c-3516-4675-a9fe-146b6da92ab4,-
Attention Is All You Need,2017-06-12,94e8cda6-a3fe-4028-9f28-d66f2a0c7c5c,this
Attention Is All You Need,2017-06-12,7922ae63-d578-467b-810b-87f5b2907feb,is
Attention Is All You Need,2017-06-12,55a83b57-0203-4acd-93d0-79aa205c88b7,what
Attention Is All You Need,2017-06-12,15ce3b49-2c43-44a8-b3bc-caa3766ee82f,we
Attention Is All You Need,2017-06-12,a93c60d4-cf81-467e-ad8c-401e789bdbe6,are
Attention Is All You Need,2017-06-12,17e2c20f-f10a-4593-975e-51e67a5e0785,missing
Attention Is All You Need,2017-06-12,5f4ee093-397b-4dfa-97a9-0a0f8a146236,","
Attention Is All You Need,2017-06-12,db31b0e7-3c29-47f5-b4d9-79b08191cf65,in
Attention Is All You Need,2017-06-12,33c8cf7b-7577-4236-8754-eb2d182ea5f6,my
Attention Is All You Need,2017-06-12,b405e4eb-b1ff-4e9c-b750-ecba6c983e37,opinion
Attention Is All You Need,2017-06-12,a73b0ee0-66e8-4c01-aa4e-19c3425e514b,.
Attention Is All You Need,2017-06-12,69060c20-b90b-4864-b628-d17e289536d9,<EOS>
Attention Is All You Need,2017-06-12,93edddfe-2ba6-4fd7-b71f-399029c11ee3,<pad>
Attention Is All You Need,2017-06-12,c20c4c8d-3df2-402c-a310-57075e25808f,The
Attention Is All You Need,2017-06-12,80595313-a3cb-45e2-9bdc-f0ddfda78560,Law
Attention Is All You Need,2017-06-12,bebde481-f269-45fa-92c4-2e8e7587563d,will
Attention Is All You Need,2017-06-12,f7370635-f2a6-44ef-b15e-cf9c41677777,never
Attention Is All You Need,2017-06-12,9917ed6d-aabd-4990-89f1-83ff94296781,be
Attention Is All You Need,2017-06-12,17994856-a949-49ca-8ad6-92630055c435,perfect
Attention Is All You Need,2017-06-12,283e0bf8-34ef-4b86-95bf-57a313577ac0,","
Attention Is All You Need,2017-06-12,e1e1ee48-d612-4f7b-89bf-c9682a2857c2,but
Attention Is All You Need,2017-06-12,0deb551f-c004-4d35-8d11-111fdced3d9f,its
Attention Is All You Need,2017-06-12,b36bcb08-2464-413d-a6c5-761369e59473,application
Attention Is All You Need,2017-06-12,dc4a7908-3e0f-4109-a0db-50625c7110ec,should
Attention Is All You Need,2017-06-12,e9bb0435-801f-431e-a575-6b8b9b2f0939,be
Attention Is All You Need,2017-06-12,2d83fee5-70d8-4216-8d0a-4db2f4783ff9,just
Attention Is All You Need,2017-06-12,839d4309-14f5-43f3-b764-8ff416d687c9,-
Attention Is All You Need,2017-06-12,df07301e-4819-4f89-bade-3f37f1c98360,this
Attention Is All You Need,2017-06-12,5c07b294-de4a-4447-a38e-a1ce236168c8,is
Attention Is All You Need,2017-06-12,c742dd62-47bc-49fd-ba3a-129f17a39698,what
Attention Is All You Need,2017-06-12,952984f9-52d1-4a85-9772-10a14182dcec,we
Attention Is All You Need,2017-06-12,988b9796-41bb-4c14-8ae1-a09500cfd54d,are
Attention Is All You Need,2017-06-12,c11d338b-8b75-4ab7-bdf3-3529ce403d9e,missing
Attention Is All You Need,2017-06-12,7455e1c6-b2f0-4299-8f45-a554872e0bcf,","
Attention Is All You Need,2017-06-12,bd616450-7e5a-4679-8ae3-e84ef55f6edb,in
Attention Is All You Need,2017-06-12,5595686d-99b1-4d42-8227-c685eef2f3d2,my
Attention Is All You Need,2017-06-12,48ece3f7-0dab-44b8-a50c-101c52555b48,opinion
Attention Is All You Need,2017-06-12,a906eeb3-7632-4d62-aa7d-f28f4af58e44,.
Attention Is All You Need,2017-06-12,740e48d8-19c1-44a5-8b7c-6f48fc3338b6,<EOS>
Attention Is All You Need,2017-06-12,94812148-884f-4cfd-9ba7-1509877bb508,<pad>
Attention Is All You Need,2017-06-12,c4aec974-a1b9-462d-8b1c-1b817a4391dc,Input-Input Layer5
Attention Is All You Need,2017-06-12,46fac14f-f943-4c11-b29e-ce43c8b55cfb,The
Attention Is All You Need,2017-06-12,45263faa-a145-4c80-9459-c421db2c8ae3,Law
Attention Is All You Need,2017-06-12,c2f9ecef-dba0-450a-b47f-011b2e6069c8,will
Attention Is All You Need,2017-06-12,546b6aec-8cf7-4ed8-8857-079ec596fc26,never
Attention Is All You Need,2017-06-12,09603858-6b29-4be6-91bd-ef1fdca1748b,be
Attention Is All You Need,2017-06-12,68cb9aa8-fc41-4cdd-bffc-d1a0c393fbb3,perfect
Attention Is All You Need,2017-06-12,792e4690-3a68-42ad-b388-e2d1722e6087,","
Attention Is All You Need,2017-06-12,a2aea7e3-dbd5-46b4-8737-03d523ec929f,but
Attention Is All You Need,2017-06-12,71fdeab7-5a36-40d2-b49e-4ac1c77ee243,its
Attention Is All You Need,2017-06-12,5b4d1c6e-6034-404d-a6a8-599baacb6037,application
Attention Is All You Need,2017-06-12,ad4da4e0-1421-454f-ba46-6e0b43ae4082,should
Attention Is All You Need,2017-06-12,0231289e-ffbc-4278-95f6-acb1080f3149,be
Attention Is All You Need,2017-06-12,236faca5-c552-4150-b1e9-165fef90c2b4,just
Attention Is All You Need,2017-06-12,5c6574d3-74f4-4b9d-a0a5-bc8e37df1fa6,-
Attention Is All You Need,2017-06-12,b48021b4-6f37-45dd-b252-1ee73ae617b2,this
Attention Is All You Need,2017-06-12,904a0b90-5868-46ad-a7a2-ece087c3092e,is
Attention Is All You Need,2017-06-12,4abe2056-cb7d-4078-bb4a-d874117d2388,what
Attention Is All You Need,2017-06-12,776a0fc9-c3ae-477c-b272-d9e2b6e8547b,we
Attention Is All You Need,2017-06-12,bcebc722-2c5c-4d2d-94b6-5224f369043c,are
Attention Is All You Need,2017-06-12,5d9060ba-8fbe-4199-89e7-ca56ad1c6a98,missing
Attention Is All You Need,2017-06-12,a27c78ae-7889-47ce-b019-39c5a7e1c53f,","
Attention Is All You Need,2017-06-12,6507830d-f539-4543-87e9-c59a92d91ff6,in
Attention Is All You Need,2017-06-12,a83d08b5-5706-4152-8597-cb13e54016a0,my
Attention Is All You Need,2017-06-12,b9a84522-2117-42b7-86fb-e1f2f81310bc,opinion
Attention Is All You Need,2017-06-12,619a64ad-b0c2-467d-a76a-adb78eb907f9,.
Attention Is All You Need,2017-06-12,d4ba009f-6011-4030-b0e6-42454358c408,<EOS>
Attention Is All You Need,2017-06-12,91ed1ebc-b665-450d-9a99-7873f4379f16,<pad>
Attention Is All You Need,2017-06-12,4ec7739f-b1ad-4e27-8e14-91fe40365068,The
Attention Is All You Need,2017-06-12,c3b20df9-9c4d-4c45-98a9-0178cf4486df,Law
Attention Is All You Need,2017-06-12,14c5ad09-a45f-4563-846e-b1be5277afba,will
Attention Is All You Need,2017-06-12,18a9d17a-a882-4e52-bf1f-0051483f7e4f,never
Attention Is All You Need,2017-06-12,864839ca-598b-4cae-bfbd-ae04d9f6896e,be
Attention Is All You Need,2017-06-12,857a68d1-3154-4b0f-a518-5cb533ea1310,perfect
Attention Is All You Need,2017-06-12,18ef7b49-c66d-4d0c-b32c-a6494c87e3ec,","
Attention Is All You Need,2017-06-12,c0795620-4e85-43e1-8004-56009640cef4,but
Attention Is All You Need,2017-06-12,58b3fb24-cf6d-4615-8919-d9800cabb170,its
Attention Is All You Need,2017-06-12,a6e784d4-37b4-48fb-940e-80227cc88b4f,application
Attention Is All You Need,2017-06-12,0918bfd6-17c6-461d-ac15-ec25cd30f891,should
Attention Is All You Need,2017-06-12,1af9efff-df39-4660-a51c-2e76d654dcb6,be
Attention Is All You Need,2017-06-12,4ae8e2eb-ee47-48a6-be82-53ac89b6bd7c,just
Attention Is All You Need,2017-06-12,1c31ad74-4e10-4bbd-903b-a6f42c7a6de0,-
Attention Is All You Need,2017-06-12,4f6891c7-1be4-4cd5-84f0-ee4f5a6aceb9,this
Attention Is All You Need,2017-06-12,e5e193e9-6c6b-4975-ade8-440144b936e3,is
Attention Is All You Need,2017-06-12,1e7c2cd2-1e1a-45cf-9478-c88650fdfbd8,what
Attention Is All You Need,2017-06-12,7586d8ed-474a-4efc-840d-4e8b94a62c52,we
Attention Is All You Need,2017-06-12,03b895e2-170a-41c2-ad7a-5019dc74c6aa,are
Attention Is All You Need,2017-06-12,8cc2bea7-972f-4164-b2f9-02e4626d9bfc,missing
Attention Is All You Need,2017-06-12,e8232950-2763-4633-b4b0-b1c490a68ecc,","
Attention Is All You Need,2017-06-12,dbaba903-60a8-4aaa-a034-1da2064408a2,in
Attention Is All You Need,2017-06-12,809c40c0-a901-471c-8ece-cfafb14960c9,my
Attention Is All You Need,2017-06-12,679b6955-0e8b-4df4-9a56-f7f18b03bab1,opinion
Attention Is All You Need,2017-06-12,d73e3164-90e5-431d-8732-8afe7840710c,.
Attention Is All You Need,2017-06-12,6420258b-dd7c-4007-9e50-637004c2e5ff,<EOS>
Attention Is All You Need,2017-06-12,5aa0cc72-4b62-4baa-91e1-c38e5bf544d5,<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
Attention Is All You Need,2017-06-12,c4b6ef88-a9cd-4f2c-9828-a246613d0d46,"sentence. We give two such examples above, from two different heads from the encoder self-attention"
Attention Is All You Need,2017-06-12,bbeb74df-4b13-4d12-bd7b-cfadc89b88ff,at layer 5 of 6. The heads clearly learned to perform different tasks.
Attention Is All You Need,2017-06-12,40872f91-c80c-4c73-8b91-632c2577291b,15
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,28ea0836-c225-4954-8aa0-b6e73861abc0,BERT: Pre-training of Deep Bidirectional Transformers for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,04cae785-0738-4de9-8483-11ebdded2aa4,Language Understanding
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6927a273-4a79-4778-8e98-9cef120ed4dc,Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,29b2439b-964c-4b75-a652-c732d7adff2d,Google AI Language
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6d05c6c4-276e-4859-90b9-d73561303dab,"fjacobdevlin,mingweichang,kentonl,kristout g@google.com"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ff8bff38-bdd6-4350-988f-991170296cdd,Abstract
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e3b47a1-3686-4ef4-a7c1-f5dc36b40cef,We introduce a new language representa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,80b6f1f2-28bb-4628-af62-e67f119c7b0e,"tion model called BERT , which stands for"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,205a42a7-b7c0-4fa9-87b2-11b868c3bb6b,Bidirectional Encoder Representations from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,264b14c9-4c3c-4685-bf77-f3dd15d0157e,Transformers. Unlike recent language repre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,20e917f1-2255-4946-be7e-e686b561915a,"sentation models (Peters et al., 2018a; Rad-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3c2b2c55-af7b-4307-8afa-a31581e26dab,"ford et al., 2018), BERT is designed to pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8474849-1f29-4642-a1b5-99b701b0ac5b,train deep bidirectional representations from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,17201798-f778-442a-a08f-c831ef0f8376,unlabeled text by jointly conditioning on both
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,90afcc10-fcf0-42da-b424-9b178f31bdf9,left and right context in all layers. As a re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc2eb048-86cd-40f1-90e8-f145d194f453,"sult, the pre-trained BERT model can be ﬁne-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2fde7e78-c00b-4237-98f4-939efdbd85b5,tuned with just one additional output layer
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6cfbecc0-eeb3-4c54-abe8-90b5d63248b6,to create state-of-the-art models for a wide
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9782d067-b64e-4c94-8220-f1c2764adf39,"range of tasks, such as question answering and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9481522d-570d-4264-b7cc-73d3e948b297,"language inference, without substantial task-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,34d32788-6f7c-4cfa-8ba2-145226f38f3f,speciﬁc architecture modiﬁcations.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eaf800b4-2928-49a2-8e64-c270a3afa573,BERT is conceptually simple and empirically
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8817a7e2-bc23-4217-99d4-a7b38c60425b,powerful. It obtains new state-of-the-art re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae3a509c-6a78-4a55-b0b5-400dd55b9665,sults on eleven natural language processing
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7b3b41ae-3be9-420b-87fe-1a133d833efa,"tasks, including pushing the GLUE score to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c28dd838-2bab-4d77-b7af-a76ae19815c3,"80.5% (7.7% point absolute improvement),"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,272fd18b-d51e-4ffc-8f90-9e8071364cb2,MultiNLI accuracy to 86.7% (4.6% absolute
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0562445d-102b-4090-9d96-a869f6dae44a,"improvement), SQuAD v1.1 question answer-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9ffe3606-93b5-4233-8921-d763eadcf0e1,ing Test F1 to 93.2 (1.5 point absolute im-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a0c48e91-fec0-4ad8-af5e-d4c8b72d8344,provement) and SQuAD v2.0 Test F1 to 83.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,28af6c0e-89c0-4180-80ce-6af2b503c61e,(5.1 point absolute improvement).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,61a0e774-481d-47d9-8141-a73d28aead6b,1 Introduction
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99c8a24a-26ef-4515-9911-ae39a23683fd,Language model pre-training has been shown to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d69f7dfe-a42a-4065-8a69-482211265e91,be effective for improving many natural language
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,76da8318-db5a-493f-b064-79754e5dc960,"processing tasks (Dai and Le, 2015; Peters et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,03424b5f-8e78-4198-9470-10bab1a434bf,"2018a; Radford et al., 2018; Howard and Ruder,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,72f02648-6d3e-441f-a096-93b54f592dc1,2018). These include sentence-level tasks such as
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c98a431e-d19d-44ef-9ea0-2b9db3c9cb6d,"natural language inference (Bowman et al., 2015;"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fe2fbefc-0669-4e2b-9c55-d7c9f3e493a5,"Williams et al., 2018) and paraphrasing (Dolan"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,24a7673a-27b2-4b46-a546-deeb5219d311,"and Brockett, 2005), which aim to predict the re-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1aa79a87-d6c2-47d7-ba7e-83dc1df9b604,lationships between sentences by analyzing them
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3c0eaf10-8082-47b6-bafc-a915422c81cb,"holistically, as well as token-level tasks such as"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a587a76b-a051-4129-93a6-4911635c18bf,"named entity recognition and question answering,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0047e36d-b927-490e-a1b3-8d6385f81def,where models are required to produce ﬁne-grained
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,62dcf016-bb07-4a63-a7f3-cc474d074220,output at the token level (Tjong Kim Sang and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,32c70c87-3ffd-4940-91bd-2fea89211637,"De Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dee87fec-a604-4017-b1b4-d82b68068053,ing pre-trained language representations to down-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08269330-c45d-4d81-abc4-094ebb273b76,stream tasks: feature-based andﬁne-tuning . The
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5370ae7-3af6-4838-9450-1b3e64fd0c92,"feature-based approach, such as ELMo (Peters"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,389e698e-a206-44bc-b1fd-9c6d6f742ed1,"et al., 2018a), uses task-speciﬁc architectures that"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b274c4ab-5ad3-416c-a896-301332ad2017,include the pre-trained representations as addi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,581d4ba2-10c5-464d-87c9-95af3b7a698c,"tional features. The ﬁne-tuning approach, such as"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cea75e53-dd56-400c-9a7a-db1d17c50ffd,the Generative Pre-trained Transformer (OpenAI
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0776ce98-79c9-4f4e-be65-cf33a5b7478f,"GPT) (Radford et al., 2018), introduces minimal"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fcf0848f-d52b-4f58-aa00-0b13a1b5aeb0,"task-speciﬁc parameters, and is trained on the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,20a880d6-d677-452f-a652-26ed91ac45a5,downstream tasks by simply ﬁne-tuning allpre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,93a00770-7f03-4b4e-a62c-a6022af593fe,trained parameters. The two approaches share the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,45b58c53-10c8-4964-81e3-bbd5915204ad,"same objective function during pre-training, where"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,971ac063-4033-4319-b353-81d7ae518760,they use unidirectional language models to learn
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8659c315-95bc-44b1-9730-1260e3914748,general language representations.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ebb08f71-ee8a-4b43-9759-9aac77326ddf,We argue that current techniques restrict the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a33cdae9-d02a-4774-89c2-8692059db082,"power of the pre-trained representations, espe-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0b85e522-a091-48ac-af95-bea0262902d4,cially for the ﬁne-tuning approaches. The ma-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a43c53a9-1791-493e-938a-bfb12288e356,jor limitation is that standard language models are
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e1c676a9-c2ef-4a0f-8081-a59a1abe5d51,"unidirectional, and this limits the choice of archi-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5675c240-6020-4a8a-af3e-0b87db34dda9,tectures that can be used during pre-training. For
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a81e7935-2fb6-4aaa-a2ef-1bf30967ecd2,"example, in OpenAI GPT, the authors use a left-to-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e119af3-c8d4-49a7-8aa4-8ac3d0f712a6,"right architecture, where every token can only at-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,95d5cfcb-89f3-4860-9bf6-13b58a0982fe,tend to previous tokens in the self-attention layers
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,50676f0b-53d6-45fd-afd4-26dd8289348f,"of the Transformer (Vaswani et al., 2017). Such re-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e2aacfd9-cff3-42b2-8d5f-6fd113726ddd,"strictions are sub-optimal for sentence-level tasks,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,718cff88-c098-46db-921f-f1c77ab7598d,and could be very harmful when applying ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5ed80e2e-79f2-43d7-a9d0-d31f32a39684,tuning based approaches to token-level tasks such
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3bed6673-5674-454c-a2f4-caab603eb689,"as question answering, where it is crucial to incor-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e1cd16a-85db-4bb9-be62-208ddb36d057,porate context from both directions.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f510391d-c011-4ff5-b862-7d9cc22dfdf5,"In this paper, we improve the ﬁne-tuning based"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,332f8632-4616-44dc-918a-6697fef8e6b6,approaches by proposing BERT: Bidirectional
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,02eaec1e-7b81-4ce1-849e-abec05593dba,Encoder Representations from Transformers.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ce44c56b-101c-4379-9e81-176eb23cd6a0,BERT alleviates the previously mentioned unidi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c0287fc-34d2-4c4d-b33e-c068b4d67aec,rectionality constraint by using a “masked lan-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6321ba94-5486-4393-b725-6a9bad47b87e,"guage model” (MLM) pre-training objective, in-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b7840331-e997-407d-828a-ba31223c9032,"spired by the Cloze task (Taylor, 1953). The"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fc160006-1ca0-435c-9377-47a9392cacaa,masked language model randomly masks some of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e9e39ff5-197d-4c74-863c-61cfd1c3ca42,"the tokens from the input, and the objective is to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8e725e28-3929-494d-b3e4-79d9f15471a2,predict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9383df34-4689-425d-8263-ed6aa8d741c0,word based only on its context. Unlike left-to-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ce213685-8dca-454f-9301-8acf6e362c24,"right language model pre-training, the MLM ob-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6db37c18-6f9a-44b4-a4b3-cff346b24e1c,jective enables the representation to fuse the left
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d2a42bcc-8f81-4b99-8762-c477cf6803ad,"and the right context, which allows us to pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5141ce74-b24b-410b-882b-8aa6e4372717,train a deep bidirectional Transformer. In addi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6ebbe924-52a5-4dfd-baaf-7ddf8c88b3c6,"tion to the masked language model, we also use"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,db76e65b-e71e-4be9-9e27-5264df593ee7,a “next sentence prediction” task that jointly pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40e6b1bd-8f55-4cd9-b141-84f6763557d5,trains text-pair representations. The contributions
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4a5e4343-0ecc-4839-8163-1a75a6f3018f,of our paper are as follows:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7827f3ef-61f5-4958-8950-1f4921bbc6da,• We demonstrate the importance of bidirectional
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,317d1c9a-bfb8-4648-9a68-e8765251bdee,pre-training for language representations. Un-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c2d267e-c2cc-4e31-aead-6c1f69eb2ca4,"like Radford et al. (2018), which uses unidirec-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a92c93e3-a415-4978-bca9-3d6b6084e2a2,"tional language models for pre-training, BERT"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a0b61b01-0892-4bdd-804f-7a2471a53796,uses masked language models to enable pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4871d00e-e7ab-425f-a4d1-e5b0ab877cfd,trained deep bidirectional representations. This
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,62ab037a-93ee-4947-8282-566fdaef0272,"is also in contrast to Peters et al. (2018a), which"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fe78e3f3-87b1-4733-9b4c-ba368e83dd91,uses a shallow concatenation of independently
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,88defd5d-932a-4d16-b0fa-1267dc8e2ceb,trained left-to-right and right-to-left LMs.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,92790f60-3411-47b9-a9ce-093365911add,• We show that pre-trained representations reduce
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b7617db8-44d7-4f61-b104-5bdf9827be41,the need for many heavily-engineered task-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f0d0c8fb-607c-4efb-9e29-3b981f7010c0,speciﬁc architectures. BERT is the ﬁrst ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f588ddaf-4fde-4614-987d-9c7ad8d76d0d,tuning based representation model that achieves
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a2094ace-d488-44cb-b674-3146536dae64,state-of-the-art performance on a large suite
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa8785df-afd9-442c-89b5-c09a86966537,"of sentence-level andtoken-level tasks, outper-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,250201b7-800c-4591-a7b8-b12f4fc7f43b,forming many task-speciﬁc architectures.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9476108-6a6d-439d-9fc9-aa0b63b5d097,• BERT advances the state of the art for eleven
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,752c6b21-ea0c-4dae-8843-64bd91d42a82,NLP tasks. The code and pre-trained mod-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,22647ddc-fe01-4709-9fb5-0d47da70e79f,els are available at https://github.com/
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5234d048-3676-431a-8a2a-93148da92778,google-research/bert .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3a8dd0c0-e15f-43e9-8de9-1cfe5a6ce4c3,2 Related Work
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,35a97a97-b9cd-4587-9d1b-a18bf9de314e,There is a long history of pre-training general lan-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8d7453e5-203b-4ce7-bb52-e5ce25470ad1,"guage representations, and we brieﬂy review the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e92d4278-8923-45b9-83ae-aab3e4354003,most widely-used approaches in this section.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e24160a6-fac1-48ee-94ff-bee6b766d9d3,2.1 Unsupervised Feature-based Approaches
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,98f6dc36-f1ec-4a2e-86de-9e9a28b7077e,Learning widely applicable representations of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4fcae66a-3b0a-43ff-b715-662426f48e67,words has been an active area of research for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,22076d5c-e271-4bc4-9cac-3695ec8c915d,"decades, including non-neural (Brown et al., 1992;"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,586532f4-3482-4757-a1da-528c8b1ca851,"Ando and Zhang, 2005; Blitzer et al., 2006) and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,90ee7548-9695-4df8-9c63-04179f2ca9ff,"neural (Mikolov et al., 2013; Pennington et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3e1c4042-4d12-45d2-8bb5-a74b8a835a5d,2014) methods. Pre-trained word embeddings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bd742ba0-e790-4016-b310-7992c66bdb40,"are an integral part of modern NLP systems, of-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8c91dcf8-35b3-445e-8540-37a35af2cd00,fering signiﬁcant improvements over embeddings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,064f650d-8432-4f87-919a-91b115733223,"learned from scratch (Turian et al., 2010). To pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,19c7e801-6b01-47a3-91bf-6c52e7d8fb27,"train word embedding vectors, left-to-right lan-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c718653-7b93-4db7-a09c-aa732335d655,guage modeling objectives have been used (Mnih
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,841c5bd9-d564-4b97-abcf-9a716d88fcc7,"and Hinton, 2009), as well as objectives to dis-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c38b6f6b-7697-454a-8967-7164a509469f,criminate correct from incorrect words in left and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,32b92931-8e20-45b6-ab71-7519797975a2,"right context (Mikolov et al., 2013).These approaches have been generalized to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d1006a44-9360-4701-aa2e-25d45f057f11,"coarser granularities, such as sentence embed-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8a815ab2-8d3f-49f5-8fc0-70266d0afab9,"dings (Kiros et al., 2015; Logeswaran and Lee,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,13bb601b-38f2-42bb-a41f-8c0165c00288,"2018) or paragraph embeddings (Le and Mikolov,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8a228e31-0c4e-4171-9239-b3048fa30d32,"2014). To train sentence representations, prior"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a338aa71-7e3d-435f-9361-8902a4165ef7,work has used objectives to rank candidate next
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f072d290-4ca4-45de-b291-4a2172bf50d4,"sentences (Jernite et al., 2017; Logeswaran and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8683910-609c-4ad3-b8de-3897f7dfc866,"Lee, 2018), left-to-right generation of next sen-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa9a2312-806c-4b6b-a5a3-7842ca8c8db7,tence words given a representation of the previous
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d4579770-8579-49d8-9456-b5be8ac186b0,"sentence (Kiros et al., 2015), or denoising auto-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c0d0b0d-7b38-4c70-b945-c4aa204b9576,"encoder derived objectives (Hill et al., 2016)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9a1e84a6-3989-4a61-a7e4-d6fc171e3081,"ELMo and its predecessor (Peters et al., 2017,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5d7cd625-b1b3-4b5d-98bb-7ec688f3035e,2018a) generalize traditional word embedding re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ebc35b16-17d8-470c-864f-e089dbbbb888,search along a different dimension. They extract
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d6027f7b-ea1f-491d-bb10-9751913272eb,context-sensitive features from a left-to-right and a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,172c5b8b-2472-4c9a-a5fc-bd943b05a5ab,right-to-left language model. The contextual rep-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ad5aea02-5018-414e-88c6-feafeda59158,resentation of each token is the concatenation of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,61967527-b19c-4c50-a332-4ad6214d5f29,the left-to-right and right-to-left representations.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eb40fe1c-f4fe-4146-9d40-316d358ec4d9,When integrating contextual word embeddings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,111f91ad-f3a4-4f21-9688-e69fb77ed43b,"with existing task-speciﬁc architectures, ELMo"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3fe9853c-3d07-43d5-8048-5648690cf5a2,advances the state of the art for several major NLP
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4b125ac7-9a18-464e-ae5c-2673d4d0aaf4,"benchmarks (Peters et al., 2018a) including ques-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,75ae7e5b-c041-4398-abfd-7b022f42b390,"tion answering (Rajpurkar et al., 2016), sentiment"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,27d0842d-a050-49d8-b310-4b5a1bd42f10,"analysis (Socher et al., 2013), and named entity"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,51a0ca93-efa7-4a09-807c-c17f7bf13c22,"recognition (Tjong Kim Sang and De Meulder,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3f1541b8-c7ac-44f8-8570-fa4c6988df03,2003). Melamud et al. (2016) proposed learning
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5b9419a7-6743-4747-a41d-305c1cfbb0d2,contextual representations through a task to pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ac0aa36f-e183-4058-a84d-f03d4bc308c1,dict a single word from both left and right context
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5206b482-1080-448f-b7a1-78953ffe1c8b,"using LSTMs. Similar to ELMo, their model is"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e89d2177-71be-48bb-a67b-23b1aa61f381,feature-based and not deeply bidirectional. Fedus
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cae96083-3426-48bd-a4fc-6ec12fd55c2b,et al. (2018) shows that the cloze task can be used
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a4cae16b-a8da-4c07-8253-08743261a4c5,to improve the robustness of text generation mod-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ec09766-6b8f-41eb-b82f-2e5bcde63ba3,els.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d6a220b4-613f-4676-ada2-916593ce2c29,2.2 Unsupervised Fine-tuning Approaches
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c1273dfd-98f4-4e3a-8e1b-d631bb6ee51f,"As with the feature-based approaches, the ﬁrst"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,46d827e4-3d2e-410d-b378-522a48654b7e,works in this direction only pre-trained word em-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6dd676d7-8945-4878-879d-15a187c94369,bedding parameters from unlabeled text (Col-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c4583bee-9cf2-40c1-ae3d-2965b4fd7a0b,"lobert and Weston, 2008)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e336f3bc-081d-40bd-91ac-477c10f8c302,"More recently, sentence or document encoders"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,deea0408-88d4-4fdc-b7d2-6105cc04229e,which produce contextual token representations
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,387eaf5d-12b6-4a60-a9bc-1b2e79364ec2,have been pre-trained from unlabeled text and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4473010f-3bbf-4041-b7c7-4c86728caaef,ﬁne-tuned for a supervised downstream task (Dai
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e0589ea1-953b-4708-b075-2dc2526ddff6,"and Le, 2015; Howard and Ruder, 2018; Radford"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,283112e7-7758-492d-9f97-464fcb772d0e,"et al., 2018). The advantage of these approaches"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ab974b98-4703-4680-a834-32f95402e682,is that few parameters need to be learned from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0df8aab7-edcf-4641-90dc-a7eed880bde3,"scratch. At least partly due to this advantage,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,80adc142-e7a4-4ca7-af31-3aeffa2f9027,"OpenAI GPT (Radford et al., 2018) achieved pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bfcaad5d-4965-466f-ae33-87a6861272ae,viously state-of-the-art results on many sentence-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ef4a8d38-b957-4fc4-ad21-70553fa1d1f9,level tasks from the GLUE benchmark (Wang
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f3cf38da-472c-43a6-949d-8e67817d7409,"et al., 2018a). Left-to-right language model-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,001025b6-2d47-4256-919a-b31083ca58c1,BERT BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5cdd93cc-4690-4d6a-963f-4d55e9ee37e1,E[CLS] E1 E[SEP] ... ENE1’... EM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,65446fb8-f02f-4adf-a1ee-d99364679f43,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2216aaf9-d80a-4b7c-bc28-cccaa790c16e,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4d5d642-deb6-41c8-9895-dd0587240d51,T[SEP] ...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e3b13ba4-7823-4594-b607-66f204016ee3,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e6d4dab-6a48-44d6-9383-de653fade97a,T1’...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3058663d-c406-4fe9-83ad-00436a2a7d32,TM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ec5877c-1a6a-4fd4-ad99-cdab19298195,[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,338eefcf-9533-40d2-be7a-128cab06a603,Question Paragraph Start/End Span
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,27715ceb-4dc3-4542-923e-83a40ca3f491,BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,976a61d6-9298-470e-bb9e-0a37ef73d357,E[CLS] E1 E[SEP] ... ENE1’... EM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ab278fad-c4a5-4795-a415-264855e9a685,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,831cc640-9389-4b69-b0aa-a6cbb3b263fb,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0114c5f3-6ca8-40cc-b1f7-a8255d317a77,T[SEP] ...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f36dac9b-190b-429a-a633-d23d9881e1e8,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ca77944d-c1fa-4c6f-873a-5b22bf436ac9,T1’...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,73562bbb-afc6-46a3-9658-f221277e2830,TM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5bc11b6b-0b7c-48b6-8e33-6adc80024309,[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2e056b86-d018-4196-93e7-4ae0d978a986,Masked Sentence A Masked Sentence B
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4fbed795-2fc2-4a48-8cc6-24e5474c7603,Pre-training Fine-Tuning NSP Mask LM Mask LM
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b6f86583-a81b-4bd4-afe5-d28d47188b6b,Unlabeled Sentence A and B Pair SQuAD
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d8be0305-7fdf-4f29-bb6e-bbd48bb1e6d1,"Question Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c02d5aae-8c48-4273-9255-4b8c76ea188c,tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1e0d78c3-2b00-4842-9d2d-cdc9db41cef5,"models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6e82f8de-9c54-4fb9-9e03-4df5445bb578,"symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4d52a7b8-2f70-422a-aea4-4d18e64df772,tions/answers).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e83a59f5-164f-44b2-8bb0-cd2437f59ee6,ing and auto-encoder objectives have been used
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1efabb81-d164-4999-8ef8-eb45ebb158a9,"for pre-training such models (Howard and Ruder,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cca7b815-b8eb-46d1-bc7c-a89b7c257622,"2018; Radford et al., 2018; Dai and Le, 2015)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,52f2373b-61b9-4dd5-9a9e-8685773eb360,2.3 Transfer Learning from Supervised Data
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b8acd462-348d-4974-8410-cfb45b6c06e9,There has also been work showing effective trans-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,78660b95-814e-4a63-ada8-1a4cd129fc10,"fer from supervised tasks with large datasets, such"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,56f493f0-9a00-42fd-aa43-8ae72aa4775d,"as natural language inference (Conneau et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6b05312-089f-439d-833d-74f1b76c5bf7,"2017) and machine translation (McCann et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,162be2d2-6b1d-4428-9490-d088972aed82,2017). Computer vision research has also demon-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,12ec43a9-bd8c-43de-8fda-cdaca93a1635,strated the importance of transfer learning from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e01a09ab-7779-4e24-8f10-78db591bdd72,"large pre-trained models, where an effective recipe"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8524e7b2-df20-4b3a-8eeb-d0261cf4571e,is to ﬁne-tune models pre-trained with Ima-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,617f0da7-3e0c-4429-9cdd-2012802414bd,"geNet (Deng et al., 2009; Yosinski et al., 2014)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d4ad287c-e90b-4f92-9a82-9813f4121e53,3 BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,de8e4a0a-bb95-45f5-a207-e74227062f6b,We introduce BERT and its detailed implementa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,34e1ece1-9989-4c00-98a9-f2ad2f5b8929,tion in this section. There are two steps in our
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1a01b935-ec45-4f15-a784-4e65860aaa52,framework: pre-training and ﬁne-tuning . Dur-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9067fcef-4263-4797-a8ce-cb05155584ff,"ing pre-training, the model is trained on unlabeled"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,10652987-e5d1-4a56-8cb6-46341ac4b55d,data over different pre-training tasks. For ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8dec862c-f6a1-4b7e-8402-41b666f99fe0,"tuning, the BERT model is ﬁrst initialized with"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a78beffe-6be8-4ef8-ae81-d50ae9da1893,"the pre-trained parameters, and all of the param-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ea695358-660c-4ee5-a79f-f055423d288e,eters are ﬁne-tuned using labeled data from the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1655d62c-3f8d-4cf8-b851-36bbe9a66861,downstream tasks. Each downstream task has sep-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,809e83f3-68e5-42b7-ba51-be929f87d4c4,"arate ﬁne-tuned models, even though they are ini-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,64bbbfaa-5e13-447f-98d0-264b2592b620,tialized with the same pre-trained parameters. The
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f3070a01-3ee6-4a2b-8ba3-710a1cd72d14,question-answering example in Figure 1 will serve
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cd2d76d6-c861-4b60-8f18-dd1aa04cd4cc,as a running example for this section.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,54572613-12b4-4d19-b7bd-87a301a55101,A distinctive feature of BERT is its uniﬁed ar-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a2689d61-75ab-49ba-be7b-a9d5372bf87d,chitecture across different tasks. There is mini-mal difference between the pre-trained architec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b6cf4b3a-932f-4b10-8239-9bc22843c5b2,ture and the ﬁnal downstream architecture.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,19d365e5-5820-4630-a1f8-d634a649e15a,Model Architecture BERT’s model architec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,96466550-af01-414a-a95d-80d6098bedf0,ture is a multi-layer bidirectional Transformer en-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a449459a-030e-4ff2-a831-b9665135e616,coder based on the original implementation de-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,24534308-e47a-4d42-b973-dec5a6dbbae7,scribed in Vaswani et al. (2017) and released in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b2221053-674b-42df-8218-18386e02a00f,thetensor2tensor library.1Because the use
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ff0902a2-125c-4e86-85f1-2590aad01de4,of Transformers has become common and our im-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2a0956e4-8f1f-47a9-bd78-d2fc307f2ed9,"plementation is almost identical to the original,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e4c91d01-c418-40b5-82bf-b343098b7787,we will omit an exhaustive background descrip-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fbdd8d17-a99d-471a-9959-b6e61d13fde2,tion of the model architecture and refer readers to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a19b1109-bbb4-4044-9a1c-a10c62dedac4,Vaswani et al. (2017) as well as excellent guides
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,461b3025-14b8-4acc-9473-e9293e7aeb74,such as “The Annotated Transformer.”2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dae5f34b-dfcd-4e6a-ada9-173277dfc719,"In this work, we denote the number of layers"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,14c74fb1-9312-47b8-aa90-c445ed173879,"(i.e., Transformer blocks) as L, the hidden size as"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0d9f66d5-b3fe-491b-95e3-63935795bf57,"H, and the number of self-attention heads as A.3"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f6db696-c818-4300-8942-8ac0313ba3a0,We primarily report results on two model sizes:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4692806e-0b35-4d45-abe2-503b39bc773b,"BERT BASE (L=12, H=768, A=12, Total Param-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d60428d7-937f-483d-9811-2f9b87e08ce7,"eters=110M) and BERT LARGE (L=24, H=1024,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8be9a4f8-58b9-434e-a015-391c97a6143e,"A=16, Total Parameters=340M)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7aadfe30-a8cf-482b-ae0b-68461920e4ff,BERT BASE was chosen to have the same model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,989c3932-b67c-466f-81b6-7dc5b04dc64d,size as OpenAI GPT for comparison purposes.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5ae70046-99ba-4c12-bda8-b4b21b54fc8c,"Critically, however, the BERT Transformer uses"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0f258ab9-4629-4d74-b75a-c68a8d7cba77,"bidirectional self-attention, while the GPT Trans-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,29181aa4-3c82-45b5-95aa-4da8719ba2dc,former uses constrained self-attention where every
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f636013f-88f8-47ad-9ffb-2cc18f2d9607,token can only attend to context to its left.4
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d5788102-fe85-442c-91ae-f849c792068f,1https://github.com/tensorﬂow/tensor2tensor
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ec65e82d-fc87-4e4c-94bb-e4de59cc492b,2http://nlp.seas.harvard.edu/2018/04/03/attention.html
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e6a31da7-384d-48ef-b0a1-fe0953bb71e4,"3In all cases we set the feed-forward/ﬁlter size to be 4H,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3028e4dd-3e73-4986-9212-85e7319d23e6,"i.e., 3072 for the H= 768 and 4096 for the H= 1024 ."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f7597948-6448-45db-b447-817120ec7a06,4We note that in the literature the bidirectional Trans-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,49787924-b8a1-4131-8f25-72810dc836c4,Input/Output Representations To make BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94bfd752-1a76-4295-a65d-2ba0b3c69f46,"handle a variety of down-stream tasks, our input"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ece64140-46e1-4c3f-a12a-f6ebc09eac3e,representation is able to unambiguously represent
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42d92491-69d8-475f-96ca-3df0dd4798d6,both a single sentence and a pair of sentences
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9ca9008f-1bda-4d57-8e0a-cd3692d441a4,"(e.g.,hQuestion, Answeri) in one token sequence."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c17553e-e7b2-4482-b898-158f53246111,"Throughout this work, a “sentence” can be an arbi-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9091f1e1-d684-4f41-a47b-f3d245720808,"trary span of contiguous text, rather than an actual"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c846a00-3076-44ba-9204-4d4efb12b6d5,linguistic sentence. A “sequence” refers to the in-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb74c695-31c6-4a80-9b4a-cbaddd9fd19e,"put token sequence to BERT, which may be a sin-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5ab0083-e8d6-4487-bb08-88324064551f,gle sentence or two sentences packed together.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4921abb9-2489-445f-9728-73cf028013fd,"We use WordPiece embeddings (Wu et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,165150a5-a902-4ee9-bfab-25b2000919e3,"2016) with a 30,000 token vocabulary. The ﬁrst"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b5852aca-7aaa-4248-8e37-c0358612de41,token of every sequence is always a special clas-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3562e2fa-9cf1-4805-b9dd-5ee24705b8c2,siﬁcation token ( [CLS] ). The ﬁnal hidden state
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c8c2187e-a47b-47fa-96e2-9fe392129edd,corresponding to this token is used as the ag-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ec50628-4dda-4d96-bc14-0c57a62b99fb,gregate sequence representation for classiﬁcation
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9bc8af91-cc36-4373-85b6-0db829d8cc57,tasks. Sentence pairs are packed together into a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2dc9a3af-3ffa-49fc-b44e-43970ad46b39,single sequence. We differentiate the sentences in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cc910000-5357-4f1f-859c-f06192205d32,"two ways. First, we separate them with a special"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,232b9ed4-39dd-4a29-a50c-38968678c8d3,"token ( [SEP] ). Second, we add a learned embed-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,43a62cef-0870-43ae-ab5e-e5a7a06b7bc6,ding to every token indicating whether it belongs
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,26dead3b-bae0-4406-bbfb-dc12095f5d85,"to sentence Aor sentence B. As shown in Figure 1,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,455511cb-a65e-4af1-90a8-aec4d8b7336e,"we denote input embedding as E, the ﬁnal hidden"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c1987d4-f317-4bb9-aa3c-8fde2e79dab8,"vector of the special [CLS] token asC2RH,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,334bf9e7-8620-4198-9ccf-8626334a694e,and the ﬁnal hidden vector for the ithinput token
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c7a91f69-40d7-4f16-af65-db4790020b64,asTi2RH.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3aca7e48-2b33-4cae-9a02-86c391acb95a,"For a given token, its input representation is"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4a1d4454-0230-4adc-9ed8-df5cc872798a,"constructed by summing the corresponding token,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fd3b63ef-6573-4729-b02d-3f69408deaa5,"segment, and position embeddings. A visualiza-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a7b151f9-2d4b-4db2-9d01-f58c27d95f98,tion of this construction can be seen in Figure 2.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a897f5eb-da45-4368-872f-ddb964c1f336,3.1 Pre-training BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,314949e3-54b6-4cf3-b462-2cc8f8b23d7e,Unlike Peters et al. (2018a) and Radford et al.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,30d42753-3c8c-4237-83e9-0aaba541b906,"(2018), we do not use traditional left-to-right or"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3631067c-3b7c-4c47-8477-dd0098afe52f,right-to-left language models to pre-train BERT.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40fbbc54-51ee-4eea-96a0-070ccba16dff,"Instead, we pre-train BERT using two unsuper-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f9e56606-5dea-454d-bdb7-b723ebd0e307,"vised tasks, described in this section. This step"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eb0adea7-4481-4f98-bb92-c017e6b3c228,is presented in the left part of Figure 1.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b7086c7e-4680-48e0-a0c8-e12eea4829ba,"Task #1: Masked LM Intuitively, it is reason-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f0a6ba3d-4a4d-4b86-bdf3-cb98eab900f6,able to believe that a deep bidirectional model is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d8abe042-08b1-41f1-95af-7c40ac397f1f,strictly more powerful than either a left-to-right
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3e5fc478-0fbd-4a50-8113-012258230e6f,model or the shallow concatenation of a left-to-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2a319eb6-1879-461f-a1af-fa99c315ce8f,"right and a right-to-left model. Unfortunately,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7d88bd01-a6f0-4bb5-904b-bd53bc794a5a,standard conditional language models can only be
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7ecb9e3a-6939-4c1f-b472-4bac07c0d371,"trained left-to-right orright-to-left, since bidirec-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7ab47145-ab3c-458c-be92-c948352c1693,tional conditioning would allow each word to in-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,baa735ab-283f-4144-b541-c1a64c9138e6,"directly “see itself”, and the model could trivially"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2ab2c504-8ca8-4879-aa8a-0c5448d52606,predict the target word in a multi-layered context.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a7a27506-0322-45a3-ac3b-0fb8a2eea842,former is often referred to as a “Transformer encoder” while
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,395f1c24-0fc8-4ceb-adb1-bafd9825cc9c,the left-context-only version is referred to as a “Transformer
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a7262481-e867-48a7-84e7-9537560af18e,decoder” since it can be used for text generation.In order to train a deep bidirectional representa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,22beb0f6-7317-4432-9360-c13d76ef51c5,"tion, we simply mask some percentage of the input"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,51f2194a-e8c6-4be7-862f-bd92251eca89,"tokens at random, and then predict those masked"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae84fb49-d59a-4055-b933-26b133ac2ae0,tokens. We refer to this procedure as a “masked
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,957e655c-5345-4494-af89-0eaf5c4db4b7,"LM” (MLM), although it is often referred to as a"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3ceb45e1-30e5-41b3-ac0b-566cb7fd7fed,"Cloze task in the literature (Taylor, 1953). In this"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2138328e-f2b6-46ea-b6a3-9d0476f302d7,"case, the ﬁnal hidden vectors corresponding to the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6e4fb08-ab33-40d1-b7b4-2ead131414f2,mask tokens are fed into an output softmax over
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e733a424-f457-4453-9886-533c9c202e16,"the vocabulary, as in a standard LM. In all of our"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4f81496a-bb67-4731-b1c1-990c65ed29cf,"experiments, we mask 15% of all WordPiece to-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1dc06703-6b20-4e4d-8407-8585b2e69a32,kens in each sequence at random. In contrast to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,016745a5-17b3-41e1-a7d1-fb26b70a33a9,"denoising auto-encoders (Vincent et al., 2008), we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ad7515a-5501-4007-9d31-bfec9a7420f2,only predict the masked words rather than recon-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,69b8c95d-399e-4bec-9117-e399a1ff46c6,structing the entire input.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,523be680-13ae-4c42-a86b-b51f607ce841,Although this allows us to obtain a bidirec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d19eabd9-7c1c-47a2-b9d2-b9dbd6b12d33,"tional pre-trained model, a downside is that we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2eeeaa56-8f8f-4899-b143-944e892a3193,are creating a mismatch between pre-training and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2a585f4a-a3cb-4c78-83dd-b23432d0f6a2,"ﬁne-tuning, since the [MASK] token does not ap-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,846dcc6c-6713-44c1-9a2e-1181382f5976,"pear during ﬁne-tuning. To mitigate this, we do"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0a23d9d3-b481-4182-9970-1f02b0b5b258,not always replace “masked” words with the ac-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9878f57b-f517-45dc-9347-8bfb76fe7010,tual[MASK] token. The training data generator
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,02d41c1a-4cf1-4e69-9038-6079b6d731ec,chooses 15% of the token positions at random for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,474efb3c-468e-4d10-9d8e-9f59f8b0bbd4,"prediction. If the i-th token is chosen, we replace"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f30c75a2-f3d7-4232-b627-34a005eda1d1,thei-th token with (1) the [MASK] token 80% of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a93e2b75-ef47-4a01-a183-dc335ecf447b,the time (2) a random token 10% of the time (3)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c984034b-4d5e-428c-836b-1b15257e0964,"the unchanged i-th token 10% of the time. Then,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9f858bf-e797-499a-9609-8fe432bbd5b0,Tiwill be used to predict the original token with
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b5f2c5eb-bac5-44aa-b72a-5120a0dfedb4,cross entropy loss. We compare variations of this
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b310f1f6-f4ce-4967-9a7f-04b7e4dbfed2,procedure in Appendix C.2.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8084de1a-9623-4b11-b892-0247958ffd9b,Task #2: Next Sentence Prediction (NSP)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b1fce224-5784-42bd-84b1-ca215b96bfeb,Many important downstream tasks such as Ques-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e53128b-5d91-4754-b1ec-847845795505,tion Answering (QA) and Natural Language Infer-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b352ed6c-961d-406d-a2e7-5d9c2edbf1fc,ence (NLI) are based on understanding the rela-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eb03559a-70a8-4f1f-b7b4-9c7c07d65e7b,"tionship between two sentences, which is not di-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7e555d51-5454-401f-b8b0-3811216d0a1b,rectly captured by language modeling. In order
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,28b2a22b-0590-41fe-a0d5-acc9b3e593e8,to train a model that understands sentence rela-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5fe4f02d-4035-4fe3-a403-4ec0130fdf75,"tionships, we pre-train for a binarized next sen-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7bebb5dc-c2e6-4eec-a3af-086e9db9e1eb,tence prediction task that can be trivially gener-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e249655c-b1fa-4c64-81e8-ee2c4b9939ae,"ated from any monolingual corpus. Speciﬁcally,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d3f9843c-e0cc-4ead-8f6c-fcc058b25a03,when choosing the sentences AandBfor each pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,95857021-ff6a-4ed2-8342-f97867073f54,"training example, 50% of the time Bis the actual"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,804f3ae7-0902-4780-8c6d-0e5ca47436a0,"next sentence that follows A(labeled as IsNext ),"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,31cfaec8-eb6f-4305-b05e-7865311cea31,and 50% of the time it is a random sentence from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fe965a03-dd5b-4436-8ed1-d781e388141c,the corpus (labeled as NotNext ). As we show
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7817735b-a64a-471e-b0e4-8219d4d985ab,"in Figure 1, Cis used for next sentence predic-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4cafd9f3-ac87-46a9-b553-7a3a1e72161c,"tion (NSP).5Despite its simplicity, we demon-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae7a89a6-1e16-400a-a263-4691d3cf2153,strate in Section 5.1 that pre-training towards this
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cbbefaa7-aa8e-40a6-89fa-9a486b4865d6,task is very beneﬁcial to both QA and NLI.6
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,558a3607-828c-4857-bcd6-6be123c32923,5The ﬁnal model achieves 97%-98% accuracy on NSP.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e01d213b-439e-4880-a486-35da73ecd4ce,6The vector Cis not a meaningful sentence representation
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f81c7eae-8b62-4593-b9ab-bb9fcd77f013,"without ﬁne-tuning, since it was trained with NSP."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,71e83aa1-3e37-4c21-ba24-6e88d45be90e,[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c873d18-6ada-486f-ae14-1a43692b7a39,E[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7755b4c7-4934-4db2-82b5-3999e76a630c,Embeddings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4a66119b-242e-41b5-82e1-5b167bb2cd72,EA EB EB EB EB EB EA EA EA EA EASegment
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ec79f702-0cfc-40c0-879e-9b9c42853b64,Embeddings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a1f68024-6afb-4dcb-9582-a314d7b6d0ee,E0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8103e409-3840-4805-9ab1-cbf8f57bc721,"Embeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99bb0392-5ef2-42eb-8fd8-ffa8d5e0b8b1,tion embeddings and the position embeddings.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,65a680d3-2fc7-429d-84fd-499831d4306f,The NSP task is closely related to representation-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,41967535-2e60-4f65-9e34-a8b09d638a6a,learning objectives used in Jernite et al. (2017) and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f8ccf277-d347-40cf-9cf5-c24b6202e9bc,"Logeswaran and Lee (2018). However, in prior"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2289c73d-e80b-487c-b7ec-3e5fd023089a,"work, only sentence embeddings are transferred to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ce07a3d-2a74-4a6b-8d95-6c4820157b5b,"down-stream tasks, where BERT transfers all pa-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8a4a9754-3a5c-47e6-b6b9-6ad5223fc730,rameters to initialize end-task model parameters.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1495011c-1690-469f-8b72-700ac9116853,Pre-training data The pre-training procedure
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4fc3dada-897e-49d0-accd-b071a4f82f26,largely follows the existing literature on language
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,52a01f1d-705a-4e33-b2ef-d647841d3a6d,model pre-training. For the pre-training corpus we
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a8c1e513-58f0-4bba-9ac9-4fc02cc11ac5,"use the BooksCorpus (800M words) (Zhu et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d50bc5b4-69f9-4053-a4cf-3da901e6961c,"2015) and English Wikipedia (2,500M words)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6bc8aab-f794-48ae-a882-b10168347529,For Wikipedia we extract only the text passages
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a3cc1da3-3951-4591-bb0c-927b51d1d165,"and ignore lists, tables, and headers. It is criti-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ef3a4370-f069-4eee-82e3-8dd0a240832a,cal to use a document-level corpus rather than a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0659976e-bbe5-4af3-9f17-fdcd82540706,shufﬂed sentence-level corpus such as the Billion
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e4b74714-4267-4d10-bf01-5790486a684e,"Word Benchmark (Chelba et al., 2013) in order to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4d08f646-9e2f-4a00-8562-b20ccbf5795e,extract long contiguous sequences.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aef406dd-a0e5-4a46-8279-b796072e1784,3.2 Fine-tuning BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94677a2b-9b7b-4c0a-9560-5b72a0eff36e,Fine-tuning is straightforward since the self-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,86a8330d-fbae-40dc-b305-b80d36523bb1,attention mechanism in the Transformer al-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1cbea636-17ae-4e76-a4f6-6514a495e7fe,lows BERT to model many downstream tasks—
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a43676e9-2ba4-470d-a002-1e1e2f557f8d,whether they involve single text or text pairs—by
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,88c8bd62-af06-4f99-97c2-77d379c7380c,swapping out the appropriate inputs and outputs.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3d48acd9-294e-4798-a0f4-856143c2eab2,"For applications involving text pairs, a common"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,68e639a1-557d-44ad-a600-1e2e5e842423,pattern is to independently encode text pairs be-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3757977c-d6fc-45cf-91c7-e08120dc9693,"fore applying bidirectional cross attention, such"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fad7d98c-bdee-4833-95f5-93175e62cbaf,as Parikh et al. (2016); Seo et al. (2017). BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,31236f3c-4a58-497e-967b-857a34683870,instead uses the self-attention mechanism to unify
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,627cb034-df64-4a6d-8c5e-4fd411868a79,"these two stages, as encoding a concatenated text"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f6726001-ebd6-4343-a4ea-efe05e00bd66,pair with self-attention effectively includes bidi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e83ab69f-5fe3-40a0-b03b-e8dd48d74ce9,rectional cross attention between two sentences.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,812e164a-15fc-42bf-a45d-dfcb6de8db55,"For each task, we simply plug in the task-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc19adb3-02da-4264-bd94-dcd536999c39,speciﬁc inputs and outputs into BERT and ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dcff5f4e-2337-49db-bc96-ed7b441b6329,tune all the parameters end-to-end. At the in-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f8e54a9b-f2f6-4831-b9f4-70fa8a4b572d,"put, sentence Aand sentence Bfrom pre-training"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d20b19f3-764f-4b4f-8897-d4c532798736,are analogous to (1) sentence pairs in paraphras-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6349ce57-c7a9-4b65-9ec2-dcfc32f4ad57,"ing, (2) hypothesis-premise pairs in entailment, (3)"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4d90859-c3e7-4c87-8c2d-c3fbe6e39f23,"question-passage pairs in question answering, and(4) a degenerate text- ?pair in text classiﬁcation"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6e1483ea-65dc-4ba4-9236-47955644477d,"or sequence tagging. At the output, the token rep-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b78200ae-ec16-48b3-a193-8e6e5f984bf6,resentations are fed into an output layer for token-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f76facc7-10be-4048-ae2c-6bf549db503e,"level tasks, such as sequence tagging or question"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,60fa8ae1-f5f9-4281-b29f-d125755d25aa,"answering, and the [CLS] representation is fed"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e109173d-d005-41e8-a842-7cb11b7d22ff,"into an output layer for classiﬁcation, such as en-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,05b20e4f-f4fc-4a11-ade9-abe0f4555d98,tailment or sentiment analysis.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e585a486-df4b-4c3d-9132-c2c7205a3943,"Compared to pre-training, ﬁne-tuning is rela-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c89838d-c71c-4c69-94c7-15b4f47e1bbd,tively inexpensive. All of the results in the pa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,16e5c93d-8ffc-4e6f-b3aa-7f2f5b8bf413,per can be replicated in at most 1 hour on a sin-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2c21c0c6-218a-42ad-af64-5a661debbfac,"gle Cloud TPU, or a few hours on a GPU, starting"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f3a2b1a2-4899-4277-bdec-afcf43ef40fe,from the exact same pre-trained model.7We de-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,68317c47-24dd-4164-b595-ddaa3c52dfe5,scribe the task-speciﬁc details in the correspond-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8bfa7ebf-b52d-49bb-bed1-bf9ff013fdec,ing subsections of Section 4. More details can be
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,55dde592-dcad-46e6-bf71-a6a0e133ed9f,found in Appendix A.5.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5d96265b-3955-4107-bb1e-77cb40547602,4 Experiments
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5d11e699-3d9e-42bc-bd11-10dcc999d7e2,"In this section, we present BERT ﬁne-tuning re-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,accc0add-9480-4df8-af27-931926ff824f,sults on 11 NLP tasks.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5350ed40-0476-41a1-8d0a-083891b2f4c5,4.1 GLUE
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c10135f3-f961-4896-9341-2b820a91d547,The General Language Understanding Evaluation
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8a7742d-9967-4b1b-8027-611e909838ae,"(GLUE) benchmark (Wang et al., 2018a) is a col-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,140f7e34-bd94-4d34-af02-5d5b859d1cbe,lection of diverse natural language understanding
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5c6c2e51-19a1-4d36-bbb5-d8eba2a37e89,tasks. Detailed descriptions of GLUE datasets are
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dd50a628-13cb-4741-a664-75dc202fbc7d,included in Appendix B.1.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b425462a-645c-458e-92dc-9fb6550529d7,"To ﬁne-tune on GLUE, we represent the input"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1989c315-6663-4470-a3f7-f9d9abec79fe,sequence (for single sentence or sentence pairs)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7976392a-6420-47f0-910d-a90914e6b46d,"as described in Section 3, and use the ﬁnal hid-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6f14f4ec-ec93-4fb8-a9fb-7fbd58befd38,den vectorC2RHcorresponding to the ﬁrst
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e0ef8e6e-d7cf-4ba7-8484-0bedca665d69,input token ( [CLS] ) as the aggregate representa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bb5acc41-f3bb-4aa1-8068-6d011c9c1cbd,tion. The only new parameters introduced during
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c111582-732a-4795-b46b-8aadb48687a9,ﬁne-tuning are classiﬁcation layer weights W2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d49f2ce3-3532-4a3e-b2f7-d9db863167ab,"RKH, whereKis the number of labels. We com-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,331fd4ce-9cdf-46cd-a215-0d0b9c4f5d76,"pute a standard classiﬁcation loss with CandW,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1bde0dce-d7f5-4838-979e-dcc9cc0f867b,"i.e.,log(softmax( CWT))."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ced6b167-0fd5-4497-8ce5-c525538bf4fb,"7For example, the BERT SQuAD model can be trained in"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,375b5c2f-a99a-4925-9b31-aef2e8e503fe,around 30 minutes on a single Cloud TPU to achieve a Dev
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bd636d1f-16c0-43a2-8bda-6f8c41de6c3b,F1 score of 91.0%.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7a8ec1a1-6759-446e-b8f4-22a53857ba70,8See (10) in https://gluebenchmark.com/faq .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c1538da4-3e98-428d-aa24-de86f4223672,System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,558d4d2d-6af3-41fc-a167-264516c0c345,392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aeb931ef-0451-484e-8b38-2e87d11233dd,Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c79aa2ee-2870-48ab-8f15-6791e43faa79,BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e54adf85-5944-4664-82f7-83e596ee91c6,OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5c289ad8-8cc6-4c00-a653-cdab63983bee,BERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,56b77ea0-e3b1-4fcc-a6b8-502b4a9257bc,BERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4b95f3dc-34ba-4da1-b1ba-9aa4c94ad96c,"Table 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard )."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,31da5336-d082-424e-beec-d48aa5e6290e,The number below each task denotes the number of training examples. The “Average” column is slightly different
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bef4b8d5-4dba-4242-8ef7-cb939cd3a423,"than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1d3f430f-c14b-4d3e-9dde-8fcbde77f733,"model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,090b75e9-bc43-48ae-9e2a-db324ede9524,accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3abd9aac-5aca-47e0-a8a7-ab03ede7da24,We use a batch size of 32 and ﬁne-tune for 3
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,393c8c37-2189-4f5f-ae4a-4399d74cb51d,epochs over the data for all GLUE tasks. For each
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b01740b7-c62c-4faf-9288-04d8b0aa724d,"task, we selected the best ﬁne-tuning learning rate"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba71c076-2beb-4d21-8a52-b567bb47a6e0,"(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,623bbd63-2b80-4fb5-8cb5-715b4939d44e,"Additionally, for BERT LARGE we found that ﬁne-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c8354e4-4d1d-42cc-acc4-bf967134c94b,"tuning was sometimes unstable on small datasets,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,537d881a-c9e7-46fd-827b-02742ee5990a,so we ran several random restarts and selected the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99ad9fb7-da84-4b5d-870d-5d560d644436,"best model on the Dev set. With random restarts,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d02fd34f-bd48-44bb-88ba-9ed71916bdff,we use the same pre-trained checkpoint but per-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,53e82d57-1841-4102-b6ba-16aba7bc485a,form different ﬁne-tuning data shufﬂing and clas-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1308e422-d322-459f-b8de-a3bfd99bdbb9,siﬁer layer initialization.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3266000c-744e-4885-919e-ec072e0e1a01,Results are presented in Table 1. Both
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,665326fa-df48-4d13-b94a-645c39aa0b2e,BERT BASE and BERT LARGE outperform all sys-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d9471159-1925-4f79-b95d-66b105de51a4,"tems on all tasks by a substantial margin, obtaining"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,927421ff-3580-4cdb-a2d1-9c0781eeb5a2,4.5% and 7.0% respective average accuracy im-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6303bba3-8849-46e9-91c0-8c1103e8fdd9,provement over the prior state of the art. Note that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e3609c42-d963-4b60-a58a-7174f6bdb268,BERT BASE and OpenAI GPT are nearly identical
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1b3357a0-6325-4de8-a540-933fff51ba45,in terms of model architecture apart from the at-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,72e4034b-e8c4-4329-96b6-4a3358729c30,tention masking. For the largest and most widely
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a12aca92-2e9f-49f8-a685-231f81e9eff2,"reported GLUE task, MNLI, BERT obtains a 4.6%"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6accfc37-b034-4352-adc9-a6906ad369ee,absolute accuracy improvement. On the ofﬁcial
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5604e1a9-a724-44ab-9b18-b32ee32b9211,"GLUE leaderboard10, BERT LARGE obtains a score"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5100ccc0-5cd0-4b10-88fa-da74562f7ce4,"of 80.5, compared to OpenAI GPT, which obtains"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7a224194-4fae-448c-a843-8b6df6a13daa,72.8 as of the date of writing.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,14b90d4f-b3a1-4608-9593-51222a13730f,We ﬁnd that BERT LARGE signiﬁcantly outper-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bf00ddf0-f989-4eb4-97eb-a34f203ee703,"forms BERT BASE across all tasks, especially those"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c83c7082-82b4-432e-8beb-0e4ee5d4d5de,with very little training data. The effect of model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f11e2d1-b403-4631-9169-7294f23f66b0,size is explored more thoroughly in Section 5.2.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb6eed06-ac10-40c7-94ac-fea2750ef807,4.2 SQuAD v1.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,61bf0173-9bf5-428c-b568-cb30dac03ba9,The Stanford Question Answering Dataset
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ee0dc675-9ec2-45dd-9fbc-9cb3886e8e8f,(SQuAD v1.1) is a collection of 100k crowd-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c4060a9f-35da-448f-aa3a-90b4b96690cf,"sourced question/answer pairs (Rajpurkar et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a572f5e4-1def-4918-8387-08bf42c37755,2016). Given a question and a passage from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3c13afb6-6433-48e9-aa6a-f8231ebccfc5,9The GLUE data set distribution does not include the Test
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0632308e-bf22-468d-85f5-76589f8ea9ea,"labels, and we only made a single GLUE evaluation server"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,78688b95-8237-4651-87eb-35ce2b1cc1a5,submission for each of BERT BASE and BERT LARGE .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94bd2f90-d901-4cff-95bf-a3f6342bfe40,"10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,224a880d-2323-4263-9ba8-27ca188f3f47,predict the answer text span in the passage.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,57e6b3f5-ab19-470d-8a80-6d397a0d196f,"As shown in Figure 1, in the question answer-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa472976-5994-41c9-946b-0b4b788efa22,"ing task, we represent the input question and pas-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e105bf08-bfa1-44c7-afd1-8c1de2d620b4,"sage as a single packed sequence, with the ques-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b444c356-28ff-4319-85da-5759620d459c,tion using the Aembedding and the passage using
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,421e0081-790c-4c45-9855-f30fa0cee485,theBembedding. We only introduce a start vec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,587c05a5-9cca-42f2-b7c6-c7671163c9d8,torS2RHand an end vector E2RHduring
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,89a8fe8e-050c-4e28-92ea-d00598cf2a16,ﬁne-tuning. The probability of word ibeing the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,06e4d910-ef19-4140-9812-7a3e33382a4b,start of the answer span is computed as a dot prod-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9923c33-0a14-48d6-bc6f-dabd4941d7a2,uct between TiandSfollowed by a softmax over
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42f81b6a-a63c-4076-bf2b-902838561562,all of the words in the paragraph: Pi=eSTiP
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3b6d2c06-fbdb-40c3-b545-2fd6d0285f67,jeSTj.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d3e83628-796c-4b62-9779-1d5e9b463237,The analogous formula is used for the end of the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70c31b2c-55f9-485a-8b6e-f0bf4bd93543,answer span. The score of a candidate span from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,95ecfea5-3678-40f7-8b09-668af317d1fa,"positionito positionjis deﬁned as STi+ETj,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,428aef27-9ac7-4e6f-b2e7-9fd16138994a,and the maximum scoring span where jiis
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9e31a1a3-1366-4e82-9292-0ae6c5081abe,used as a prediction. The training objective is the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b1a9973b-b184-4711-9d50-765a345f65f2,sum of the log-likelihoods of the correct start and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f98a7a34-c4b8-46b7-a1bd-1db73593a5b9,end positions. We ﬁne-tune for 3 epochs with a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8db022f2-c3b5-4f69-9d85-f8eb84906682,learning rate of 5e-5 and a batch size of 32.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77ed6b40-f976-46a4-a7df-f8220c021f74,Table 2 shows top leaderboard entries as well
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6dbfb3ad-18ef-4708-b263-6ec0acd5a688,"as results from top published systems (Seo et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,779aa709-36e4-438d-bdc6-26ae2effd638,"2017; Clark and Gardner, 2018; Peters et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,263f026b-7406-4575-b45f-4b0d31c3a87a,"2018a; Hu et al., 2018). The top results from the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a6eef922-55bb-403e-a4cd-741e20c76fa5,SQuAD leaderboard do not have up-to-date public
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,572e214a-8eba-492c-afe6-fc906da6fd28,"system descriptions available,11and are allowed to"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,75bdb85b-a4b2-445b-8b9b-43e5f583ce36,use any public data when training their systems.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4df2b9d7-5f1b-4f38-ad4a-a5325752a3da,We therefore use modest data augmentation in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,388a74ef-94eb-4f27-b94b-536d3210bf9b,our system by ﬁrst ﬁne-tuning on TriviaQA (Joshi
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a65c02de-187c-40b7-9897-22019f8d5c3e,"et al., 2017) befor ﬁne-tuning on SQuAD."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8452b7df-18c0-40e9-80fc-4284525d9de1,Our best performing system outperforms the top
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,16af7093-2b4f-4c25-bdc4-7bbf45df11a6,leaderboard system by +1.5 F1 in ensembling and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f2d87d4c-1bca-456c-a53f-a3284fd3a58d,"+1.3 F1 as a single system. In fact, our single"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6afa4f8f-c231-4bd2-975a-825be7362b0f,BERT model outperforms the top ensemble sys-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,48b974fc-7c09-44d1-af74-5487cbb7c16e,tem in terms of F1 score. Without TriviaQA ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3660a2ad-85e9-4822-90b0-7b77bb207ae1,"11QANet is described in Yu et al. (2018), but the system"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f751da90-6df4-4cae-a0c5-d2e0e8a785a2,has improved substantially after publication.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fea24fe3-0db2-4843-9d42-5d2b7d04b2fd,System Dev Test
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1fa08641-6503-4822-95c6-7c9bf5bf1fa8,EM F1 EM F1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e18ba597-ff23-4ca8-8d35-914ed18e2779,"Top Leaderboard Systems (Dec 10th, 2018)"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,89e3f1c7-eecf-413d-a109-f2c68fc02cc5,Human - - 82.3 91.2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,64d1346e-98b3-4110-a6ae-cd2b86621f76,#1 Ensemble - nlnet - - 86.0 91.7
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ee27986-2e49-4af0-9c20-dd349b478980,#2 Ensemble - QANet - - 84.5 90.5
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3fc799cb-b7ef-4a98-87d9-7cc63b1e5102,Published
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7cde6ab0-68b3-45de-ad7b-b6a232116ee8,BiDAF+ELMo (Single) - 85.6 - 85.8
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,761017cf-08d1-46de-a5b5-9580783c1052,R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3ed5ae2a-4c89-46b5-bc17-077d08c9790c,Ours
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4b75ed49-2008-4b20-a4d5-ad4b75cc79a4,BERT BASE (Single) 80.8 88.5 - -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,301b2262-ed3e-4887-9e35-184909bd7c07,BERT LARGE (Single) 84.1 90.9 - -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,41cb8627-29b3-425a-a877-913500c40719,BERT LARGE (Ensemble) 85.8 91.8 - -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,00eb0b31-a4cc-4537-a6f5-ed71b708f5b1,BERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f1804a9-d5cb-4535-b00b-bf8e6e842ad4,BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,291f3ae1-6a38-4e51-8b6e-765647c09c6c,Table 2: SQuAD 1.1 results. The BERT ensemble
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0092e499-6d8f-4ff7-8945-c8ec7abfc529,is 7x systems which use different pre-training check-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b59a99f1-dea4-40ba-852c-5de82eb985ef,points and ﬁne-tuning seeds.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,38a92ad2-0239-4a90-ba50-d791f2579112,System Dev Test
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,edb7f7db-f8db-4c2c-ba73-38ddb854349f,EM F1 EM F1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,af85f7ec-d091-457f-8281-19cff8daf2d9,"Top Leaderboard Systems (Dec 10th, 2018)"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b4d50b37-1a34-4e01-8b5f-8dd95d3b8d28,Human 86.3 89.0 86.9 89.5
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3d407db8-17ac-4a60-897e-8c71b47f5888,#1 Single - MIR-MRC (F-Net) - - 74.8 78.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,95c62cd0-a5a9-44fb-be58-41f2880fb591,#2 Single - nlnet - - 74.2 77.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c18a15d-f9aa-4d66-80f5-c4617b2fa16b,Published
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ecc5f8e4-9649-4d14-9539-dc856c6f8ee6,unet (Ensemble) - - 71.4 74.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b1b5ce81-010d-4447-8eca-80bf372cda91,SLQA+ (Single) - 71.4 74.4
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc74b2e8-1647-484e-b9ff-0753a8a54969,Ours
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7ff63496-4a3f-40f1-99e4-f8e461e628d1,BERT LARGE (Single) 78.7 81.9 80.0 83.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b3d2d2a5-9fab-42ab-ad22-17bc1462e0a5,Table 3: SQuAD 2.0 results. We exclude entries that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b980b61a-d6a4-425c-ac7b-52b7ae5bd061,use BERT as one of their components.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4cc48ed8-7783-4180-84b6-b16c7785df30,"tuning data, we only lose 0.1-0.4 F1, still outper-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70ac1fcf-10a5-42c6-b5c5-c591ab75b930,forming all existing systems by a wide margin.12
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,44733632-6c2c-4f3f-9bcd-18a5fc0331a2,4.3 SQuAD v2.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,73f029c4-2511-4da5-8bd7-da52fb62c2d3,The SQuAD 2.0 task extends the SQuAD 1.1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77da8336-1dad-43be-b5a7-3f0234fc01b9,problem deﬁnition by allowing for the possibility
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f3171d41-4713-4b3b-b443-c5bdacee5c96,that no short answer exists in the provided para-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,322c03f7-ae4a-4817-ad68-2861a6e0178f,"graph, making the problem more realistic."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ca0cf366-6107-49d7-9bde-0af47bab330e,We use a simple approach to extend the SQuAD
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d4ec6c3d-d6c4-4da8-8ea5-dd0e574a321a,v1.1 BERT model for this task. We treat ques-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a269b5a5-9990-4fd4-ab76-ddd124de5ad7,tions that do not have an answer as having an an-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c610bf12-d8f1-4e7f-a85b-640273d63720,swer span with start and end at the [CLS] to-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e4691948-37c4-47ca-b8e1-d588a464aaa1,ken. The probability space for the start and end
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1a3c42d8-06fa-4bb7-9332-d5eee9d1f5ac,answer span positions is extended to include the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ab70c334-017c-4ab6-925a-63c78e3c6b88,"position of the [CLS] token. For prediction, we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f961d93c-911d-4a9e-bae5-0a8a1072f431,compare the score of the no-answer span: snull=
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3d9a2b2f-f051-4a89-83be-33d264e0e1e7,SC+ECto the score of the best non-null span
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,93013ba4-742b-422f-b0fb-2036c09f3c0b,12The TriviaQA data we used consists of paragraphs from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fe7ced49-9dc8-4e75-a073-0419774c0d3f,"TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e3abc419-0441-41bb-9efd-106fb14c7cc6,that contain at least one of the provided possible answers.System Dev Test
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ef42dfd-8c13-4b67-b101-74b5403bd03d,ESIM+GloVe 51.9 52.7
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ed8dc3b5-3f9c-42fa-8d3d-742d0c29763f,ESIM+ELMo 59.1 59.2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6f38d2df-1256-4313-a2b6-95d5fa5408f5,OpenAI GPT - 78.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f8ceecc6-70cf-40e4-960f-6a166c6f46aa,BERT BASE 81.6 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a259033e-6cae-4dd4-95f4-f2fd355b1556,BERT LARGE 86.6 86.3
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c5bb6fe4-f79c-41d7-aff0-1c22d70f45f6,Human (expert)y- 85.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,03d21d92-eb70-427f-87cf-0a10121d4ec1,Human (5 annotations)y- 88.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c33cb418-df65-4f53-aca1-bd0311fcec6e,Table 4: SWAG Dev and Test accuracies.yHuman per-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9c102fd-55e9-4d33-a428-9a2b3b533564,"formance is measured with 100 samples, as reported in"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6eb208ed-63e1-4eb8-b73a-656216c374e6,the SWAG paper.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,758c9a5b-d1ba-41ff-85f8-9acef1e3537f,^si;j=maxjiSTi+ETj. We predict a non-null
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5b5ea592-0737-4d2b-b564-a9fd17f3eb7c,"answer when ^si;j> s null+, where the thresh-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cdb3fba7-5008-430a-9524-b27d4e8bf7fa,oldis selected on the dev set to maximize F1.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42960861-854a-4cea-a188-4fe90dabcee9,We did not use TriviaQA data for this model. We
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,53247e7f-a213-4145-b2dc-ff4257240832,ﬁne-tuned for 2 epochs with a learning rate of 5e-5
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,634c4e8e-f96f-4a75-9f79-00f3ff816699,and a batch size of 48.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7eb9ead7-315e-4108-8c10-c7ac37ac18b5,The results compared to prior leaderboard en-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c89d9dcd-4b80-49a6-9a5a-58b6fb931618,"tries and top published work (Sun et al., 2018;"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4f273a26-c15a-4379-b831-dd444a29c690,"Wang et al., 2018b) are shown in Table 3, exclud-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b55e54e9-ab82-47a0-8c6d-f9b2a2c57edb,ing systems that use BERT as one of their com-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,217dee54-b0af-44f5-8490-c433f1b0d455,ponents. We observe a +5.1 F1 improvement over
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fbf58b75-9559-4695-8930-6ab42d07c318,the previous best system.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,064967f7-8d7f-4acd-bdb1-45233f799ee3,4.4 SWAG
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,07c7c4e6-1223-4961-8c0a-7c02cc83db84,The Situations With Adversarial Generations
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,13ed4130-9057-4a03-bcf3-8955b12ebe49,(SWAG) dataset contains 113k sentence-pair com-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,79c52379-c629-4c14-a03a-d16492c3fd24,pletion examples that evaluate grounded common-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cab48cfe-647a-4b64-8132-5996983cfb5e,"sense inference (Zellers et al., 2018). Given a sen-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77e0f7f4-501a-4253-af95-10acf43749b2,"tence, the task is to choose the most plausible con-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c62b6863-e047-4452-aeb5-d58bef51130e,tinuation among four choices.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae2938d3-c7a9-4ef0-bfc4-c8707a213b89,"When ﬁne-tuning on the SWAG dataset, we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2b0c1030-7aa0-4a0d-8c3f-ed61eae32e8e,"construct four input sequences, each containing"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fcd3cc41-db8b-4060-ae10-59fd34121bcf,the concatenation of the given sentence (sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,084c7896-dba5-45b9-afa2-87613deddee3,A) and a possible continuation (sentence B). The
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1de52137-a87b-4d68-954b-fe8a889cd269,only task-speciﬁc parameters introduced is a vec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2502a1eb-0177-413d-ae07-239cf0d545c8,tor whose dot product with the [CLS] token rep-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1bd180c0-6e98-48d7-b018-7f6373cb01ac,resentation Cdenotes a score for each choice
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d955a257-fc37-4c5a-b2c4-26271db19ab1,which is normalized with a softmax layer.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c40d8daa-518a-4d25-b044-cbb49e292edf,We ﬁne-tune the model for 3 epochs with a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8119da99-7b86-406e-9d6b-f75f24c02e74,learning rate of 2e-5 and a batch size of 16. Re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,69757d7f-ea65-4533-b99b-4a9d60ae73e6,sults are presented in Table 4. BERT LARGE out-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3ad604a3-a673-42d2-868c-35e3a8b90584,performs the authors’ baseline ESIM+ELMo sys-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fc185699-f197-47c5-82ad-9ca51baf5110,tem by +27.1% and OpenAI GPT by 8.3%.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,16f3f3c1-afb6-43f6-a88b-5963e6f78cec,5 Ablation Studies
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5de63cc-6cbe-49bc-a21f-1a7f73a58ff7,"In this section, we perform ablation experiments"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,562b6d90-8c85-41ff-89b1-162c03d97527,over a number of facets of BERT in order to better
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,79ea4fa3-42b1-44a5-a36d-4ccdb58e03c0,understand their relative importance. Additional
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f9eac13c-33da-4c6e-832b-c17a3b0869fd,Dev Set
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c66a289-6d90-483c-9944-288ffefbef4e,Tasks MNLI-m QNLI MRPC SST-2 SQuAD
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d16e5459-4075-40b2-9ce8-a8b35ea0a87a,(Acc) (Acc) (Acc) (Acc) (F1)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2737aa37-d05d-4020-803f-992964ebb9bf,BERT BASE 84.4 88.4 86.7 92.7 88.5
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a86f9e78-f57e-454b-af98-38a31f6fe19c,No NSP 83.9 84.9 86.5 92.6 87.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3cf4ccc7-26e2-41a1-a697-6ac8b032d5fd,LTR & No NSP 82.1 84.3 77.5 92.1 77.8
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b029a24-bd31-40ec-bdd3-9521f1e18a59,+ BiLSTM 82.1 84.1 75.7 91.6 84.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ed7d27ac-4b01-49c9-b62a-fc86446d10ca,Table 5: Ablation over the pre-training tasks using the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e9f703b2-eaee-4ea4-8594-93abd87a55cb,BERT BASE architecture. “No NSP” is trained without
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,738935ee-a234-44d4-a72e-20e4c5d297c4,the next sentence prediction task. “LTR & No NSP” is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5534dafc-43cf-465d-85a2-35d39da5dc64,trained as a left-to-right LM without the next sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,35c125fa-faec-4e50-951c-12ed432f9247,"prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a0c1a4f2-576b-4fc2-8fb6-b7ce0ee5143b,domly initialized BiLSTM on top of the “LTR + No
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d243460c-4d9c-4630-981d-6715cf3e96dc,NSP” model during ﬁne-tuning.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2d5a70c4-540b-4c20-b725-f561a3f31add,ablation studies can be found in Appendix C.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d24c42b3-36bc-402e-b01f-00f88e20dbdf,5.1 Effect of Pre-training Tasks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,512b3109-5ccb-4382-812d-16faa454fe01,We demonstrate the importance of the deep bidi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,282744e2-ab43-4a46-a94a-6d217676b4b1,rectionality of BERT by evaluating two pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5c2a6131-7bb6-4d1e-ad11-7aa028154d0b,training objectives using exactly the same pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c00b055-768e-4c70-8c94-9186a5a52a05,"training data, ﬁne-tuning scheme, and hyperpa-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f91df16f-504c-4e1e-a154-bfa0ae92e877,rameters as BERT BASE :
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1a03b7e5-6407-49b0-9756-6aab0daaef77,No NSP : A bidirectional model which is trained
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3c4fc215-58e2-48c6-8c1e-ab711af88466,using the “masked LM” (MLM) but without the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ea0c486f-c608-404c-ab5d-b637d0309db2,“next sentence prediction” (NSP) task.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f8504fb9-3fcd-4df6-baf4-649aed792792,LTR & No NSP : A left-context-only model which
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6135e623-b100-462a-86b2-326790d5cf3b,is trained using a standard Left-to-Right (LTR)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2e2d1115-c1df-4fbd-a116-db2f86c040f8,"LM, rather than an MLM. The left-only constraint"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cf4512da-973f-44f3-ab7c-536215d0ff19,"was also applied at ﬁne-tuning, because removing"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5c21bdd9-e2af-41ea-87a5-cef26f69f44c,it introduced a pre-train/ﬁne-tune mismatch that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,29d8ad84-400d-44c9-bcb8-1d3f57c896e2,"degraded downstream performance. Additionally,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d2df066a-03f9-41d4-994b-a35bde43c624,this model was pre-trained without the NSP task.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,255dfbdb-7290-42fa-ae64-5b35c404da24,"This is directly comparable to OpenAI GPT, but"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4dfae7fd-2651-44ef-a969-80fdcf8eb6ca,"using our larger training dataset, our input repre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2fa5149c-c2d8-4b19-af4c-272c25ce1f12,"sentation, and our ﬁne-tuning scheme."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1af2b4db-695d-4be0-a32f-c32f61252022,We ﬁrst examine the impact brought by the NSP
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ea21d448-ff9a-4c6c-8aec-6230f3ff976c,"task. In Table 5, we show that removing NSP"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7f63602b-fae2-405f-aea8-fafc3a37dac3,"hurts performance signiﬁcantly on QNLI, MNLI,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f81d339a-262f-443f-91f8-7d499c9fc98d,"and SQuAD 1.1. Next, we evaluate the impact"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c60a58e3-e60c-4c2b-9425-8e6f64863404,of training bidirectional representations by com-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7b37ff6d-e494-4b13-9143-a226803a251f,paring “No NSP” to “LTR & No NSP”. The LTR
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8192576-1377-40ac-8c3a-5f2506a8925f,model performs worse than the MLM model on all
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9316b0a9-0e2b-4561-93eb-c928dd113d73,"tasks, with large drops on MRPC and SQuAD."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,445b4ba7-82bb-4830-b6db-e5187b91043b,For SQuAD it is intuitively clear that a LTR
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d6a2c74a-d508-4745-a297-ed38372c643a,"model will perform poorly at token predictions,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f71bc3c1-eebe-41e6-89ed-c4807481922d,since the token-level hidden states have no right-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b81f92d9-b9cb-4b15-94e3-3f4cba36d498,side context. In order to make a good faith at-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,96ce93e6-fd5d-4dad-9b6b-f9eed82371e4,"tempt at strengthening the LTR system, we added"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2b770b2e-5212-4034-aeb5-c1e4c611afdd,a randomly initialized BiLSTM on top. This does
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cddc11be-160d-452e-b27e-b8a4d7f0bd84,"signiﬁcantly improve results on SQuAD, but theresults are still far worse than those of the pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f80f9967-b867-4fdc-a855-b25f1875c71b,trained bidirectional models. The BiLSTM hurts
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,496cc614-20a7-4058-9de0-b5330ba7cff9,performance on the GLUE tasks.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b1366a07-0ad8-4bea-943f-c867949a514b,We recognize that it would also be possible to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94f54231-a1d9-4d5b-addd-51cf8fe2e6fd,train separate LTR and RTL models and represent
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,325d34f2-7c5c-4747-88ce-8036b49b4247,each token as the concatenation of the two mod-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,24503783-3f49-49fe-9550-c7beb1f92f16,"els, as ELMo does. However: (a) this is twice as"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a6b86bc7-dff1-45f2-9245-789949caead2,expensive as a single bidirectional model; (b) this
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ad37f795-facd-4aea-99ae-83e56186d360,"is non-intuitive for tasks like QA, since the RTL"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,26866025-c908-41d2-be91-a73980455b9c,model would not be able to condition the answer
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8e47ba8a-72d2-49fc-9296-7e7e70890011,on the question; (c) this it is strictly less powerful
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,de905089-9527-43b7-b658-02176b507f96,"than a deep bidirectional model, since it can use"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2da7b9ae-8e3d-47ba-bcfc-c77389a24b18,both left and right context at every layer.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5ba6a53d-c2d3-4e45-9715-37a63dadd03d,5.2 Effect of Model Size
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,004212ec-571e-4959-b3be-6fe9d070335c,"In this section, we explore the effect of model size"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9268d461-1676-4a44-b388-cce3fb169826,on ﬁne-tuning task accuracy. We trained a number
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3f03de29-34d0-4e57-8b2b-396277ac1443,"of BERT models with a differing number of layers,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,89c5a3b1-f0ee-4bc4-96c4-4d9b3d489691,"hidden units, and attention heads, while otherwise"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5b0b5ded-5547-48ae-8d4a-97a2faec7d0f,using the same hyperparameters and training pro-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,11cf3788-cef2-4fff-9559-525f80d7be81,cedure as described previously.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a537e59b-347d-4073-9919-5fe52627e93f,Results on selected GLUE tasks are shown in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2ed9f8df-6159-43ea-9964-b8435d49ee05,"Table 6. In this table, we report the average Dev"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ed534c90-a769-4345-ade9-d174073fed67,Set accuracy from 5 random restarts of ﬁne-tuning.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4fc777c7-7588-4c04-9e72-e878b5f3f8c6,We can see that larger models lead to a strict ac-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3343dd1b-cba9-4f3d-877d-ab8b5e0acb18,"curacy improvement across all four datasets, even"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9368e010-02a9-429a-9bb2-7ee0946d2c31,"for MRPC which only has 3,600 labeled train-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3a11d12e-ef39-4f45-948a-209291bfeb61,"ing examples, and is substantially different from"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ca5fb34f-79d0-4722-9aa7-7d6e33cdd895,the pre-training tasks. It is also perhaps surpris-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3e217c7a-2f16-4d47-ab00-f5473a04ca88,ing that we are able to achieve such signiﬁcant
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,86a260c7-e84c-4d04-baa2-15eb3e841821,improvements on top of models which are al-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f54ec0d5-b663-40f4-a1ce-7aa9f0316233,ready quite large relative to the existing literature.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,09ef2e2c-1c67-4d5b-a06e-c582af4c3875,"For example, the largest Transformer explored in"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1cc0550a-9998-44cb-9d2b-41274b81365b,"Vaswani et al. (2017) is (L=6, H=1024, A=16)"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0a74c4f7-f451-45c9-8c5c-866c69efedf9,"with 100M parameters for the encoder, and the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,db5ea120-9962-4c43-aadf-11d5b66569b4,largest Transformer we have found in the literature
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7b274468-45f6-4571-8b48-648acdf620c3,"is (L=64, H=512, A=2) with 235M parameters"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4236f27c-e06e-49b1-ac31-525098a8968e,"(Al-Rfou et al., 2018). By contrast, BERT BASE"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1a61b336-96ca-46f7-a6d5-f378dfd686b1,contains 110M parameters and BERT LARGE con-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1a16da41-90c2-400d-ab91-33de024712a2,tains 340M parameters.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,acf320eb-1773-4713-b03f-6c109b68abfc,It has long been known that increasing the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a51048e6-937c-4c31-8036-cb1bec36979e,model size will lead to continual improvements
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c72fae0-effc-46e2-b85c-10e876834a5d,on large-scale tasks such as machine translation
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f1dc1f60-ddad-4c81-aac9-d9795b892939,"and language modeling, which is demonstrated"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d27ad7f9-f417-412c-9cce-a5e1d20ee8a2,by the LM perplexity of held-out training data
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,43f32639-865f-490c-b508-32d72183525e,"shown in Table 6. However, we believe that"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bd3bb31c-b3d0-429c-aa87-512ba4e37e4a,this is the ﬁrst work to demonstrate convinc-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4d853f9b-1e3b-4ba2-b47c-e8b042cfa0c5,ingly that scaling to extreme model sizes also
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa14c4a3-d2e5-496d-be7c-fa405125e7b9,leads to large improvements on very small scale
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1cc88c44-24a4-4d7e-ba39-7a546fd349e7,"tasks, provided that the model has been sufﬁ-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,05d84c4f-b352-4372-9d2c-a3ceb1cbc03e,ciently pre-trained. Peters et al. (2018b) presented
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,89fe4869-e7fd-4bd5-82db-194bfd799de9,mixed results on the downstream task impact of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,72e30dea-69e5-48d7-bdc2-29356a2f75ea,increasing the pre-trained bi-LM size from two
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6e7e3a4-a124-453f-8c51-88fd3d19a331,to four layers and Melamud et al. (2016) men-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c04559af-64b2-4794-8a7b-0387a5869240,tioned in passing that increasing hidden dimen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,515c6686-d009-423e-a55f-1448774537c5,"sion size from 200 to 600 helped, but increasing"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f7eaf52f-0ec1-4e2a-bbff-33c0c5aa8c33,"further to 1,000 did not bring further improve-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f31784d2-613f-479b-b1ad-2ad015bcc6a0,ments. Both of these prior works used a feature-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3841dbc2-98de-4155-b59a-c35b0061a2c2,based approach — we hypothesize that when the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7a3a1631-b136-408f-87b2-2b48ac2673d4,model is ﬁne-tuned directly on the downstream
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,02fdc482-299c-49c8-93f8-4b0a6c5cacc0,tasks and uses only a very small number of ran-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8dd5f1f9-1ef3-4c14-b42b-12ddeb5e134b,"domly initialized additional parameters, the task-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,df8eae1a-e531-4f45-b3c5-ccfc0c15abab,"speciﬁc models can beneﬁt from the larger, more"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,db09b3aa-c162-4beb-b2c3-f8ec8868985d,expressive pre-trained representations even when
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0f7aaa41-e66c-4d29-a7ab-f2a9f1970a47,downstream task data is very small.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ff1822a4-ed0d-4229-9e7c-312b6e9c74e0,5.3 Feature-based Approach with BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dc823102-46ba-4009-b8e5-353f9cbe104a,All of the BERT results presented so far have used
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c4820d0-ebae-4282-ae7e-7f58dc8024f6,"the ﬁne-tuning approach, where a simple classiﬁ-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b3ec2d6a-e460-4427-a355-59aac1c6f921,"cation layer is added to the pre-trained model, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ce379fc4-f2f4-473b-b06a-4989600206db,all parameters are jointly ﬁne-tuned on a down-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fdb57d5f-4b80-4fe0-9e0b-b168fed66c9f,"stream task. However, the feature-based approach,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,20ff5fe5-d500-4835-bb8a-a56a3863ec67,where ﬁxed features are extracted from the pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,abbae8ed-e4e2-4514-954e-7b4cf7b21e6c,"trained model, has certain advantages. First, not"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,17bf4123-f1e7-4980-bd72-5a34141fd6f2,all tasks can be easily represented by a Trans-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e279d9e6-1824-491e-a266-41a1ec728494,"former encoder architecture, and therefore require"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f072af5f-d581-4e45-91d8-7be4b52a8b9a,a task-speciﬁc model architecture to be added.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,57d1035d-3aec-4bc5-9bd8-54017935a5fa,"Second, there are major computational beneﬁts"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5053bf57-285a-4e93-bcd7-b003b5537f0a,to pre-compute an expensive representation of the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cf3338ad-03dc-4501-bcc8-b40ee59d9431,training data once and then run many experiments
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d1860130-915a-47a7-be94-59c558c870a3,with cheaper models on top of this representation.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70c8312a-22aa-452e-9355-a47a3430c4a0,"In this section, we compare the two approaches"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bcb4f8db-38d8-4f84-9c00-6810552e0f21,by applying BERT to the CoNLL-2003 Named
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99a396aa-dce4-44d8-b3a1-7863fb7c0111,Entity Recognition (NER) task (Tjong Kim Sang
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,23807174-90cd-4cf6-9e5b-5e566d06d13e,"and De Meulder, 2003). In the input to BERT, we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,91a95fd2-1282-49c3-8ecf-ae1e4b34cea5,"use a case-preserving WordPiece model, and we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d6ac5695-9224-4664-8783-49a33efba5f5,include the maximal document context provided
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c729cf7-cb04-499e-ac53-a4bfd3788665,"by the data. Following standard practice, we for-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5af65962-0403-4d20-8fa0-d1d1a0559e7d,mulate this as a tagging task but do not use a CRF
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,120a8cf9-805b-42e1-867d-c15546b135f8,Hyperparams Dev Set Accuracy
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77777d87-93ba-485c-acb8-c3af7853b4de,#L #H #A LM (ppl) MNLI-m MRPC SST-2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,da6b8bee-4570-481f-bc85-e6e8d5a60d84,3 768 12 5.84 77.9 79.8 88.4
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c069f8ca-f284-4390-9800-6fd1a0203e33,6 768 3 5.24 80.6 82.2 90.7
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,82ca7f84-ea40-4746-bce7-55c215cf697b,6 768 12 4.68 81.9 84.8 91.3
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cfd00437-c6d1-4e6b-a92a-3624a8de46d9,12 768 12 3.99 84.4 86.7 92.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,39ca4541-74ec-4d1c-9f13-1098bf85e15c,12 1024 16 3.54 85.7 86.9 93.3
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1854ca0c-28a3-4e52-9436-b1cb92f5328f,24 1024 16 3.23 86.6 87.8 93.7
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c0d37c7c-76fb-4bdd-bba0-21cdae92d38b,Table 6: Ablation over BERT model size. #L = the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2c07bd69-cc13-4348-a6f7-135a8d991813,number of layers; #H = hidden size; #A = number of at-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4b0e57e-4b7b-493a-9ac3-6defe18aa746,tention heads. “LM (ppl)” is the masked LM perplexity
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,24ac7a14-7ccd-424b-be5c-b0ede47ec01f,of held-out training data.System Dev F1 Test F1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ed61038-f801-49dd-b96d-e3c1b2fab7cf,"ELMo (Peters et al., 2018a) 95.7 92.2"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,96c371ea-31e2-4b6a-8c25-f8cd2f4a4fe9,"CVT (Clark et al., 2018) - 92.6"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,21cf2c88-155d-489d-835f-73c718ed62aa,"CSE (Akbik et al., 2018) - 93.1"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,696b89eb-228a-4518-b459-64c8072ef22d,Fine-tuning approach
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99bca4a9-89de-42c6-a38b-5507a7792d46,BERT LARGE 96.6 92.8
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3bf05e49-cc7a-4df1-87b6-6ca3647592df,BERT BASE 96.4 92.4
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a85caf7a-c407-4519-9d0b-fec5f8287261,Feature-based approach (BERT BASE)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b04d1951-abc0-4668-a0e2-be23b243b5f2,Embeddings 91.0 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ec79786a-902e-4eca-b520-afdd88eabf20,Second-to-Last Hidden 95.6 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5e1870b3-923a-4134-bbcd-1721d24dc734,Last Hidden 94.9 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77284b7b-478b-4cad-ab93-f666ebad59ac,Weighted Sum Last Four Hidden 95.9 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,50efebee-5ac5-4dbe-8836-a68ff7e37f17,Concat Last Four Hidden 96.1 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c1159c7-5843-449d-9b83-50f720fc255c,Weighted Sum All 12 Layers 95.5 -
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d863a8c2-31c1-4895-8fdd-4c7fec64def5,Table 7: CoNLL-2003 Named Entity Recognition re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ac8f65fd-7f8e-492c-a5ed-29c45193c132,sults. Hyperparameters were selected using the Dev
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1743afd3-d959-426d-9d3c-386a69d4e580,set. The reported Dev and Test scores are averaged over
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e61bfa78-ec60-4026-a54d-ccb7efbc1474,5 random restarts using those hyperparameters.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eccb9b03-aae5-4224-9a65-e5994501d7f6,layer in the output. We use the representation of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,867cc4d1-6b32-4599-a317-881050d85213,the ﬁrst sub-token as the input to the token-level
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7682efef-8105-4cc9-a41b-592362882435,classiﬁer over the NER label set.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0178b386-ba55-4425-96eb-c7aa94b0b0bc,"To ablate the ﬁne-tuning approach, we apply the"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,229de53f-d99c-4065-bb2a-a4807440f6f0,feature-based approach by extracting the activa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,79048f99-591a-4465-a09e-50e560efe494,tions from one or more layers without ﬁne-tuning
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9002688-b4bd-4ddd-8919-3a45c972587f,any parameters of BERT. These contextual em-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b32878a-e3f5-49e1-bf1c-2fd6115d54d8,beddings are used as input to a randomly initial-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5310165d-55b7-4a08-9683-429783221e56,ized two-layer 768-dimensional BiLSTM before
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d50c5e56-18e0-4fd4-a05c-0df6c287b46d,the classiﬁcation layer.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9700a336-459a-4c0b-b04c-d26b10d8ff20,Results are presented in Table 7. BERT LARGE
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e5273f46-f7ee-4c49-92e1-e30f9debd2cd,performs competitively with state-of-the-art meth-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a6061132-6b8e-49c6-8d22-e07c37856cc2,ods. The best performing method concatenates the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9352273e-96c7-44ef-b306-292cb1b5dd49,token representations from the top four hidden lay-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,04d353be-17ce-4288-bffb-d15d1feeb100,"ers of the pre-trained Transformer, which is only"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6ed7b2c1-c8ea-4fc0-bca6-dc0b814a3385,0.3 F1 behind ﬁne-tuning the entire model. This
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,efe3981d-6854-4e2b-a9d7-fdea6b983f8f,demonstrates that BERT is effective for both ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ce9b1aa9-ed58-4ca9-b667-71680c22b0f1,tuning and feature-based approaches.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c51c3886-cf7e-415b-900c-3bc95758bd82,6 Conclusion
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9ac4306-63af-48e4-b80f-5dead43aa775,Recent empirical improvements due to transfer
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70f2e2cb-9465-4461-be2d-4cfab092d980,learning with language models have demonstrated
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e05d4898-55a9-4a03-a743-bdd85fdd9561,"that rich, unsupervised pre-training is an integral"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,142f6419-3045-4030-9b61-c10ecf86f6d2,part of many language understanding systems. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ac2a69bd-acbb-4cc8-aea7-c1c387c92b3b,"particular, these results enable even low-resource"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,febdef6e-71dd-48c1-b2d6-48079f504b1a,tasks to beneﬁt from deep unidirectional architec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,00affc6e-09a2-45f3-8b7d-4c5c8575c242,tures. Our major contribution is further general-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2dc4e933-7252-4703-b763-fe3565f921a1,izing these ﬁndings to deep bidirectional architec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,97e6813e-7dc9-40f1-b9b0-fafe075254aa,"tures, allowing the same pre-trained model to suc-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,661fcefb-79b8-4f8d-969b-f5a9061aa51b,cessfully tackle a broad set of NLP tasks.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,00dbbf43-7094-4ae7-be45-8ae4d11459cf,References
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c743866-391b-45d9-8707-672646511af3,"Alan Akbik, Duncan Blythe, and Roland V ollgraf."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a14242c4-2b5c-4e3b-937d-014f63bb09c4,2018. Contextual string embeddings for sequence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4483167e-c9d5-4a40-9b7d-b804d36f934a,labeling. In Proceedings of the 27th International
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bea944c6-5361-4de5-b696-134122513a40,"Conference on Computational Linguistics , pages"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5243a965-ebca-41fe-b5a8-0b93ffb1cf24,1638–1649.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ff3afd0c-f4e5-4856-b0f6-8303800dbd7e,"Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c5ec9467-3c77-482f-af5f-2e1b2812a81c,"Guo, and Llion Jones. 2018. Character-level lan-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c44c12a5-f5a2-4891-8c00-0ddce2a91b1d,guage modeling with deeper self-attention. arXiv
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,46695147-aa79-4ceb-9c82-238f1e8c2d34,preprint arXiv:1808.04444 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba14bcdb-9029-441c-b8b5-f9658524fb14,Rie Kubota Ando and Tong Zhang. 2005. A framework
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fadc947e-8b00-49a1-91f6-b0adf8844761,for learning predictive structures from multiple tasks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cbc3d41b-c9b1-4ef4-bcba-ae506d455824,and unlabeled data. Journal of Machine Learning
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,37e6df7a-a873-4700-9d33-c17630b65ec3,"Research , 6(Nov):1817–1853."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,538fe0ea-d9d0-4157-8bb1-6cb11917a61b,"Luisa Bentivogli, Bernardo Magnini, Ido Dagan,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,57363a07-7f88-44e2-84ce-c60a6b12f1fb,"Hoa Trang Dang, and Danilo Giampiccolo. 2009."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fa16be9c-8cc8-40a5-a911-fe6a4b5450b1,The ﬁfth PASCAL recognizing textual entailment
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,16c8f097-df37-45f4-8b41-55671f60e411,challenge. In TAC. NIST.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,efe4012b-5db3-43b4-9d15-fae3a9847d2f,"John Blitzer, Ryan McDonald, and Fernando Pereira."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f7689bcd-a594-42e6-871e-431a51c56feb,2006. Domain adaptation with structural correspon-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba63a9bf-fac4-4c73-aafd-df08615f5288,dence learning. In Proceedings of the 2006 confer-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5304a80f-f4a1-4f55-ad8a-1fb388254576,ence on empirical methods in natural language pro-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f3a29fc4-a5df-46e5-acf5-ba908358fc0b,"cessing , pages 120–128. Association for Computa-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1b03edb9-9722-4e97-b618-55ebb607b267,tional Linguistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c430f39-be53-4813-80c1-6e06bfdef0b1,"Samuel R. Bowman, Gabor Angeli, Christopher Potts,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e3bd5e65-8d79-44db-b452-4d93d88124ed,and Christopher D. Manning. 2015. A large anno-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d5f3bbaa-1459-4d79-93ff-74742078bec0,tated corpus for learning natural language inference.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b9c0ae96-43c4-4896-8075-ebafb6b9247c,InEMNLP . Association for Computational Linguis-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b47244d7-d97d-448d-b280-106f6e885bf4,tics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,65bed14e-f9c8-47c2-9d2a-1fb34a62f63c,"Peter F Brown, Peter V Desouza, Robert L Mercer,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f6e1a514-85cc-4e09-9969-e8aa399f6dbd,"Vincent J Della Pietra, and Jenifer C Lai. 1992."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba1bde1e-7742-4bd4-b159-d8682d6bf2d8,Class-based n-gram models of natural language.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40829b8e-5e82-427d-83c6-c1b9f9fad851,"Computational linguistics , 18(4):467–479."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,282fb1f9-82e1-4909-8651-de6559851d37,"Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,76fbfb0d-3452-4d8d-8d23-4ebaf3ab7532,"Gazpio, and Lucia Specia. 2017. Semeval-2017"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,edaaada3-a408-4e59-a006-47bd422d11a3,task 1: Semantic textual similarity multilingual and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,edfc65e7-6a23-46d3-af01-0519d93fc9f2,crosslingual focused evaluation. In Proceedings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a3c18f9b-1732-495b-8e20-3d70b23e43cc,of the 11th International Workshop on Semantic
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,15aaf707-efd8-482f-970f-94bed36b0811,"Evaluation (SemEval-2017) , pages 1–14, Vancou-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e154e00a-f4ac-45ca-9b3f-2a689d0130c5,"ver, Canada. Association for Computational Lin-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ccebcd64-19c3-49c6-b817-615912433a00,guistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,11e14e18-fb2d-4d41-a742-c91f8c7b49d1,"Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d5035876-7e38-4359-861e-fb1e015abe61,"Thorsten Brants, Phillipp Koehn, and Tony Robin-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7fbe98d1-eb64-4d9a-80bb-792196049520,son. 2013. One billion word benchmark for measur-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,76e47bce-aca0-4f1f-9727-659ddac9d5ea,ing progress in statistical language modeling. arXiv
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e735c37-54b4-4be4-b103-cb6878dd0845,preprint arXiv:1312.3005 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a4b858d4-b52d-4f60-92f5-cb2eaeca4334,"Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bcc915c1-1279-4e3e-a12f-b5e475587864,Quora question pairs.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9cce6771-2448-44ff-9ca0-1388b4c11362,Christopher Clark and Matt Gardner. 2018. Simple
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d71de1ea-4ede-40fe-a184-6b337391065b,and effective multi-paragraph reading comprehen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3acd05e0-6cd3-4903-ae51-f55892e3fd81,"sion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,79b5b602-6fb4-4494-86ba-eeb14ab5490c,"ning, and Quoc Le. 2018. Semi-supervised se-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ef4b6dd-a573-4793-9a1e-35eeb7b5d6d6,quence modeling with cross-view training. In Pro-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d37253e5-881a-4802-8d15-ab19546ec8e2,ceedings of the 2018 Conference on Empirical Meth-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1905e4b4-8622-4d51-aae7-05367bad0c11,"ods in Natural Language Processing , pages 1914–"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a5e53c0f-b3c6-46d7-a18f-7f42f0fdd4d4,1925.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b6f3baff-7e67-40f3-92a9-31aa0f8110c5,Ronan Collobert and Jason Weston. 2008. A uniﬁed
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e09149bb-931e-4725-b3f7-0ee59539c563,architecture for natural language processing: Deep
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2f8f59dc-0a88-4dc7-949f-794131d7f0c5,neural networks with multitask learning. In Pro-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9c27f278-1728-4822-b468-f9c0d86cf84c,ceedings of the 25th international conference on
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d4037f9e-dbfd-4df8-b39b-a136052a01fa,"Machine learning , pages 160–167. ACM."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3201c43b-9588-47e1-bee5-579759144ee9,"Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,50268b50-cdaa-442b-aaea-6fbf5773f450,"Barrault, and Antoine Bordes. 2017. Supervised"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dd50eaf5-1362-49c0-a6d0-6300ffa1e65e,learning of universal sentence representations from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fd842823-e0af-4170-aca3-0d548776ff63,natural language inference data. In Proceedings of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a2018dbb-3ab6-411d-8d59-81eda8ddf191,the 2017 Conference on Empirical Methods in Nat-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f685d11f-1bd4-4c3f-b997-048067384d1c,"ural Language Processing , pages 670–680, Copen-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,677b5d20-812e-4667-8fbf-29de2b20435c,"hagen, Denmark. Association for Computational"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e6b0dab9-c3b5-4096-b284-77bae57fd91e,Linguistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5ba8eb6-e619-44ad-9871-043f451404e8,Andrew M Dai and Quoc V Le. 2015. Semi-supervised
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a12d041f-ac0a-45ea-8857-06f0388c55eb,sequence learning. In Advances in neural informa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b853d3b7-b713-4b0a-85cf-4722284f9892,"tion processing systems , pages 3079–3087."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,343546df-e6cc-489b-ad33-a2e66bcb5885,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,988fbbf6-7077-4da8-bf46-b49873c76cf9,Fei. 2009. ImageNet: A Large-Scale Hierarchical
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d3328e94-a9e8-4997-b343-9c7e4fc92862,Image Database. In CVPR09 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6cc2b8c0-b5d5-4884-b5c9-e032852bedfc,William B Dolan and Chris Brockett. 2005. Automati-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c74f147-5160-4ad1-b13b-d6443b77fa4d,cally constructing a corpus of sentential paraphrases.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,75724815-2868-4a00-bdab-6b8a51301d51,InProceedings of the Third International Workshop
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d7ce6807-d42a-4d1f-8cba-bccfa45bb845,on Paraphrasing (IWP2005) .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f682508-7f7e-45a7-9c4c-be513d0cdc9a,"William Fedus, Ian Goodfellow, and Andrew M Dai."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c4fa6056-b21c-4dbb-b542-30be4548e1de,2018. Maskgan: Better text generation via ﬁlling in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b2dfbde4-d8f6-48ff-98ed-4c38bfc6cdbe,the.arXiv preprint arXiv:1801.07736 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0717bcbe-cea3-4794-b870-b19545b5caf3,Dan Hendrycks and Kevin Gimpel. 2016. Bridging
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a353d923-3947-4f43-8cae-7b4d27223ea3,nonlinearities and stochastic regularizers with gaus-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b61e0d45-13de-4ddc-8a36-17c06a4351c2,"sian error linear units. CoRR , abs/1606.08415."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4bee153f-65a3-476d-8bf5-fac602a10aa1,"Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c5bc9bc5-5f93-4a95-a51e-95728001e00a,Learning distributed representations of sentences
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,966df458-0ba6-436e-8a33-c8e1bb51d315,from unlabelled data. In Proceedings of the 2016
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ba40140-29d6-4ab8-ac1a-aaa411b0ae97,Conference of the North American Chapter of the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a7a8b5b9-0dc1-40fc-ac5d-9fc2027fb14a,Association for Computational Linguistics: Human
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8d041124-4ea9-4f76-ae4d-93ea40e0c523,Language Technologies . Association for Computa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c5af3c9-89e4-4c94-931b-3427a3779266,tional Linguistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,38cc24fb-6b34-4508-b8c0-e4a3e3ec03e3,Jeremy Howard and Sebastian Ruder. 2018. Universal
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d23826ab-5523-4bcc-9b11-480bb32d4041,language model ﬁne-tuning for text classiﬁcation. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ee2cee7-3e56-40ae-865f-a0c33d7807f4,ACL. Association for Computational Linguistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2ac372c8-aa02-4014-a16c-1140183b0715,"Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,da546261-177f-4a04-bbf9-30e3a1b5ebb3,"Furu Wei, and Ming Zhou. 2018. Reinforced"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cff92df4-6f43-4873-af60-aa89df60334d,mnemonic reader for machine reading comprehen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa3ac806-e19b-4846-aba2-e647065cc2d5,sion. In IJCAI .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d1ef5cef-5ac5-4e4d-b240-d485c835441d,"Yacine Jernite, Samuel R. Bowman, and David Son-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f8251a64-6efd-4d4e-a763-580033f00f1c,tag. 2017. Discourse-based objectives for fast un-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9640051-1b7e-4609-9356-d4575504cf8f,"supervised sentence representation learning. CoRR ,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,993330af-1f44-4b2c-b4cb-fdcf99675f87,abs/1705.00557.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5d5cad78-7e8e-4abe-9362-3c64acd34d10,"Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,10d0802c-8a2e-4723-b01d-d6d1d59ddf8d,Zettlemoyer. 2017. Triviaqa: A large scale distantly
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40a0aed6-ab29-4403-b65d-ae9a7b14b5a7,supervised challenge dataset for reading comprehen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b3993824-089a-4c56-9170-2e16c95fde6b,sion. In ACL.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,028bdd94-9791-47f1-98fa-b7046419c9c7,"Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e7ea59d7-48d8-467d-9a54-0850aae3e6cb,"Richard Zemel, Raquel Urtasun, Antonio Torralba,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,635b2df5-039d-4634-b757-c24858006dca,and Sanja Fidler. 2015. Skip-thought vectors. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8b9509cb-0a7c-4620-9fc5-15d50c0b15c4,"Advances in neural information processing systems ,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,119df4bd-7a3b-4537-87ff-c8770f1bdac6,pages 3294–3302.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ad91291-8904-4d40-b1ad-569c2bbcc012,Quoc Le and Tomas Mikolov. 2014. Distributed rep-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,71e7cc5a-76fd-43d2-89ac-9b95d8aa6f8d,resentations of sentences and documents. In Inter-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,24a7fb21-a7b2-47c6-b0f1-db0ef1bec036,"national Conference on Machine Learning , pages"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ed7be44-3697-49f9-bbb9-1751474eabfd,1188–1196.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,546a8add-be8a-4108-b77e-91552cd14adb,"Hector J Levesque, Ernest Davis, and Leora Morgen-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4f779a8b-6090-46ea-bb01-d06351e42f25,stern. 2011. The winograd schema challenge. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7fbca64e-d4c4-43b3-8956-d555a1b7c3d9,Aaai spring symposium: Logical formalizations of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6e1b0d76-83dc-48a3-840b-0a9734bc7c3f,"commonsense reasoning , volume 46, page 47."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e18dc2d-a2c7-458a-9a55-f3a58aaf64c6,Lajanugen Logeswaran and Honglak Lee. 2018. An
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fce08d55-3877-4d5e-a07b-dc338e94d4e7,efﬁcient framework for learning sentence represen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1d2bd8af-c4b2-47ca-b091-8a1c1f9b327b,tations. In International Conference on Learning
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f94f4508-1b28-4114-9fbc-822272857058,Representations .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2978cec9-fc95-4702-a42b-9029f6ec46bb,"Bryan McCann, James Bradbury, Caiming Xiong, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3ba9136e-460d-40e1-95e4-47dcc5c2729e,Richard Socher. 2017. Learned in translation: Con-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b3076601-7e86-482d-afe8-cc48f5f1f045,textualized word vectors. In NIPS .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5d1a290-ee07-4411-8745-3dd7f8832c86,"Oren Melamud, Jacob Goldberger, and Ido Dagan."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c418e02b-8e6a-4a5b-9ce1-493968545701,2016. context2vec: Learning generic context em-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,abd6e67a-fd05-4f4c-ad80-6c2d1c0cd686,bedding with bidirectional LSTM. In CoNLL .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7886be6c-c806-4ef1-953b-8f1889c0bbc4,"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1aa0db18-a62c-4138-b146-c1d3b9408713,"rado, and Jeff Dean. 2013. Distributed representa-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2a97e0f4-dc7a-4fac-b7dd-d30aa47f6c62,tions of words and phrases and their compositional-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e882e97c-6fb6-474c-a284-971307926a3a,ity. In Advances in Neural Information Processing
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,167292fd-ed9e-4fc6-a02d-3b98129d4105,"Systems 26 , pages 3111–3119. Curran Associates,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,441901fb-99ec-4c65-b9b7-5b655f83a557,Inc.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9d9a78d9-991b-499e-8d20-fa11009a3f9d,Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b52b4112-a910-48ee-b441-6092d8edbb88,able hierarchical distributed language model. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,43c14209-de14-4738-8339-b6570db86208,"D. Koller, D. Schuurmans, Y . Bengio, and L. Bot-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8920f5a6-77da-4899-bebf-bd6a0ff8a30d,"tou, editors, Advances in Neural Information Pro-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f594e626-3431-4587-b1b6-68dac077590b,"cessing Systems 21 , pages 1081–1088. Curran As-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,91acdbba-7da3-46b7-882f-90527fb4c6db,"sociates, Inc."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,25c148cb-3bab-465a-aa3a-f0beb9f2c1b2,"Ankur P Parikh, Oscar T ¨ackstr ¨om, Dipanjan Das, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,de38939c-b229-4853-b877-74ac7d071ca9,Jakob Uszkoreit. 2016. A decomposable attention
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7fc8fe89-5409-4fac-9300-1c7a6b8721e0,model for natural language inference. In EMNLP .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,30f380b5-5375-4bc4-9d38-bc5ad3810481,"Jeffrey Pennington, Richard Socher, and Christo-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2728d07d-a0a5-46a1-9bba-6d79a53db51e,pher D. Manning. 2014. Glove: Global vectors for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a56675bb-555d-4fff-aa2e-bc46afd636e9,word representation. In Empirical Methods in Nat-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,757b6d3b-1244-4957-a260-a851d6d650da,"ural Language Processing (EMNLP) , pages 1532–"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,abf19a70-b52e-459f-b06a-ae348d6fa2b6,1543.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cef205fd-3037-4b0a-89ae-72a80d68e6e9,"Matthew Peters, Waleed Ammar, Chandra Bhagavat-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7d1405e3-cd80-406c-9a00-cdb3330fc300,"ula, and Russell Power. 2017. Semi-supervised se-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,76e0297a-6ad3-439e-af78-99675a042cfa,quence tagging with bidirectional language models.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b37cad54-6d70-40a1-a1e5-7d01ae3eec8c,InACL.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,10bc8188-f1bd-484c-8d10-d68c283bb313,"Matthew Peters, Mark Neumann, Mohit Iyyer, Matt"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8c646831-a3b2-40e6-93c7-2ab198e70df7,"Gardner, Christopher Clark, Kenton Lee, and Luke"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fdaf9bab-ce9c-4e5c-aa0a-b71b1faa4250,Zettlemoyer. 2018a. Deep contextualized word rep-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,35cfe279-d331-40fe-b73d-da421eed25db,"resentations. In NAACL .Matthew Peters, Mark Neumann, Luke Zettlemoyer,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8a649999-39d9-4e4b-82dc-892a7f75b16d,and Wen-tau Yih. 2018b. Dissecting contextual
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2477e3e9-53b3-4240-a2df-417288e9ea73,word embeddings: Architecture and representation.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4b588063-10fc-4c23-9ed3-05e39ebf98c7,InProceedings of the 2018 Conference on Empiri-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,432ecaf5-3987-40bd-82ca-cd722a935e27,"cal Methods in Natural Language Processing , pages"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2021bfce-7e38-4a47-bfb4-a5380438f4db,1499–1509.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6e80f252-439f-466c-bfca-464a38a83cc4,"Alec Radford, Karthik Narasimhan, Tim Salimans, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,91dff002-dc32-4c63-a9e0-e07371eecbcc,Ilya Sutskever. 2018. Improving language under-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2c448c8f-6f02-4553-ae75-2d86bbaf0875,standing with unsupervised learning. Technical re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ec9b7692-5b26-402b-8159-9a93b88fc8a4,"port, OpenAI."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dc121b95-2ac4-4564-bd19-fb5da9cda392,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2dcc2161-4421-495a-bc19-1a28899ff53e,"Percy Liang. 2016. Squad: 100,000+ questions for"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9c429b6c-43bb-4f56-8d2e-0eec0aa282d1,machine comprehension of text. In Proceedings of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42505484-5ae0-4ef5-9aef-8b4d968c7e86,the 2016 Conference on Empirical Methods in Nat-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3bfa7c64-c898-48ca-98f7-19fdc001edca,"ural Language Processing , pages 2383–2392."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,79383104-6834-4bb3-aca1-dc5ea9a3cb5f,"Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f6eaee7b-0869-4db5-8913-bc24aed1c59a,Hannaneh Hajishirzi. 2017. Bidirectional attention
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cd060d55-b5e7-48f9-8b20-2497ffc78156,ﬂow for machine comprehension. In ICLR .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0b7cd4dd-9ffa-426f-bb0e-a7647a4c7676,"Richard Socher, Alex Perelygin, Jean Wu, Jason"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08c37723-2607-4aaa-bdc2-c068055231a0,"Chuang, Christopher D Manning, Andrew Ng, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6cc8dded-2256-40ab-93e0-07eaec0f0994,Christopher Potts. 2013. Recursive deep models
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2122a708-bcf5-49a8-952b-b9f030f64384,for semantic compositionality over a sentiment tree-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,140b5641-9792-4a00-957e-a7c5e9fd855f,bank. In Proceedings of the 2013 conference on
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6d64584a-bfd8-47e2-bb7d-771d9bbeff1d,"empirical methods in natural language processing ,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e61a2a79-eb7b-4ba8-bccc-e37b4492881a,pages 1631–1642.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,53853736-1bd6-40c1-9d2b-525512c6a5c4,"Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7d789d21-258e-4b4b-b211-0e115e0dded1,2018. U-net: Machine reading comprehension
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fe085e2b-0e7f-407d-8059-b8afd31f59c5,with unanswerable questions. arXiv preprint
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d2624abd-bd35-4563-bbd6-8c51fd374469,arXiv:1810.06638 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4079692e-aa63-4d9d-aebe-97a213f31e5f,Wilson L Taylor. 1953. Cloze procedure: A new
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,98aa97b5-b774-4a02-90e9-ce692e9335c7,"tool for measuring readability. Journalism Bulletin ,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,09a00e9a-dc29-4064-8b1c-7a49d81b7579,30(4):415–433.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ec047ce-37ad-49b6-bd8e-6f5fe84c06fc,Erik F Tjong Kim Sang and Fien De Meulder.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eb312e01-e132-45d7-9514-ccf99a3e8ffa,2003. Introduction to the conll-2003 shared task:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,02b59a06-a726-4ff1-97d8-4c15c55927c8,Language-independent named entity recognition. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,317de896-8daa-4e9a-a7be-a6bc593f22aa,CoNLL .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70d7bda2-8623-4074-875e-e5c61528dab6,"Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,092fd601-281e-483f-af0a-1280bd1197a1,Word representations: A simple and general method
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ad12867f-b720-4428-8e67-8ea598f78032,for semi-supervised learning. In Proceedings of the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,97bc69fc-66fd-4b80-9cc8-fbcb9b76826f,48th Annual Meeting of the Association for Compu-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6881ad5-0b26-4a6a-ba66-a51b594c458f,"tational Linguistics , ACL ’10, pages 384–394."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b8e8a273-b804-40c9-8288-568f43e06f74,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94047d24-5c93-4758-9627-5045cfaa5e67,"Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c396800-2f2c-420f-9c57-458b05580082,"Kaiser, and Illia Polosukhin. 2017. Attention is all"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f10e40c-ec73-4f18-92d7-df533522b6fb,you need. In Advances in Neural Information Pro-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c3d18887-f561-46ee-806f-0f8526a62797,"cessing Systems , pages 6000–6010."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bd7719e3-4baf-43f8-bc88-16a61339991c,"Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,88b68a2c-c3dc-4c7b-8934-2cb53ede09be,Pierre-Antoine Manzagol. 2008. Extracting and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9d43ab80-1b4c-4716-8e8d-bdf88e61ecf2,composing robust features with denoising autoen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6623990a-8285-401f-b56f-2c5b6b9ac67b,coders. In Proceedings of the 25th international
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,19399dea-9b24-4433-abd8-7585ac02edbb,"conference on Machine learning , pages 1096–1103."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,30f2c978-ea1f-4940-8bdc-1c3e4071873e,ACM.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,43031348-92e1-4b25-bb2f-aff5461d4510,"Alex Wang, Amanpreet Singh, Julian Michael, Fe-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c5e2ef8b-e77d-45a9-9232-193e952717db,"lix Hill, Omer Levy, and Samuel Bowman. 2018a."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,90da1ecd-9945-4d93-a6cb-0abccd2cd2ba,Glue: A multi-task benchmark and analysis platform
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0680cb9a-6745-42e5-a363-3dec36f21c83,for natural language understanding. In Proceedings
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ccea1bec-5766-460c-b5d3-0a3593258929,of the 2018 EMNLP Workshop BlackboxNLP: An-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2be7e661-163c-416c-be53-b8b26b03243b,"alyzing and Interpreting Neural Networks for NLP ,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,871e8b77-42c9-435f-b2ad-dd3ba2830748,pages 353–355.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9d35e6f6-e5d4-4bf5-9f22-4611cd954f62,"Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6420231e-34af-42ca-a9a2-34ec11a0ccca,granularity hierarchical attention fusion networks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a744a1b5-704b-407e-a73b-9ccdaa22b505,for reading comprehension and question answering.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c4da77c6-ba62-4465-80c7-5eb07e13a959,InProceedings of the 56th Annual Meeting of the As-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bdbe1274-ac56-4015-9579-89b1353472d2,sociation for Computational Linguistics (Volume 1:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5903f256-a119-4973-bc4a-b1c2a6de2f37,Long Papers) . Association for Computational Lin-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dbbb6f25-e5a3-4208-ab41-11262498f56e,guistics.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,294c0732-4086-4941-b88d-a4484676a7e3,"Alex Warstadt, Amanpreet Singh, and Samuel R Bow-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5fb225e9-17d5-4bb5-97f6-52d210567cbd,man. 2018. Neural network acceptability judg-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0ccca144-2daa-4c44-a0e9-a6769b127b14,ments. arXiv preprint arXiv:1805.12471 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aff9f72a-e7ce-4d61-ad68-ed82b9fc660c,"Adina Williams, Nikita Nangia, and Samuel R Bow-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,09623b2a-dd67-42ed-b17e-0ccfd246191e,man. 2018. A broad-coverage challenge corpus
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,93ea3f82-3497-4665-aaec-0bcd66a2def4,for sentence understanding through inference. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e5fcda97-0259-4af0-8f55-2187be424088,NAACL .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a3c42a25-9f98-4a48-8d97-8b90e54a8c98,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4f075935-1a75-43b1-84d6-0b78d493bc8f,"Le, Mohammad Norouzi, Wolfgang Macherey,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3863e971-1994-4490-b33c-83c4bbc10f52,"Maxim Krikun, Yuan Cao, Qin Gao, Klaus"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f287cc66-184f-43aa-bf75-722b9a2dd1d2,"Macherey, et al. 2016. Google’s neural ma-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b88dcd88-af7f-4203-8fd4-dd573b69424b,chine translation system: Bridging the gap between
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0dbe84da-005a-4afe-a9bd-aa9d95d8bf71,human and machine translation. arXiv preprint
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,320e8118-20a9-4f37-b1c8-bd0b7209ad1a,arXiv:1609.08144 .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,defa32c9-ec1c-45b9-ab8e-e1ab504a5f9e,"Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7d31b491-8893-400e-a97c-112415a5a7c8,Lipson. 2014. How transferable are features in deep
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,67d8adfb-9a10-470c-bdf8-575d832246b7,neural networks? In Advances in neural information
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f74ece13-30c1-4d2b-bf47-b35076f327f9,"processing systems , pages 3320–3328."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08045f96-ff5c-48fd-9657-c471db423f79,"Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c55a0c62-20a3-48b5-9a32-6dfece96fe4e,"Zhao, Kai Chen, Mohammad Norouzi, and Quoc V"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3983c8d0-4720-4136-bd10-b0ede4e68ee2,Le. 2018. QANet: Combining local convolution
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fd56daa8-e989-4675-b9d9-45b2a93c2d24,with global self-attention for reading comprehen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,86d06342-28c4-4c8f-8774-f31529c4cb56,sion. In ICLR .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc0b122a-318d-4d60-8caa-104899dee968,"Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f69d67d7-d3df-4d79-a3f3-3f897d2fa71a,Choi. 2018. Swag: A large-scale adversarial dataset
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e9df5856-2532-42c4-ad25-4de8604bcf52,for grounded commonsense inference. In Proceed-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cb286b5f-6779-4fa9-87e1-62be1fe19785,ings of the 2018 Conference on Empirical Methods
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4df70d79-d608-414b-b36f-2be8c71ec941,in Natural Language Processing (EMNLP) .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8c850432-edec-4fb6-8ad2-7da58db6cf57,"Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b4e56029-820b-466e-ae1a-d35ec35aed5d,"dinov, Raquel Urtasun, Antonio Torralba, and Sanja"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,20b08deb-766a-41c8-994d-5c04e1bf1b02,Fidler. 2015. Aligning books and movies: Towards
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e0a38bae-105f-4265-bf96-289377a50228,story-like visual explanations by watching movies
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,98408a13-1b95-448b-8f83-3043560f96b8,and reading books. In Proceedings of the IEEE
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7bd3c550-00fe-428c-982f-ba29daced54c,"international conference on computer vision , pages"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,921fadf2-7710-45e9-9d16-ca739697b319,19–27.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6489eab6-dfad-4144-9c66-b241d9a21bcd,Appendix for “BERT: Pre-training of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d99bdb08-0812-413a-a404-82e4ddd2ce47,Deep Bidirectional Transformers for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,185c844a-4df0-4d3e-bf8a-f1e2b5349a13,Language Understanding”
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dfc4297a-2467-4594-8e06-324e8c33744c,We organize the appendix into three sections:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1ac22ee6-062a-444c-a487-a72f2e9adc1a,• Additional implementation details for BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ad39fb15-f6db-4c15-99cf-d652aec732ff,are presented in Appendix A;• Additional details for our experiments are
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8370e4e4-5e7b-466f-b652-e199f243e349,presented in Appendix B; and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7ba73aae-3339-426a-be9b-144c935404be,• Additional ablation studies are presented in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ca33b067-cce8-4039-a674-e7819d278230,Appendix C.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc40ac2d-8898-4575-99c9-5c42dab26926,We present additional ablation studies for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,62bf0a64-d898-407a-8e5e-383b9ea8a34b,BERT including:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a9f663a4-32ad-48dc-be43-6ebb90542f46,–Effect of Number of Training Steps; and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,119f21fb-312c-4eec-b69c-078b276291bb,–Ablation for Different Masking Proce-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8e9c6459-1f52-4e8a-a81d-c56ff5fe3e8e,dures.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1037d370-8860-4614-9f21-f5e8907adf77,A Additional Details for BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,eeeca580-2649-4a2e-87be-a73bcf13bea4,A.1 Illustration of the Pre-training Tasks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2d362ede-9e8c-4b94-9729-4bc308f8bdd5,We provide examples of the pre-training tasks in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,463e37d9-25c4-48cb-95d2-1b8ff4fab688,the following.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c649ecc6-5eba-41f3-b924-a3e54279d038,Masked LM and the Masking Procedure As-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a03a695f-7302-4444-9db1-37fa0a2326c5,suming the unlabeled sentence is my dog is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb1a4658-fbe1-4b4a-bee8-c6728b1e9d1a,"hairy , and during the random masking procedure"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a3b42a2b-418a-4772-9ee4-f410aa2fa5a0,we chose the 4-th token (which corresponding to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0c268633-02ca-4377-a7e4-cc5f9b42fc4f,"hairy ), our masking procedure can be further il-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1596d913-a2c5-434b-bfe6-5495076e6bd0,lustrated by
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,baa88e74-e0f4-4990-8af8-06519a647b68,• 80% of the time: Replace the word with the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4c3b8272-aa63-475a-a8a5-18cea8421d16,"[MASK] token, e.g., my dog is hairy !"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,358e6083-f097-499b-b769-2ddc077b2b66,my dog is [MASK]
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a7dc3a23-e898-4cf6-a52a-210660309ea0,• 10% of the time: Replace the word with a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d2f8e571-7fe2-4d9a-a7b7-cde2974c0a1c,"random word, e.g., my dog is hairy !my"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,00d5cb0b-ef7a-4e6d-9076-bf9ae1d9fbee,dog is apple
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8d16adea-b398-438c-aff5-c02c9c21eb61,• 10% of the time: Keep the word un-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1d55e321-f962-4c84-8a46-b4f48351ad22,"changed, e.g., my dog is hairy !my dog"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c6d16491-eb3c-486e-b68f-634d75288c8b,is hairy . The purpose of this is to bias the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,61a587ff-205d-4082-8117-b7bc7df1f1c0,representation towards the actual observed
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0070ec95-5008-4c37-a788-e7dd72f26c85,word.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6a73a1fa-f9e1-4428-b5f1-cd662fffc3c6,The advantage of this procedure is that the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b114ccd1-f71c-4aec-9737-5110e15a61ad,Transformer encoder does not know which words
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,291fc288-e657-4564-8003-0f4ed1ce0996,it will be asked to predict or which have been re-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b77eef3-9f57-402a-922f-13a45f11d3bd,"placed by random words, so it is forced to keep"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,71fa1a20-992a-4a6e-b890-20533e0c637c,a distributional contextual representation of ev-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2e75bb68-8806-4475-b977-4ac7796fac11,"eryinput token. Additionally, because random"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,63033dab-099b-426a-bed8-9aee554a29b6,replacement only occurs for 1.5% of all tokens
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7a5fdd1c-ef72-49a4-bed6-6a150df71f38,"(i.e., 10% of 15%), this does not seem to harm"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,74ac06ba-2611-4fcb-b675-098d8b443ae5,the model’s language understanding capability. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba7fd99c-8432-4ff6-990d-ac763a708245,"Section C.2, we evaluate the impact this proce-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8abfc656-7776-4a42-9d48-b54f458fe164,dure.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,140d0f81-e345-495c-a01a-d18a8afd174d,"Compared to standard langauge model training,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,da215c15-ac04-4986-92c4-dee80760b651,the masked LM only make predictions on 15% of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6b46707c-9123-41ce-8401-8e3b8b94cadd,"tokens in each batch, which suggests that more"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,00581b51-1a5a-465d-87b0-e1092f29bbe1,pre-training steps may be required for the model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bcfbf1f4-84db-4922-94bd-2bc51f0a5491,BERT (Ours)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f70dbc5-3ef3-45db-be32-9ce1409ba1ca,Trm Trm Trm
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2e44c16c-b8e4-4589-b1a5-76be04a04222,Trm Trm Trm...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8d996a1b-2ec1-4cef-b967-80f76e0994b6,...Trm Trm Trm
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8935e749-654f-47c2-9f35-2cf7febb1aa3,Trm Trm Trm...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2132cca1-d0c3-4cac-bc9e-b30d9c18cf65,...OpenAI GPT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e48abc1-fb3d-451b-a9db-6af7b9eb89b8,Lstm ELMo
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b917af75-7541-4bfc-8b83-da909592d6aa,Lstm Lstm
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6d4791d6-f172-4f32-91b0-f7b379febf05,Lstm Lstm Lstm Lstm Lstm Lstm
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a526c443-b6e8-44d2-bdc0-314fcba68242,Lstm Lstm Lstm  T1 T2 TN...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,879d607a-79de-4045-81f9-6bc90a60e2fb,...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c3663c1-3f94-4dd7-b59b-9938ec00a0e5,......
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,69da235e-b05d-4450-8924-2a2b96c00fb0,...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3081b606-5ffd-4283-a5fe-05a432837715,E1 E2 EN... T1 T2TN...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,932a3b88-3472-47e1-973b-24ec4437543d,E1 E2 EN ... T1 T2 TN...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,93cb0b9e-1185-4fe2-87b5-f4b39e1d7cf1,E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b94d3d40-14c0-480b-b91e-91a22f632a5c,uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bec62709-74ae-4fad-bc56-506b6500496d,"left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,215e1d81-e8e8-4ed9-b7e0-327716f89f71,"conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b2d439bb-c397-4623-8383-19610fed710b,"OpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,93635f28-6348-4067-a16f-f26c83da0483,to converge. In Section C.1 we demonstrate that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dbf9dbb1-eb07-49c8-9665-05f46a646bd1,MLM does converge marginally slower than a left-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c9f6df8f-e6b8-4e0b-b03b-bb24d9eed67d,"to-right model (which predicts every token), but"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0c781aa8-8e82-44ec-bc20-c2a6f9062a88,the empirical improvements of the MLM model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,805eb1f6-fb61-4667-a795-eaee827ed541,far outweigh the increased training cost.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8b01108a-0ac1-40e8-b617-fa9710a7f225,Next Sentence Prediction The next sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0aac70d8-cb7a-4fed-83e8-62e829f490ff,prediction task can be illustrated in the following
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1fc175d9-cfbd-4696-a49d-5bbc4f7d3fdd,examples.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,03d99af3-b5ad-45a8-9792-e468fa56fac7,Input =[CLS] the man went to [MASK] store [SEP]
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,38d50d5f-91f0-4037-b0ae-d5cd47b4a213,he bought a gallon [MASK] milk [SEP]
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f5914149-f74b-4b58-88cf-06b541845249,Label =IsNext
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,277c2559-3a28-4194-aa71-3a093508cdd1,Input =[CLS] the man [MASK] to the store [SEP]
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,15fd7b01-d495-4f54-8224-f4478942b958,penguin [MASK] are flight ##less birds [SEP]
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5dc37eff-6b7a-4d38-872e-1085eb235c4c,Label =NotNext
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,583015bd-5ae0-4ec4-846e-ba744b0d588a,A.2 Pre-training Procedure
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,82e5edbc-d91a-4787-be65-1ccddac119fd,"To generate each training input sequence, we sam-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,70f80808-47e2-4c26-913c-15fd393e81e6,"ple two spans of text from the corpus, which we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8ac4a70b-6c94-436c-993a-6e4d6e5599da,refer to as “sentences” even though they are typ-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c50a3825-c4fa-4e28-9661-442c9c9ae508,ically much longer than single sentences (but can
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3bce88b9-d14e-474e-a069-6643a86a8649,be shorter also). The ﬁrst sentence receives the A
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6b124012-0645-4810-a571-2c840fc26aee,embedding and the second receives the Bembed-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0fdb1b42-d8df-4d63-8324-a7818a9670be,ding. 50% of the time Bis the actual next sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,23878fbe-9fd7-434f-8d9b-8bd5bc4ba709,that follows Aand 50% of the time it is a random
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c72457ae-d622-4539-9956-d2c52f201905,"sentence, which is done for the “next sentence pre-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f992237-06eb-4c6b-a4b1-086673c0acb2,diction” task. They are sampled such that the com-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cbb0192c-abbd-449c-adf3-ab62a56f5884,bined length is512 tokens. The LM masking is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0a9fba2b-15ec-49a4-9fc8-b854e416c025,applied after WordPiece tokenization with a uni-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e26408b0-d426-41e1-8493-ea2f888a55ac,"form masking rate of 15%, and no special consid-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d4c33cf7-1d7a-4fe6-aef4-9a08ccd02296,eration given to partial word pieces.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0170d7c6-8946-401f-9634-c93ccdef2a4b,We train with batch size of 256 sequences (256
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,453c164f-b461-4271-81d0-4021b85b9646,"sequences * 512 tokens = 128,000 tokens/batch)"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5ac76c55-a8f2-4f1c-adca-7ffa99797549,"for 1,000,000 steps, which is approximately 40epochs over the 3.3 billion word corpus. We"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6c4457cd-22aa-4f86-b7d1-49e286872d82,"use Adam with learning rate of 1e-4, 1= 0:9,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ce81799f-1eb3-46c7-a493-0e5b90f2ea6c,"2= 0:999, L2 weight decay of 0:01, learning"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c06de909-6a21-4315-a7d3-5aae208611a4,"rate warmup over the ﬁrst 10,000 steps, and linear"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,abc4e83c-4dfe-4ad9-b608-2167400ad3e9,decay of the learning rate. We use a dropout prob-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4fd4bf8-7398-4fd5-89ed-619d6ab20e5d,ability of 0.1 on all layers. We use a gelu acti-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a00bb0b6-1c98-4f5b-b866-7a0524b32787,"vation (Hendrycks and Gimpel, 2016) rather than"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3350c62b-ae00-479d-add4-53d214b9564f,"the standard relu , following OpenAI GPT. The"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f9d2a375-9d93-412e-a0fc-e03829a41a30,training loss is the sum of the mean masked LM
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0b4b9773-02cd-4e8f-af11-ede4a313b134,likelihood and the mean next sentence prediction
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40e85151-8dbc-4ca1-976d-14701e46f236,likelihood.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ac992dcb-7751-4dfe-b63d-99dbf55fcae1,Training of BERT BASE was performed on 4
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3b7bc9db-ff75-4eb4-b7af-0692dbad843d,Cloud TPUs in Pod conﬁguration (16 TPU chips
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,65a1b6ce-20d6-4047-be8f-a3a25761fce4,total).13Training of BERT LARGE was performed
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f43a6c3f-ad04-4ba0-a49c-1e3b5f14d8c0,on 16 Cloud TPUs (64 TPU chips total). Each pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b189c618-d03e-4443-9070-dda3d7943a1b,training took 4 days to complete.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bf2c4eb6-fbd1-4455-99eb-6a14519da691,Longer sequences are disproportionately expen-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9c3b311c-69d7-4e18-bfca-1137e43422c4,sive because attention is quadratic to the sequence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bcb7d13a-8e43-4e8d-a057-2746a3833458,"length. To speed up pretraing in our experiments,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e2cb6c72-ae07-4361-963e-cae542658c18,we pre-train the model with sequence length of
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a0ca9206-54b0-4de8-ae43-cf568e0ae595,"128 for 90% of the steps. Then, we train the rest"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b112e723-38e9-4c41-aaa0-2dd8d1edd760,10% of the steps of sequence of 512 to learn the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,41547d99-a6a9-4393-96bb-cb071b6c1b0c,positional embeddings.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,da4b797a-4a3a-4812-8553-9dc54e278d99,A.3 Fine-tuning Procedure
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,54d4b88b-38e3-41cf-97e4-7219cdc87f3d,"For ﬁne-tuning, most model hyperparameters are"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8827a8fa-b86a-4f7a-8e47-11e9ab2a3a90,"the same as in pre-training, with the exception of"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,52abb456-3619-4a87-a0d9-6c47723d31e5,"the batch size, learning rate, and number of train-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fd0d66c5-80f4-49b4-af07-a712b55ede2e,ing epochs. The dropout probability was always
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ab09ff34-297f-47b3-b757-f171f730b00a,kept at 0.1. The optimal hyperparameter values
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0c3ba645-ea3e-44bb-9908-afa7a4b1a171,"are task-speciﬁc, but we found the following range"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ca702277-201c-4a88-9cf4-c6c807c2ae9f,of possible values to work well across all tasks:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,62adb3e3-17dd-4f4e-9591-fb98cc5c1afd,"•Batch size : 16, 32"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,60128962-a4db-4ee7-8ccb-2985eaef0baa,13https://cloudplatform.googleblog.com/2018/06/Cloud-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,83c3d658-3740-406e-8069-afefbfe14488,TPU-now-offers-preemptible-pricing-and-global-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bb1884a7-3501-4965-82b3-d650accc1e03,availability.html
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,321e8ee4-d159-48d0-832d-83236a8cd099,"•Learning rate (Adam) : 5e-5, 3e-5, 2e-5"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,65362465-11e1-43dd-af34-5179dc20b558,"•Number of epochs : 2, 3, 4"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2f19fcb5-0be3-4b79-9fd7-36cb53256a03,"We also observed that large data sets (e.g.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4d6eb10-5dd3-438f-a151-69bdb4bcffde,100k+ labeled training examples) were far less
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c742c0d0-3d54-4868-a884-21ffc7583613,sensitive to hyperparameter choice than small data
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5000a285-7a87-4547-b5d8-36557a5d9a7c,"sets. Fine-tuning is typically very fast, so it is rea-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08969ede-4564-4e09-a0ff-efa0cb6f58fb,sonable to simply run an exhaustive search over
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5d356877-b72d-4294-99fc-cf2b1a7791ce,the above parameters and choose the model that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,138bdf93-bb18-493e-a971-35ab6acb6248,performs best on the development set.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,113faa41-500e-44a1-99e7-cfa38dfa8a3c,"A.4 Comparison of BERT, ELMo ,and"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8d7e0d47-70b1-415f-859c-db2279182c9c,OpenAI GPT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,58b19ff2-3766-4ebe-a76e-628fc45e5d04,Here we studies the differences in recent popular
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,78667457-bdd8-453f-b6d9-e0f0a17c631e,"representation learning models including ELMo,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ad87e2c5-de6f-40ac-a570-df11598abb6f,OpenAI GPT and BERT. The comparisons be-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,32a65b08-50b2-4399-a2cc-6c2ba2fefed8,tween the model architectures are shown visually
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,29250701-94ef-49ff-83e8-42c2f5986c9e,in Figure 3. Note that in addition to the architec-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8d76f01-bd1c-401c-8f7b-a5b408be48db,"ture differences, BERT and OpenAI GPT are ﬁne-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,83067164-8816-4b6b-bf0c-8f2112db9785,"tuning approaches, while ELMo is a feature-based"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae2bf726-1799-4471-8d8c-17d09f6853fb,approach.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a13449f5-8528-4fda-8fe4-9238c78a8ba8,The most comparable existing pre-training
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,88865589-2fad-470a-8601-d1a62445dab3,"method to BERT is OpenAI GPT, which trains a"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2d3f7dcf-addb-4c19-93b9-195c317bc9b1,left-to-right Transformer LM on a large text cor-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,18cd3a02-8f35-4159-b0e5-f0136d058f75,"pus. In fact, many of the design decisions in BERT"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3457651a-9af0-44e8-8163-efd9b047196a,were intentionally made to make it as close to
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,db3c5d73-ef7c-43f4-abf0-671950449003,GPT as possible so that the two methods could be
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,131500b2-4ed3-4664-b8a6-2526e7122740,minimally compared. The core argument of this
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c43769f9-a85d-48fd-96ac-f807ff624b11,work is that the bi-directionality and the two pre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99343a9e-16d8-495b-ad35-d02a52a6c0f1,training tasks presented in Section 3.1 account for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,105044b3-d04f-4179-9829-42bc72b5e67b,"the majority of the empirical improvements, but"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e0a8f7de-8c6c-454e-9f0e-243362eee6c1,we do note that there are several other differences
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ea87bf9d-2eac-48ea-a768-2e43323cf7d8,between how BERT and GPT were trained:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b9c8ce5-dfa2-4cf5-82a0-906c2e72529b,• GPT is trained on the BooksCorpus (800M
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,be630fd9-6520-4b31-9ac9-99dc1b71190f,words); BERT is trained on the BooksCor-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f6085369-f360-4b9c-b087-270ac4aa9989,"pus (800M words) and Wikipedia (2,500M"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7f2274cf-0806-436e-b0b3-02c2e918af13,words).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7e2c30f9-dd49-4c26-8f23-1c687f01de67,• GPT uses a sentence separator ( [SEP] ) and
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ba36765c-85b3-4e23-999d-7883ed7c28d6,classiﬁer token ( [CLS] ) which are only in-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,86db6cdc-368c-4cc0-a19c-5da551025b8b,troduced at ﬁne-tuning time; BERT learns
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9ad33aee-f5f3-413e-8d08-99ff373f64d4,"[SEP] ,[CLS] and sentence A/Bembed-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cdad1a49-585f-4115-bdb0-28eecf1ff051,dings during pre-training.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3691741e-da01-4dfc-86bd-95db69fa14c6,• GPT was trained for 1M steps with a batch
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,814c34c5-d5d9-441f-9092-a21c2a784fe2,"size of 32,000 words; BERT was trained for"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42349603-c01d-4538-8914-faaa735dfa16,"1M steps with a batch size of 128,000 words."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,38afff8d-640b-417b-b470-b7a6dc4c6c40,• GPT used the same learning rate of 5e-5 for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fbaadd80-cbeb-4c11-884d-7574ead70174,all ﬁne-tuning experiments; BERT chooses a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e713ec05-94de-4586-a5ea-55855c45a639,task-speciﬁc ﬁne-tuning learning rate which
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a126118c-b914-4a7c-b6cb-65474f93bbd1,"performs the best on the development set.To isolate the effect of these differences, we per-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e997d6a4-a020-46bb-a4a6-5c30ea7ed05e,form ablation experiments in Section 5.1 which
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c368212-4457-4627-90aa-211cd52bcb04,demonstrate that the majority of the improvements
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb6a87f0-6bb4-4db7-9b9e-6c983fbf33b0,are in fact coming from the two pre-training tasks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f20b9782-f3dd-4bfa-a026-b0474a9194e3,and the bidirectionality they enable.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f9a4aa5-6052-40bc-ae22-31f70761f5bc,A.5 Illustrations of Fine-tuning on Different
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,54e932bf-16a3-4f42-80f4-9b42403090a6,Tasks
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,db79ffa1-ac4c-4647-a7f0-ec190b75fca9,The illustration of ﬁne-tuning BERT on different
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f311091b-2e91-4499-9821-14f5f24a1275,tasks can be seen in Figure 4. Our task-speciﬁc
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,595e1fbf-594b-46b3-895a-6713116238d8,models are formed by incorporating BERT with
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f6735c0-eb6f-46e4-82dd-2fb4bc6b9f16,"one additional output layer, so a minimal num-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,63bffd09-e7bb-49e7-ba94-698051f0fa60,ber of parameters need to be learned from scratch.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,85b69bd0-5fc7-4d36-915c-97ead0913241,"Among the tasks, (a) and (b) are sequence-level"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,af25900e-a47a-4e6e-a2c3-aa701cd51ed4,tasks while (c) and (d) are token-level tasks. In
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8e2bb46-9dca-4a29-a8c3-fe9ffd8ed3b4,"the ﬁgure,Erepresents the input embedding, Ti"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ec4c5f9-72a8-4e27-9a4f-da6af96e9b56,"represents the contextual representation of token i,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,42de3666-2a4f-44a8-9606-fa8109d61baa,[CLS] is the special symbol for classiﬁcation out-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c9f7ae54-d70c-4c20-9257-df2b5be4592f,"put, and [SEP] is the special symbol to separate"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,67fbbd7e-7602-45a9-b09a-9e400368e7ba,non-consecutive token sequences.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,40027ccb-7499-4227-8739-e5f504c30462,B Detailed Experimental Setup
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4f03137-4f27-4587-bdcf-2c9d4b26c303,B.1 Detailed Descriptions for the GLUE
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,757cb601-be6f-4a53-b467-7412df70b116,Benchmark Experiments.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c33d9c12-dcf4-48b3-bb88-ba46c600e663,Our GLUE results in Table1 are obtained
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e8378510-62ce-4603-8b91-3307dc09b0de,from https://gluebenchmark.com/
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a6ea4c36-cb2c-4735-9110-d584685bd76f,leaderboard and https://blog.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,67f4e313-4a02-4adf-a43f-144b743766e0,openai.com/language-unsupervised .
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,daac1c28-b8e8-4c55-abd2-2adadaf3d9cd,The GLUE benchmark includes the following
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f109a59-9e3f-42ae-95f4-3450f618a9c3,"datasets, the descriptions of which were originally"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,91ec5bd6-dd6c-4f22-831e-116e3e65adbb,summarized in Wang et al. (2018a):
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6d626fa4-1a1c-4de1-bcf4-0d9184b5602e,MNLI Multi-Genre Natural Language Inference
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1570b971-dad5-450c-8c14-a37cebef374e,"is a large-scale, crowdsourced entailment classiﬁ-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,94cca129-d2cb-42df-a344-01549d642ed9,"cation task (Williams et al., 2018). Given a pair of"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,66d5ec47-dced-4acf-bf93-4fe29d0b1d6b,"sentences, the goal is to predict whether the sec-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e59b1d9c-67e9-434c-9b67-d99104a3d523,"ond sentence is an entailment ,contradiction , or"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,afacc94a-9d15-497d-9a72-e9eedc8ff2a5,neutral with respect to the ﬁrst one.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1f328ca7-8032-4f30-a758-b0f38e53d57e,QQP Quora Question Pairs is a binary classiﬁ-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,37bfe957-0de5-4021-95ed-f097d8dd4797,cation task where the goal is to determine if two
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b42df712-f1d3-4237-b34d-fc437be4269e,questions asked on Quora are semantically equiv-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,99229c6b-6d92-4b63-a3ea-08dc7fa41a4a,"alent (Chen et al., 2018)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d3640af8-9ec3-4124-8d6b-9d0a771b438f,QNLI Question Natural Language Inference is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,751342bd-727a-4f24-bb4d-068a20d937e7,a version of the Stanford Question Answering
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,59ab7f36-4a84-46b7-8408-5468e4deee79,"Dataset (Rajpurkar et al., 2016) which has been"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1ced9c93-5306-4445-b87f-d418acfa3017,converted to a binary classiﬁcation task (Wang
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,919138a1-05a5-4fae-9fb1-79e43ec99ab8,"et al., 2018a). The positive examples are (ques-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d7f29dd8-d040-48a2-a6ab-366e59215e3a,"tion, sentence) pairs which do contain the correct"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2440344c-eca7-45eb-ad4a-bcb5625e1cde,"answer, and the negative examples are (question,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6d195565-ac45-4ac7-8def-58978a0427c5,sentence) from the same paragraph which do not
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,37cd717c-5a7f-4893-8971-79a3eec4bca7,contain the answer.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,33b53951-f7dd-4ccd-bb8a-64c880a2ae02,BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a701cf7f-8376-415e-b912-4145b829f6df,E[CLS] E1 E[SEP] ... ENE1’... EM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5cb0c01c-6c26-4c4f-ad8f-5b21796da1f7,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,401c0d83-82f9-48e6-b844-e3daa9b289e7,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,66443c1c-d812-45ca-85ab-5eb11ab212f0,T[SEP] ...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7261e1af-7dac-4abb-9fd4-a0d569a972ce,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,de44ba46-5a5c-420d-9026-33e25f028002,T1’...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,499dbfa2-9ca6-4086-90cb-989b14458768,TM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e459f32b-e4d1-4a13-8431-53e61fef9be9,[CLS] Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,98857e1e-f7a5-4799-8d1d-0dcf44f365ea,1 [SEP] ...Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a97d01e3-0e7f-4598-9cb5-e6259f3c1ecd,NTok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa2bcda0-f4a8-4d60-857e-d947198f86da,1...Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b3ef90d3-b53f-4b9a-a76a-7ab3f591ae50,M
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ef3018a8-1020-4027-bdd5-74ed88b7fad4,Question Paragraph BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e66542f-fc70-405c-b658-259a56d1a22b,E[CLS] E1 E2 EN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3246abe1-7f6a-46db-8d11-02888d63a5b1,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c69b0759-8194-4583-866a-1a7382f968fc,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f804567-d4c0-4b1e-a3ae-7002dbca154f,T2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,43f279d3-f81e-41ed-9815-1cdbd691b605,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2748b7e2-2e56-438b-a6a8-70c9e5200ef5,Single Sentence ...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ae495d54-c277-432b-acf1-67eaf7faf4b7,...BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5992adef-7ca1-4904-88e1-2a69628f1462,Tok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,de46c904-7313-43dc-a68d-657b27cf1359,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4a53c8fe-469e-4b90-8c41-0640b74857ca,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,97216eaa-3264-44c7-921c-952df97e59c4,T2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,54092cc7-0eb6-421b-8485-1cd464d41f10,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4ad619a7-df38-4e9e-9bd1-6a4ca8dfdc70,Single Sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,20509f1a-dbfa-45e7-bd60-890b009ff1b4,B-PER O O...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e497c47b-c689-4757-8c15-efd7445d7ec7,... E[CLS] E1 E[SEP] Class
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,310b0cb9-a519-41f0-8255-38a386739511,Label
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d8424e76-6e14-4162-adfe-9aefca1e30dc,... ENE1’... EM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0faf162e-5107-437f-832c-16c1e121ff2c,C
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c39c86ed-0c43-4227-b884-7b82520f5763,T1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0f3cc4aa-2931-4ed1-9faa-37d7d1d9ac54,T[SEP] ...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2b71804e-ad53-458d-b5c9-57fada444c57,TN
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8af937fd-0e41-45d9-b5ff-a079979584f0,T1’...
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a963aea6-7bde-4fe4-833b-6c208a3efe1b,TM’
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e83c1112-4f63-4a3c-9ac9-109c030f4edb,Start/End Span Class
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,45c01af8-433e-4f0c-ad51-2ba0db585b8e,Label
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,984640a8-55a7-4483-ab08-60334adcd748,BERT
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c0007900-d3b5-4539-b42d-4a6adfbda553,Tok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8898c595-8d2d-42be-8f24-e8652e06d2d2,1 [SEP] ...Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1312c489-8bd0-4953-a6ba-11b7462cbd94,NTok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8c282662-dee5-4aed-9869-9c20c43a62fa,1...Tok
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,78f5d998-4f27-4739-934d-e42e0d7ccf3c,M
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,733fa715-7238-4343-8689-7d46f441f271,Sentence 1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,638acf84-1ba4-4abc-9718-beeb74e6a1c5,...Sentence 2
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5864f929-3d15-4489-9114-b74ae08b6d22,Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d1249f96-36c1-446e-8752-6dd246864fe9,SST-2 The Stanford Sentiment Treebank is a
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,48909fbc-4e45-4810-982d-f676169ec381,binary single-sentence classiﬁcation task consist-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e5dd5a5e-c7d6-4019-b1e8-2eea5a345a1f,ing of sentences extracted from movie reviews
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c1357acc-3c0c-4e1f-90f1-2397415f245d,with human annotations of their sentiment (Socher
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a183a2a4-43df-4610-93a8-cc92164dc1f0,"et al., 2013)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ff43e006-3e56-4b3b-ac3a-b1c1761c1ebb,CoLA The Corpus of Linguistic Acceptability is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4bc9bb7f-ac4b-4bee-b168-384b00b0a730,"a binary single-sentence classiﬁcation task, where"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,92954137-7ca0-4e18-b117-ff2e669ff6ba,the goal is to predict whether an English sentence
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9f02419a-ba58-4daa-ab74-2507fabd596f,is linguistically “acceptable” or not (Warstadt
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bc73fb73-f763-4a36-a720-28edfeab606c,"et al., 2018)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,aa4b7b84-cab7-4534-bda1-272da87b564f,STS-B The Semantic Textual Similarity Bench-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4831a1d1-3727-4a4b-9630-2fd0b8c1c40f,mark is a collection of sentence pairs drawn from
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dc27506e-a620-4f7b-8a18-b918c5c44913,"news headlines and other sources (Cer et al.,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,dbb61ad1-387a-4dd4-a5fd-fabeedaf4c07,2017). They were annotated with a score from 1
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5a630652-15fb-4025-b468-1c985220e459,to 5 denoting how similar the two sentences are in
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,04d8e8c0-10d6-47e4-b295-8a7edae9f487,terms of semantic meaning.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,502e8512-b10b-47c0-b4ce-3250b69b8259,MRPC Microsoft Research Paraphrase Corpus
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e9b89f42-d7d3-4078-8e07-779c3c142eee,consists of sentence pairs automatically extracted
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,667e5d29-5683-40f4-85fa-58b469c7793d,"from online news sources, with human annotationsfor whether the sentences in the pair are semanti-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6a34e679-6ee3-46d7-892c-35f4bc8a76a7,"cally equivalent (Dolan and Brockett, 2005)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7c7e5721-ac51-4b88-97d7-ad42d5faeb7b,RTE Recognizing Textual Entailment is a bi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,237eedc5-0799-48eb-894b-8d7c049a3135,"nary entailment task similar to MNLI, but with"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb4722ad-bb4c-4e97-8aa2-6033e94eb258,"much less training data (Bentivogli et al., 2009).14"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9bc2a725-2f82-4cb1-abbf-3c8b8d0cea3b,WNLI Winograd NLI is a small natural lan-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ef23514c-cc11-4e0d-a1c6-543d25273024,"guage inference dataset (Levesque et al., 2011)."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7902d75d-3a41-4196-bb0c-43109c8a0c87,The GLUE webpage notes that there are issues
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08dbd279-b0fe-41ac-8596-5fe4ba702303,"with the construction of this dataset,15and every"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6a1bc597-0ff8-4759-967e-996638febc5f,trained system that’s been submitted to GLUE has
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cf547f94-f1e2-4ceb-8cba-17e61379e446,performed worse than the 65.1 baseline accuracy
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,018b11db-5f46-4286-a87c-93c10ccf9168,of predicting the majority class. We therefore ex-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,906adbec-505b-45d6-9d41-fa0018aba81e,clude this set to be fair to OpenAI GPT. For our
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8a1833a7-05ee-4e1a-9826-0afdf00f3c12,"GLUE submission, we always predicted the ma-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0f058fe6-2d79-45ef-bb34-16b9b809c9ac,14Note that we only report single-task ﬁne-tuning results
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b8f5b19d-3e4a-43fd-a3a9-a38bc5605b3f,in this paper. A multitask ﬁne-tuning approach could poten-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,029abf8e-4a85-41f8-9fe7-a1d0bdcbb954,"tially push the performance even further. For example, we"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5348d29b-1d61-45b3-89b0-9421e837f4c6,did observe substantial improvements on RTE from multi-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,016dce08-0658-4ba2-b46f-f59caf545337,task training with MNLI.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c4db7fa7-60b1-4715-8915-89345c0e58d5,15https://gluebenchmark.com/faq
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,157747ee-933f-4c1a-8a96-3cbfd367861b,jority class.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3324a3c5-dc8d-450d-bbb4-47c869f25bb1,C Additional Ablation Studies
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,4e8bbb02-611b-4f5c-b340-20c30bfac475,C.1 Effect of Number of Training Steps
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,08b12abf-e438-4cd2-aeb2-fe129cf35e22,Figure 5 presents MNLI Dev accuracy after ﬁne-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d1a46078-d690-4cb5-a477-356d6eae586a,tuning from a checkpoint that has been pre-trained
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,acf48283-ce82-4f3a-ac0c-4a0732ef3125,forksteps. This allows us to answer the following
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a78f2456-bdfc-4539-9bae-13f4cbe403f2,questions:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fdacb13a-91a0-4f84-84ab-a12b9e273733,1. Question: Does BERT really need such
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,694287de-e79d-497a-9de0-d476efca641f,"a large amount of pre-training (128,000"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c6aa16e-ac8c-4458-827c-b87077b130bb,"words/batch * 1,000,000 steps) to achieve"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7579876f-bb06-440a-9f9b-39790fb52789,high ﬁne-tuning accuracy?
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,6df42fdc-0b49-41f7-9c66-808d9c2d0547,"Answer: Yes, BERT BASE achieves almost"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1eb6aa6b-51a0-4c7d-ad8f-45b01ce38689,1.0% additional accuracy on MNLI when
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,344abb22-e2b2-4a00-8f43-3fdb79c0894b,trained on 1M steps compared to 500k steps.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2483bdb5-6f7b-4a03-a87b-6ca06b4784d8,2. Question: Does MLM pre-training converge
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,54086b22-56bc-426b-a036-ac829bd79838,"slower than LTR pre-training, since only 15%"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,666ce647-8daa-4378-8fbd-970b1f3e0dfd,of words are predicted in each batch rather
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,949a0f53-dd0f-4659-a4df-6dad7a9b34cd,than every word?
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7569d7e8-016b-4dd6-88ae-253ba26bf4b7,Answer: The MLM model does converge
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,059ece8e-3cd5-4289-86d6-9a6bc9944940,slightly slower than the LTR model. How-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,49870a27-ccd0-41f3-88cf-51fe17cf60c5,"ever, in terms of absolute accuracy the MLM"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,36728c86-e951-43ca-b21b-c75db06dfd72,model begins to outperform the LTR model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,31ea3e1a-87f4-442b-9b85-612ffedf0fca,almost immediately.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fa13a398-8eba-42a8-bbce-8c94357df257,C.2 Ablation for Different Masking
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1c731c19-7bb0-4b41-8c79-a3eca3685201,Procedures
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,a65e379e-633b-4963-a5cf-7e58d1130098,"In Section 3.1, we mention that BERT uses a"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,ee4dfe93-cbe4-491d-9d9a-76c343da565d,mixed strategy for masking the target tokens when
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9c1353b1-011d-415f-89df-c2e53d252715,pre-training with the masked language model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,88102fa6-32cb-44ba-8ee6-8ea53e81c733,(MLM) objective. The following is an ablation
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,370311d8-9086-4e92-b231-be8c44627cb3,study to evaluate the effect of different masking
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,60b767de-73eb-4690-90e5-9f8d9ec6cc78,strategies.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d842bebc-7a6e-4b47-9d54-de42679000a1,200 400 600 800 1;0007678808284
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,2a9a12f2-3730-4dd3-b486-d3f6acdf5cbb,Pre-training Steps (Thousands)MNLI Dev Accuracy
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,967a7127-5a8f-4f6c-b88e-fd66229e24aa,BERT BASE (Masked LM)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b49164e-7857-4231-a4db-e26823763d8c,BERT BASE (Left-to-Right)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,96feead4-bed1-4450-9175-349a8c4925f3,Figure 5: Ablation over number of training steps. This
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,d187d5e7-a89d-49a4-86b1-ba365e597888,"shows the MNLI accuracy after ﬁne-tuning, starting"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0102e9b0-14e8-4230-84d7-6a4c164d41d2,from model parameters that have been pre-trained for
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,707129d5-352f-4bc9-bc32-ac13759d2e28,ksteps. The x-axis is the value of k.Note that the purpose of the masking strategies
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,95514158-5aca-4bab-87c1-2a5f06f34480,is to reduce the mismatch between pre-training
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c28efb27-1e32-46c1-bccc-82fb290d839f,"and ﬁne-tuning, as the [MASK] symbol never ap-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b4185395-867b-4bdf-987e-6391f1803612,pears during the ﬁne-tuning stage. We report the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1ec22f02-412b-43a7-b36c-22e876cb5c5a,"Dev results for both MNLI and NER. For NER,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,755fed5c-430b-453f-9ada-d90a7a2e0235,we report both ﬁne-tuning and feature-based ap-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bd9bb276-c559-4813-bb25-4df496133df3,"proaches, as we expect the mismatch will be am-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cda8c9c9-6e1c-41f3-ae47-f1457e8d8a45,pliﬁed for the feature-based approach as the model
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9a75e8a4-deaf-4247-949d-0e256e4febd5,will not have the chance to adjust the representa-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,3e43fdfd-79d4-467e-b687-88edee07b2f0,tions.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,8bc6706c-53c7-492a-8100-5a38dcaf4552,Masking Rates Dev Set Results
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,c1342b84-868f-4a08-9e7e-54ff177dd5c5,MASK SAME RND MNLI NER
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,482e55ab-9ff0-4e6c-b86d-d7ec78a31b0b,Fine-tune Fine-tune Feature-based
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5f1da5c2-a416-4f4d-9e94-b7035ee6f052,80% 10% 10% 84.2 95.4 94.9
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fc04f9f8-fb3c-4d47-9f9f-a5a3cd83b03a,100% 0% 0% 84.3 94.9 94.0
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,04df1082-68a6-450d-bc4d-10f72583e2ca,80% 0% 20% 84.1 95.2 94.6
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,930d635d-446d-4b26-abfa-ef3a2e50d10f,80% 20% 0% 84.4 95.2 94.7
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e709cfb-2926-44e1-9a62-2aa75794a804,0% 20% 80% 83.7 94.8 94.6
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,0e04b827-0c79-4b21-b466-d0c4eb230ea7,0% 0% 100% 83.6 94.9 94.6
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e1f2b418-527b-4b4e-af33-a487a4742051,Table 8: Ablation over different masking strategies.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,cb6e97df-f67d-4555-99ca-8d42002fa153,"The results are presented in Table 8. In the table,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,60c84aa1-f8c1-4313-b645-df3aa157cf4f,MASK means that we replace the target token with
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e828099b-3cf1-4efd-afd4-13813b3c5f09,the[MASK] symbol for MLM; S AME means that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,71287885-fd25-46e7-adb1-802ccaafe696,we keep the target token as is; R NDmeans that
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1712c1be-d1b6-4ca1-a08c-9555c44f0789,we replace the target token with another random
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,07c312bf-5f56-4219-a077-24f2170c61e8,token.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,fb36d86e-d3db-4ff1-b7f7-101001aa9857,The numbers in the left part of the table repre-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9a03533c-0c73-4a81-8c5d-78c0f9af2512,sent the probabilities of the speciﬁc strategies used
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,bf1957fb-6821-4d6a-b036-17c20b72526b,"during MLM pre-training (BERT uses 80%, 10%,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,75dbf378-9fbd-4467-80a9-a15f818d12c2,10%). The right part of the paper represents the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,b6d047da-90a9-42bc-9e96-89b218eea067,"Dev set results. For the feature-based approach,"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f4a3ad2e-d38a-4cf9-a3d1-13413e97e1cb,we concatenate the last 4 layers of BERT as the
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,9b2c4395-9f9f-4523-a4ce-8993ca53df51,"features, which was shown to be the best approach"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,7efae9b7-12c1-4427-b1a3-6a42f61f1865,in Section 5.3.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,77051276-060d-4ab7-a465-ff6a840683c5,From the table it can be seen that ﬁne-tuning is
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,1b7f1565-d46c-4f91-b2e4-f29f56ea00e8,surprisingly robust to different masking strategies.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,5c846933-1f62-41fe-a2c2-56ee6103f079,"However, as expected, using only the M ASK strat-"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,f31d640b-de71-4b44-8f20-a849d18976eb,egy was problematic when applying the feature-
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,e5dd0ae5-88ff-4b64-8971-52615db823ff,"based approach to NER. Interestingly, using only"
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,80f6b977-f333-4440-8269-d1071686327c,the R NDstrategy performs much worse than our
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018-10-11,38b7bb3b-458f-4de3-b859-ba88dcb16ad2,strategy as well.
Language Models are Unsupervised Multitask Learners,2019-02-14,588e306a-c51d-40cc-b08f-478031773b49,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,01188a23-6652-418a-9115-c8d8eba09c7e,Alec Radford*1Jeffrey Wu*1Rewon Child1David Luan1Dario Amodei**1Ilya Sutskever**1
Language Models are Unsupervised Multitask Learners,2019-02-14,397e254e-b04f-40c7-b23a-624dc8499631,Abstract
Language Models are Unsupervised Multitask Learners,2019-02-14,380ca3a4-9f0e-4d80-880b-36afc65ab523,"Natural language processing tasks, such as ques-"
Language Models are Unsupervised Multitask Learners,2019-02-14,7054727a-b443-49b2-b826-523c1fb16b21,"tion answering, machine translation, reading com-"
Language Models are Unsupervised Multitask Learners,2019-02-14,e88c2057-71e1-48ea-84b8-62c6028e1458,"prehension, and summarization, are typically"
Language Models are Unsupervised Multitask Learners,2019-02-14,ace0f4f9-0c48-4650-a269-16efbbc21db0,approached with supervised learning on task-
Language Models are Unsupervised Multitask Learners,2019-02-14,1b7cd7a5-fc3f-4727-acc3-a53bd265f0fb,speciﬁc datasets. We demonstrate that language
Language Models are Unsupervised Multitask Learners,2019-02-14,61c8f6ac-eaa7-495a-86cb-5d443a9fa15d,models begin to learn these tasks without any ex-
Language Models are Unsupervised Multitask Learners,2019-02-14,d49874ea-e093-4ea3-b3f3-daf2e19a8b80,plicit supervision when trained on a new dataset
Language Models are Unsupervised Multitask Learners,2019-02-14,d593e246-343d-4811-b1ee-36b1f941d2d5,of millions of webpages called WebText. When
Language Models are Unsupervised Multitask Learners,2019-02-14,569d201b-8df2-45c4-8f0e-c7c2eedd5916,"conditioned on a document plus questions, the an-"
Language Models are Unsupervised Multitask Learners,2019-02-14,79562317-ff4c-4c4e-a87b-20bf2fd4132e,swers generated by the language model reach 55
Language Models are Unsupervised Multitask Learners,2019-02-14,25dcdc17-8c7f-4ef8-9392-faae2315dd98,F1 on the CoQA dataset - matching or exceeding
Language Models are Unsupervised Multitask Learners,2019-02-14,7cd3e6ef-7731-4c8e-a5c8-90067b3e5dda,the performance of 3 out of 4 baseline systems
Language Models are Unsupervised Multitask Learners,2019-02-14,37207806-77f1-4745-9439-1d6bc58365ba,"without using the 127,000+ training examples."
Language Models are Unsupervised Multitask Learners,2019-02-14,a8d2b3ba-6c8a-4de1-b07c-d87d24324a41,The capacity of the language model is essential
Language Models are Unsupervised Multitask Learners,2019-02-14,e983de23-cd76-4f66-bc9d-5d0a7f893601,to the success of zero-shot task transfer and in-
Language Models are Unsupervised Multitask Learners,2019-02-14,b665e498-0a27-4202-9134-4d426ca761d3,creasing it improves performance in a log-linear
Language Models are Unsupervised Multitask Learners,2019-02-14,d0bdc915-6f46-428a-a31d-982b9500e52d,"fashion across tasks. Our largest model, GPT-2,"
Language Models are Unsupervised Multitask Learners,2019-02-14,8d0fcd18-a622-400f-bd5b-ff446bb2d784,is a 1.5B parameter Transformer that achieves
Language Models are Unsupervised Multitask Learners,2019-02-14,1b38e708-5c92-4d1c-96c9-464cc9a453aa,state of the art results on 7 out of 8 tested lan-
Language Models are Unsupervised Multitask Learners,2019-02-14,9399401f-e61f-4836-8dc0-84f60fca374d,guage modeling datasets in a zero-shot setting
Language Models are Unsupervised Multitask Learners,2019-02-14,51039ccf-d9aa-4592-92c9-ded0f5090d45,but still underﬁts WebText. Samples from the
Language Models are Unsupervised Multitask Learners,2019-02-14,0febbe11-2849-440c-ae09-8530a3caef45,model reﬂect these improvements and contain co-
Language Models are Unsupervised Multitask Learners,2019-02-14,0113c26a-4f7e-4c97-ab68-1fd6c2a6abd5,herent paragraphs of text. These ﬁndings suggest
Language Models are Unsupervised Multitask Learners,2019-02-14,b200b288-eb59-42b6-bcfb-86d2f87ad91a,a promising path towards building language pro-
Language Models are Unsupervised Multitask Learners,2019-02-14,d22e1f0e-fec6-4517-a814-3398f35c7011,cessing systems which learn to perform tasks from
Language Models are Unsupervised Multitask Learners,2019-02-14,bcf603b9-6e54-4d67-8317-e1f23804a4cb,their naturally occurring demonstrations.
Language Models are Unsupervised Multitask Learners,2019-02-14,2c7d06b8-08be-4632-b3a8-43483e4ead08,1. Introduction
Language Models are Unsupervised Multitask Learners,2019-02-14,c3a83fe5-b9a0-43ee-ad42-fae37fa2d50a,Machine learning systems now excel (in expectation) at
Language Models are Unsupervised Multitask Learners,2019-02-14,14d2de81-2be4-4b63-934b-8f93a139875a,tasks they are trained for by using a combination of large
Language Models are Unsupervised Multitask Learners,2019-02-14,f46de4fb-beae-473c-a6d4-876837f94488,"datasets, high-capacity models, and supervised learning"
Language Models are Unsupervised Multitask Learners,2019-02-14,2d60455e-ca5b-4e38-ada2-889ee2076bea,"(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei"
Language Models are Unsupervised Multitask Learners,2019-02-14,d5d59f69-8520-4084-a4d1-d8e1f5537417,"et al., 2016). Yet these systems are brittle and sensitive to"
Language Models are Unsupervised Multitask Learners,2019-02-14,334aca14-a28b-489e-ae30-8cbaa3b7e126,"slight changes in the data distribution (Recht et al., 2018)"
Language Models are Unsupervised Multitask Learners,2019-02-14,8214196d-f1e4-40b5-908e-d2c8b9063b68,"and task speciﬁcation (Kirkpatrick et al., 2017). Current sys-"
Language Models are Unsupervised Multitask Learners,2019-02-14,31afb204-774d-4b01-81d9-992fc53c52b1,tems are better characterized as narrow experts rather than
Language Models are Unsupervised Multitask Learners,2019-02-14,abb33d4e-62ee-493d-a103-dea2d37fbff4,"*, **Equal contribution1OpenAI, San Francisco, Califor-"
Language Models are Unsupervised Multitask Learners,2019-02-14,739117e8-0a80-4e0a-b09b-b94f66bf99ed,"nia, United States. Correspondence to: Alec Radford"
Language Models are Unsupervised Multitask Learners,2019-02-14,32780b05-8eca-469a-8d78-4533862161df,<alec@openai.com >.competent generalists. We would like to move towards more
Language Models are Unsupervised Multitask Learners,2019-02-14,a4aa78f1-0f90-464e-88ac-c3dac68c6a29,general systems which can perform many tasks – eventually
Language Models are Unsupervised Multitask Learners,2019-02-14,b1a48dda-401b-4be2-8248-579af0933faf,without the need to manually create and label a training
Language Models are Unsupervised Multitask Learners,2019-02-14,177db29d-aaa2-4b82-bba6-f65f55707dcd,dataset for each one.
Language Models are Unsupervised Multitask Learners,2019-02-14,48a16d14-2b50-426c-bc17-e06cd2529c70,The dominant approach to creating ML systems is to col-
Language Models are Unsupervised Multitask Learners,2019-02-14,f15a535e-bc89-4755-9747-0c6206d80e80,lect a dataset of training examples demonstrating correct
Language Models are Unsupervised Multitask Learners,2019-02-14,4d117d70-e8d8-41cf-9df7-304e19478f08,"behavior for a desired task, train a system to imitate these"
Language Models are Unsupervised Multitask Learners,2019-02-14,fa084559-1fee-4beb-86b0-1e6399bc304d,"behaviors, and then test its performance on independent"
Language Models are Unsupervised Multitask Learners,2019-02-14,abeac96b-5f11-4e6b-89d8-bf4608209f7e,and identically distributed (IID) held-out examples. This
Language Models are Unsupervised Multitask Learners,2019-02-14,40d75984-11e5-4412-87a9-7c94c88aa484,has served well to make progress on narrow experts. But
Language Models are Unsupervised Multitask Learners,2019-02-14,2042a670-5cdf-454b-acf7-3c71d71b20a8,"the often erratic behavior of captioning models (Lake et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f651b0f3-1f73-494d-b6cb-3f7a14d26659,"2017), reading comprehension systems (Jia & Liang, 2017),"
Language Models are Unsupervised Multitask Learners,2019-02-14,bc1fae6d-549e-4db8-94e5-252ceabc79d3,"and image classiﬁers (Alcorn et al., 2018) on the diversity"
Language Models are Unsupervised Multitask Learners,2019-02-14,c1e6dbb8-5934-4804-9584-b7cfbb3782ba,and variety of possible inputs highlights some of the short-
Language Models are Unsupervised Multitask Learners,2019-02-14,60104744-d552-4def-983c-ce47c95ef050,comings of this approach.
Language Models are Unsupervised Multitask Learners,2019-02-14,277be741-8e5b-4d80-9486-29f54d0accd2,Our suspicion is that the prevalence of single task training
Language Models are Unsupervised Multitask Learners,2019-02-14,a292e948-7738-4fbc-bf87-d9339bd2043e,on single domain datasets is a major contributor to the lack
Language Models are Unsupervised Multitask Learners,2019-02-14,ae06fb6e-2a9a-4de6-a8ee-48b0a63b923a,of generalization observed in current systems. Progress
Language Models are Unsupervised Multitask Learners,2019-02-14,d52d4722-3d8a-4dbd-a6da-dab20be7b0d3,towards robust systems with current architectures is likely
Language Models are Unsupervised Multitask Learners,2019-02-14,c5fabbca-a068-41bc-9116-97e1bbef12e3,to require training and measuring performance on a wide
Language Models are Unsupervised Multitask Learners,2019-02-14,2f1cd9a3-e777-46e6-85d3-2b0d77cb582b,"range of domains and tasks. Recently, several benchmarks"
Language Models are Unsupervised Multitask Learners,2019-02-14,43cf5416-eb91-483c-aaa2-71eacd6d10f5,"have been proposed such as GLUE (Wang et al., 2018) and"
Language Models are Unsupervised Multitask Learners,2019-02-14,02b1c7c0-b457-44e4-af60-240f61f38bb1,"decaNLP (McCann et al., 2018) to begin studying this."
Language Models are Unsupervised Multitask Learners,2019-02-14,f6ebee06-36b4-4da0-8eb2-7f50a7ab495f,"Multitask learning (Caruana, 1997) is a promising frame-"
Language Models are Unsupervised Multitask Learners,2019-02-14,e1cd9778-e5e4-4d6c-aeb8-5ae8b217f9d5,"work for improving general performance. However, mul-"
Language Models are Unsupervised Multitask Learners,2019-02-14,3a5adf1b-71be-4316-bde8-6f842111984a,titask training in NLP is still nascent. Recent work re-
Language Models are Unsupervised Multitask Learners,2019-02-14,bca283fe-d45b-4a31-8edd-35f47d560843,"ports modest performance improvements (Yogatama et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,945b5772-c090-4c52-82f9-f37f4c04d1aa,2019) and the two most ambitious efforts to date have
Language Models are Unsupervised Multitask Learners,2019-02-14,a4b5ad7d-8e97-405f-acb8-4e3aaa6c77e3,"trained on a total of 10 and 17 (dataset, objective)"
Language Models are Unsupervised Multitask Learners,2019-02-14,24f7746e-a543-4993-9778-8e169b04fae4,"pairs respectively (McCann et al., 2018) (Bowman et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,d7c95b25-804a-423e-80af-01a22c2dd35f,"2018). From a meta-learning perspective, each (dataset,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f642a529-020e-46da-84cd-fa37e6bd8bf4,objective) pair is a single training example sampled
Language Models are Unsupervised Multitask Learners,2019-02-14,8e25be18-743f-4b59-bfb4-a25e1c282e7d,from the distribution of datasets and objectives. Current
Language Models are Unsupervised Multitask Learners,2019-02-14,2b3b6179-1b6a-4a41-9ca4-693689a3675b,ML systems need hundreds to thousands of examples to
Language Models are Unsupervised Multitask Learners,2019-02-14,64ea9f5e-bbdd-4d45-9f51-d3059fa0c2c6,induce functions which generalize well. This suggests that
Language Models are Unsupervised Multitask Learners,2019-02-14,9af6712d-9233-4cc3-b260-83c39c2d7063,multitask training many need just as many effective training
Language Models are Unsupervised Multitask Learners,2019-02-14,14610127-d4b1-4c83-93fc-ab5e91651692,pairs to realize its promise with current approaches. It will
Language Models are Unsupervised Multitask Learners,2019-02-14,4e3064a1-6752-42d5-bfdf-4cf9b144b85a,be very difﬁcult to continue to scale the creation of datasets
Language Models are Unsupervised Multitask Learners,2019-02-14,7154871f-6655-43b0-89f8-8f8d8e9f9a9b,and the design of objectives to the degree that may be re-
Language Models are Unsupervised Multitask Learners,2019-02-14,7d3fceba-26b1-4f5a-8c99-d14d44665a2d,quired to brute force our way there with current techniques.
Language Models are Unsupervised Multitask Learners,2019-02-14,5e19eab9-6b95-4ce8-9139-df8d43844116,This motivates exploring additional setups for performing
Language Models are Unsupervised Multitask Learners,2019-02-14,4eb6254c-0494-4b48-92fd-9f4e00e8f475,multitask learning.
Language Models are Unsupervised Multitask Learners,2019-02-14,d78b4597-f801-4581-a3f3-a550bb91a198,The current best performing systems on language tasks
Language Models are Unsupervised Multitask Learners,2019-02-14,66bcf8bd-7d24-4763-8115-b350db215fbd,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,6bddc5bd-1ce9-45d2-b1a0-c8c602a75ab9,Figure 1. Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results
Language Models are Unsupervised Multitask Learners,2019-02-14,17022b8a-38b3-409c-90f4-5a9befbf883d,"are on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,e13c2942-2d24-4ce6-8c5d-79dc4d15acc5,"2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result."
Language Models are Unsupervised Multitask Learners,2019-02-14,b4bbf6f6-c946-442d-98d2-78bc9c9274cf,utilize a combination of pre-training and supervised ﬁne-
Language Models are Unsupervised Multitask Learners,2019-02-14,c09ef56a-824c-4d8b-826f-5b1789f13af3,tuning. This approach has a long history with a trend to-
Language Models are Unsupervised Multitask Learners,2019-02-14,3e498f29-4bb8-44ba-b269-bd52f673ebec,"wards more ﬂexible forms of transfer. First, word vectors"
Language Models are Unsupervised Multitask Learners,2019-02-14,f02fce8e-d2ea-4811-859b-7ea08255fed7,were learned and used as inputs to task-speciﬁc architec-
Language Models are Unsupervised Multitask Learners,2019-02-14,363ee92e-70e7-4406-9cb4-e78fca812a6d,"tures (Mikolov et al., 2013) (Collobert et al., 2011), then"
Language Models are Unsupervised Multitask Learners,2019-02-14,a3a8ea81-2426-4716-ab12-2707c3d1fc82,the contextual representations of recurrent networks were
Language Models are Unsupervised Multitask Learners,2019-02-14,10dc38a5-a733-46c9-bf62-8ade4cf49eeb,"transferred (Dai & Le, 2015) (Peters et al., 2018), and re-"
Language Models are Unsupervised Multitask Learners,2019-02-14,aecae62a-bf74-4e24-8fc9-b2e5b5256b69,cent work suggests that task-speciﬁc architectures are no
Language Models are Unsupervised Multitask Learners,2019-02-14,e817d9f5-5d63-44ff-accd-10ddf314667e,longer necessary and transferring many self-attention blocks
Language Models are Unsupervised Multitask Learners,2019-02-14,567ea4bb-92e9-4ba0-987c-436bb1a4bde1,"is sufﬁcient (Radford et al., 2018) (Devlin et al., 2018)."
Language Models are Unsupervised Multitask Learners,2019-02-14,3fc91c9f-4c1d-4102-a6d1-473e40a2294d,These methods still require supervised training in order
Language Models are Unsupervised Multitask Learners,2019-02-14,ee0dc3dd-ea3f-4c1f-af5e-3ff50549055b,to perform a task. When only minimal or no supervised
Language Models are Unsupervised Multitask Learners,2019-02-14,7774d96b-b6fe-4293-96ac-ca5ff749c60f,"data is available, another line of work has demonstrated"
Language Models are Unsupervised Multitask Learners,2019-02-14,7cd2d424-10c8-4634-ae8e-7bed409313f8,"the promise of language models to perform speciﬁc tasks,"
Language Models are Unsupervised Multitask Learners,2019-02-14,df20d3f0-d869-4792-97a8-30c14c782fec,"such as commonsense reasoning (Schwartz et al., 2017) and"
Language Models are Unsupervised Multitask Learners,2019-02-14,99c4f8f8-e3e0-4b86-9f8a-929adbf89aa9,"sentiment analysis (Radford et al., 2017)."
Language Models are Unsupervised Multitask Learners,2019-02-14,d00c094d-e22e-41bf-b3ba-496d32b68261,"In this paper, we connect these two lines of work and con-"
Language Models are Unsupervised Multitask Learners,2019-02-14,97dd9def-5007-4eeb-81d8-26a35d4ab3c8,tinue the trend of more general methods of transfer. We
Language Models are Unsupervised Multitask Learners,2019-02-14,8fbbe8fa-3cf1-4828-b11c-807ef8939824,demonstrate language models can perform down-stream
Language Models are Unsupervised Multitask Learners,2019-02-14,5f1834ad-62df-4a33-b8f5-0b37300e3d78,tasks in a zero-shot setting – without any parameter or archi-
Language Models are Unsupervised Multitask Learners,2019-02-14,3b309f56-17c4-40e7-81a5-7146e6e60135,tecture modiﬁcation. We demonstrate this approach shows
Language Models are Unsupervised Multitask Learners,2019-02-14,4ccb2aca-735e-4f15-a364-457c0095f5ea,potential by highlighting the ability of language models to
Language Models are Unsupervised Multitask Learners,2019-02-14,4bd8ec96-23f9-4f12-9295-c0ea37e8e94e,perform a wide range of tasks in a zero-shot setting. We
Language Models are Unsupervised Multitask Learners,2019-02-14,dc3cd3a0-8c4d-44af-874e-9a0b73dd74b1,"achieve promising, competitive, and state of the art results"
Language Models are Unsupervised Multitask Learners,2019-02-14,d5da2b69-7509-47eb-9d2e-3e2b41969aa4,depending on the task.
Language Models are Unsupervised Multitask Learners,2019-02-14,371b07d2-2cf2-40f7-8a0f-1def2677877d,2. Approach
Language Models are Unsupervised Multitask Learners,2019-02-14,44be911e-ba78-4b60-a9c7-37aa364614d3,At the core of our approach is language modeling. Lan-
Language Models are Unsupervised Multitask Learners,2019-02-14,110bcbe5-838f-486c-aebe-9ca1bdd72bef,guage modeling is usually framed as unsupervised distri-
Language Models are Unsupervised Multitask Learners,2019-02-14,71863f08-ded5-432c-8396-5b59e7968390,bution estimation from a set of examples (x1; x2; :::; x n)
Language Models are Unsupervised Multitask Learners,2019-02-14,89f47a28-a663-4af0-8025-97621bf68b97,each composed of variable length sequences of symbols
Language Models are Unsupervised Multitask Learners,2019-02-14,8571d67a-5ddc-47af-8b7e-88503027c9bf,(s1; s2; :::; s n). Since language has a natural sequential or-
Language Models are Unsupervised Multitask Learners,2019-02-14,31f3f095-6bd5-4887-a517-9f04ae38746e,"dering, it is common to factorize the joint probabilities oversymbols as the product of conditional probabilities (Jelinek"
Language Models are Unsupervised Multitask Learners,2019-02-14,99c0c64d-a63b-4133-bdab-201b62bee577,"& Mercer, 1980) (Bengio et al., 2003):"
Language Models are Unsupervised Multitask Learners,2019-02-14,480d340e-d4b0-4909-9101-4d9571516127,p(x) =nY
Language Models are Unsupervised Multitask Learners,2019-02-14,d485117b-c7c5-4871-869b-7de52aa49d31,i=1p(snjs1; :::; s n 1) (1)
Language Models are Unsupervised Multitask Learners,2019-02-14,7146ad2b-3619-48e1-9f84-27ad65f6cd8c,This approach allows for tractable sampling from and es-
Language Models are Unsupervised Multitask Learners,2019-02-14,a40ce19b-be55-4a22-84ba-0c4f1d0ff751,timation of p(x)as well as any conditionals of the form
Language Models are Unsupervised Multitask Learners,2019-02-14,67dacd58-6aad-479d-b022-1e47816f8a07,"p(sn k; :::; s njs1; :::; s n k 1). In recent years, there have"
Language Models are Unsupervised Multitask Learners,2019-02-14,c7325de3-7a7a-4847-880f-2a592c76579c,been signiﬁcant improvements in the expressiveness of mod-
Language Models are Unsupervised Multitask Learners,2019-02-14,38baded5-6f67-4a70-b3f4-0b505e2ba885,"els that can compute these conditional probabilities, such as"
Language Models are Unsupervised Multitask Learners,2019-02-14,ad823ed4-d673-4af8-ad32-9285c7466d29,self-attention architectures like the Transformer (Vaswani
Language Models are Unsupervised Multitask Learners,2019-02-14,19c0eb0e-6752-4fe2-bd02-5cd5c8ea688b,"et al., 2017)."
Language Models are Unsupervised Multitask Learners,2019-02-14,e2b44618-01b4-4d4f-ad2b-3824cfb5a488,Learning to perform a single task can be expressed in a
Language Models are Unsupervised Multitask Learners,2019-02-14,111efb4a-96a2-4316-bdbd-0736b1dcbf5c,probabilistic framework as estimating a conditional distri-
Language Models are Unsupervised Multitask Learners,2019-02-14,941b5a4f-bc50-47f7-b163-11fd3dd8a09d,bution p(output jinput ). Since a general system should be
Language Models are Unsupervised Multitask Learners,2019-02-14,16b5072f-49fd-4f57-9998-a13530e8a06b,"able to perform many different tasks, even for the same"
Language Models are Unsupervised Multitask Learners,2019-02-14,4e77de79-0cf0-4195-869b-76171da63ff9,"input, it should condition not only on the input but also"
Language Models are Unsupervised Multitask Learners,2019-02-14,12e008f1-1f7f-4b9e-a315-65a4a329c0f1,"on the task to be performed. That is, it should model"
Language Models are Unsupervised Multitask Learners,2019-02-14,9933d4b0-9ec4-43ff-8da5-3b8851035b99,p(output jinput; task ). This has been variously formalized
Language Models are Unsupervised Multitask Learners,2019-02-14,f2682b6d-1c5d-4542-958a-c41c05fd8d8b,in multitask and meta-learning settings. Task conditioning
Language Models are Unsupervised Multitask Learners,2019-02-14,ef65e3d5-6098-40c2-933e-2e769eeeddd9,"is often implemented at an architectural level, such as the"
Language Models are Unsupervised Multitask Learners,2019-02-14,4617fe72-0ad8-4d58-a356-09b777526ebf,"task speciﬁc encoders and decoders in (Kaiser et al., 2017)"
Language Models are Unsupervised Multitask Learners,2019-02-14,069913e0-3df3-421a-ad79-1b64e400a478,or at an algorithmic level such as the inner and outer loop
Language Models are Unsupervised Multitask Learners,2019-02-14,5c90d2fe-fcc5-4629-b050-b56ed17228b5,"optimization framework of MAML (Finn et al., 2017). But"
Language Models are Unsupervised Multitask Learners,2019-02-14,cdbfa984-c22e-4295-82d7-5189ea7e6e41,"as exempliﬁed in McCann et al. (2018), language provides"
Language Models are Unsupervised Multitask Learners,2019-02-14,db799fe7-fdd0-412e-819b-06fa3f337868,"a ﬂexible way to specify tasks, inputs, and outputs all as a"
Language Models are Unsupervised Multitask Learners,2019-02-14,0787940b-3a78-4321-a9bc-8fb95f454135,"sequence of symbols. For example, a translation training"
Language Models are Unsupervised Multitask Learners,2019-02-14,a5323e9b-5032-4b83-b93f-f747045f5dce,example can be written as the sequence (translate to
Language Models are Unsupervised Multitask Learners,2019-02-14,d72fd97b-76f9-45fb-a8ea-12249f0496f0,"french, english text, french text) . Like-"
Language Models are Unsupervised Multitask Learners,2019-02-14,677f3d65-8d0c-416e-84f2-a5cd60baa93c,"wise, a reading comprehension training example can"
Language Models are Unsupervised Multitask Learners,2019-02-14,ecb82e23-1cd2-4fba-b1f4-514be3055bed,"be written as (answer the question, document,"
Language Models are Unsupervised Multitask Learners,2019-02-14,eabdd3b8-8a5b-4d38-8111-7a57f79f3a8b,"question, answer) . McCann et al. (2018) demon-"
Language Models are Unsupervised Multitask Learners,2019-02-14,d94a3dbc-e808-4786-8aa4-93c06d133650,"strated it was possible to train a single model, the MQAN,"
Language Models are Unsupervised Multitask Learners,2019-02-14,e859204a-0ae1-4ab0-9c56-00767f608a2b,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,386e51d3-be4b-44ac-8b0f-caaeb3edef36,to infer and perform many different tasks on examples with
Language Models are Unsupervised Multitask Learners,2019-02-14,c72c1a6a-3771-49ff-a7aa-e3955f9510ba,this type of format.
Language Models are Unsupervised Multitask Learners,2019-02-14,99d20ffe-af26-41c0-bb42-01db44d65879,"Language modeling is also able to, in principle, learn the"
Language Models are Unsupervised Multitask Learners,2019-02-14,c514b7ea-5830-4985-8894-fb93e5f9e66c,tasks of McCann et al. (2018) without the need for explicit
Language Models are Unsupervised Multitask Learners,2019-02-14,9af304b8-a445-47f0-9783-870fd56dc4b5,supervision of which symbols are the outputs to be pre-
Language Models are Unsupervised Multitask Learners,2019-02-14,3ad75ca5-9c36-4244-b724-0515538a702e,dicted. Since the supervised objective is the the same as the
Language Models are Unsupervised Multitask Learners,2019-02-14,172f8085-8078-48c5-817b-b68c64d2bc6f,unsupervised objective but only evaluated on a subset of the
Language Models are Unsupervised Multitask Learners,2019-02-14,d868a2f7-9736-4068-9d32-2ed1e52f92c8,"sequence, the global minimum of the unsupervised objective"
Language Models are Unsupervised Multitask Learners,2019-02-14,3db687a1-1dc3-4a29-9a0f-6ec3d0ef536f,is also the global minimum of the supervised objective. In
Language Models are Unsupervised Multitask Learners,2019-02-14,86ad6b43-9c22-4d30-8cc5-4cd195d86daf,"this slightly toy setting, the concerns with density estimation"
Language Models are Unsupervised Multitask Learners,2019-02-14,3263ebed-94e8-42c2-9f94-0568a73600c7,as a principled training objective discussed in (Sutskever
Language Models are Unsupervised Multitask Learners,2019-02-14,48a88643-b596-4780-ada2-e40aa461060d,"et al., 2015) are side stepped. The problem instead becomes"
Language Models are Unsupervised Multitask Learners,2019-02-14,d7bdfbfc-a81d-4c1a-a4cf-e68338456496,"whether we are able to, in practice, optimize the unsuper-"
Language Models are Unsupervised Multitask Learners,2019-02-14,16238c49-6d6a-40a3-a3a7-bef001864ced,vised objective to convergence. Preliminary experiments
Language Models are Unsupervised Multitask Learners,2019-02-14,7e83470f-c130-4127-bf86-45bf002c14d8,conﬁrmed that sufﬁciently large language models are able to
Language Models are Unsupervised Multitask Learners,2019-02-14,6364f0a8-8efc-410d-bf76-1bec0f2b52d0,perform multitask learning in this toy-ish setup but learning
Language Models are Unsupervised Multitask Learners,2019-02-14,e287afd0-fa46-4830-90a7-51dadc7019fe,is much slower than in explicitly supervised approaches.
Language Models are Unsupervised Multitask Learners,2019-02-14,3d08122e-3d0f-4327-a455-0c2d1d08913c,While it is a large step from the well-posed setup described
Language Models are Unsupervised Multitask Learners,2019-02-14,eb4f6c22-8644-418f-909f-6dda9423d081,"above to the messiness of “language in the wild”, Weston"
Language Models are Unsupervised Multitask Learners,2019-02-14,633a05fc-fa1a-471e-9df9-e5bb1afa95ff,"(2016) argues, in the context of dialog, for the need to"
Language Models are Unsupervised Multitask Learners,2019-02-14,2670d2e8-beda-4034-bf7e-a8ea8407172e,develop systems capable of learning from natural language
Language Models are Unsupervised Multitask Learners,2019-02-14,2188f79a-650d-4982-b362-45e3e8625656,directly and demonstrated a proof of concept – learning a
Language Models are Unsupervised Multitask Learners,2019-02-14,aab301fb-0361-4970-a9cc-caea815c02ac,QA task without a reward signal by using forward prediction
Language Models are Unsupervised Multitask Learners,2019-02-14,2e4b1a61-8b10-4153-a0ce-7771797fe04b,"of a teacher’s outputs. While dialog is an attractive approach,"
Language Models are Unsupervised Multitask Learners,2019-02-14,3f981a4e-7643-40b2-86f9-3df490ab836f,we worry it is overly restrictive. The internet contains a vast
Language Models are Unsupervised Multitask Learners,2019-02-14,30e41192-9a03-40b8-bae6-d9af4ac7fe8d,amount of information that is passively available without
Language Models are Unsupervised Multitask Learners,2019-02-14,b231ec68-aaa4-4eee-ae1e-5fbf2a20439c,the need for interactive communication. Our speculation is
Language Models are Unsupervised Multitask Learners,2019-02-14,2a5d43f3-8b7a-449a-a560-964e28bf6ad6,that a language model with sufﬁcient capacity will begin
Language Models are Unsupervised Multitask Learners,2019-02-14,5f9d5c40-0115-45f9-84b8-458a850f7157,to learn to infer and perform the tasks demonstrated in
Language Models are Unsupervised Multitask Learners,2019-02-14,335da9bf-0ad0-4b73-8ab1-e28804378b3f,"natural language sequences in order to better predict them,"
Language Models are Unsupervised Multitask Learners,2019-02-14,275f85f7-f18e-43e0-a730-0aa274a7aa79,regardless of their method of procurement. If a language
Language Models are Unsupervised Multitask Learners,2019-02-14,fab43442-0bfc-448c-ae60-c102842d8545,"model is able to do this it will be, in effect, performing"
Language Models are Unsupervised Multitask Learners,2019-02-14,8a0097f2-0547-4b33-bede-9fd882c91c6f,unsupervised multitask learning. We test whether this is the
Language Models are Unsupervised Multitask Learners,2019-02-14,b4816a30-3186-444d-ab40-aeedb8133fa1,case by analyzing the performance of language models in a
Language Models are Unsupervised Multitask Learners,2019-02-14,e999c969-2d6d-4fe9-a571-430081e0c38c,zero-shot setting on a wide variety of tasks.
Language Models are Unsupervised Multitask Learners,2019-02-14,3252f650-8c9c-4d5c-b90e-32f4fa2774c6,2.1. Training Dataset
Language Models are Unsupervised Multitask Learners,2019-02-14,ff120e22-0a54-4f9f-bf6e-61ab6043b1bc,Most prior work trained language models on a single do-
Language Models are Unsupervised Multitask Learners,2019-02-14,f731dca0-4c5b-4093-abc4-b84637f4f802,"main of text, such as news articles (Jozefowicz et al., 2016),"
Language Models are Unsupervised Multitask Learners,2019-02-14,de0c9afa-32e6-4e85-a126-f5722efea175,"Wikipedia (Merity et al., 2016), or ﬁction books (Kiros"
Language Models are Unsupervised Multitask Learners,2019-02-14,6fdc9569-7a13-4add-83fc-122b8098bd80,"et al., 2015). Our approach motivates building as large and"
Language Models are Unsupervised Multitask Learners,2019-02-14,453d2b75-cab6-4254-848c-8f7d68de8267,diverse a dataset as possible in order to collect natural lan-
Language Models are Unsupervised Multitask Learners,2019-02-14,b4d6392f-3e09-4200-9b8c-73b9ca30f69a,guage demonstrations of tasks in as varied of domains and
Language Models are Unsupervised Multitask Learners,2019-02-14,ddf9d0d2-49b3-4e47-b763-641f8446072a,contexts as possible.
Language Models are Unsupervised Multitask Learners,2019-02-14,926734e0-6828-4467-aa90-e20f09d7ecff,A promising source of diverse and nearly unlimited text is
Language Models are Unsupervised Multitask Learners,2019-02-14,60fa881e-7429-48ba-a2f5-78fdb04ed513,web scrapes such as Common Crawl. While these archives
Language Models are Unsupervised Multitask Learners,2019-02-14,0a240f26-5cd5-4f93-a5e2-0d626a837f5c,are many orders of magnitude larger than current language
Language Models are Unsupervised Multitask Learners,2019-02-14,9bf28280-86c8-4d63-9071-f3107b5b3f5a,"modeling datasets, they have signiﬁcant data quality issues."
Language Models are Unsupervised Multitask Learners,2019-02-14,41cdd782-fb94-4091-bcdb-c9a55e02cf08,Trinh & Le (2018) used Common Crawl in their work on
Language Models are Unsupervised Multitask Learners,2019-02-14,f3b9886a-5d52-4246-aa53-b36b1793a233,commonsense reasoning but noted a large amount of doc-
Language Models are Unsupervised Multitask Learners,2019-02-14,cc65c003-88c4-40d9-9b80-d2f2ca59987d,uments “whose content are mostly unintelligible”. We ob-
Language Models are Unsupervised Multitask Learners,2019-02-14,6644152b-2064-41b2-985b-a685a420f9c7,"served similar data issues in our initial experiments with”I’m not the cleverest man in the world, but like they say in"
Language Models are Unsupervised Multitask Learners,2019-02-14,7302944d-8c0f-4742-a4e5-be988d8d1efa,French: Je ne suis pas un imbecile [I’m not a fool].
Language Models are Unsupervised Multitask Learners,2019-02-14,a7b58417-d91c-4d2b-bb48-08bb4b260535,"In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate"
Language Models are Unsupervised Multitask Learners,2019-02-14,42dc166a-9453-441d-807b-a727e5cb55d4,"in the riding of Joliette, wrote in French: ” Mentez mentez,"
Language Models are Unsupervised Multitask Learners,2019-02-14,3bae8d45-48de-48e3-abb8-f5372157f8d8,"il en restera toujours quelque chose ,” which translates as,"
Language Models are Unsupervised Multitask Learners,2019-02-14,6403ba51-8f8d-478d-9bce-65541066e3f7,”Lie lie and something will always remain. ”
Language Models are Unsupervised Multitask Learners,2019-02-14,d6d4a3f4-691a-422d-bb28-7b35e591ac03,"“I hate the word ‘ perfume ,”’ Burr says. ‘It’s somewhat better"
Language Models are Unsupervised Multitask Learners,2019-02-14,5c545744-539e-4de4-8631-b55f0084e9e3,in French: ‘ parfum .’
Language Models are Unsupervised Multitask Learners,2019-02-14,5e728e15-e521-41f4-bfd8-d7dff3086e25,"If listened carefully at 29:55, a conversation can be heard"
Language Models are Unsupervised Multitask Learners,2019-02-14,124f34ad-92b5-48df-98f4-30f15a0a1b16,between two guys in French: “ -Comment on fait pour aller
Language Models are Unsupervised Multitask Learners,2019-02-14,8d00cde8-4040-4574-8af5-18d7c6cacd63,"de l’autre cot ´e? -Quel autre cot ´e?”, which means “ - How"
Language Models are Unsupervised Multitask Learners,2019-02-14,5f413fdf-9c6e-4628-a1bf-eda65d991f73,do you get to the other side? - What side? ”.
Language Models are Unsupervised Multitask Learners,2019-02-14,f159f4e4-0374-41c1-88ad-8f4fd8027c76,"If this sounds like a bit of a stretch, consider this ques-"
Language Models are Unsupervised Multitask Learners,2019-02-14,912852cc-de29-4d8d-8f75-997245539438,"tion in French: As-tu aller au cin ´ema? , orDid you go to"
Language Models are Unsupervised Multitask Learners,2019-02-14,7067c7ea-7e36-4a1e-9350-f78095b32cdb,"the movies? , which literally translates as Have-you to go to"
Language Models are Unsupervised Multitask Learners,2019-02-14,3507646c-dc8d-490f-9815-694bc0d32bae,movies/theater?
Language Models are Unsupervised Multitask Learners,2019-02-14,f9fe6a7c-059e-4d79-af79-eed7466753d9,"“Brevet Sans Garantie Du Gouvernement ”, translated to"
Language Models are Unsupervised Multitask Learners,2019-02-14,595d7e38-440d-4a06-a258-0fa13a0c6486,English: “ Patented without government warranty ”.
Language Models are Unsupervised Multitask Learners,2019-02-14,ecf845d0-d59a-4f9e-b492-79861c637859,Table 1. Examples of naturally occurring demonstrations of En-
Language Models are Unsupervised Multitask Learners,2019-02-14,0002f5ea-a888-4c66-b595-b74eddb7aca9,glish to French and French to English translation found throughout
Language Models are Unsupervised Multitask Learners,2019-02-14,0ebb6304-c82c-499d-a395-01c77072b26d,the WebText training set.
Language Models are Unsupervised Multitask Learners,2019-02-14,faa729ca-3cab-4941-8510-abcbb6e8ad1d,Common Crawl. Trinh & Le (2018)’s best results were
Language Models are Unsupervised Multitask Learners,2019-02-14,9aeacf14-6d7f-4c11-94ee-ec37a2d7afba,achieved using a small subsample of Common Crawl which
Language Models are Unsupervised Multitask Learners,2019-02-14,e789fa6e-8378-476a-84b8-2404dc01f32b,"included only documents most similar to their target dataset,"
Language Models are Unsupervised Multitask Learners,2019-02-14,5be447d3-9c6f-4bdc-bd53-f0e6ed75bb6c,the Winograd Schema Challenge. While this is a pragmatic
Language Models are Unsupervised Multitask Learners,2019-02-14,447aa5e0-5ac1-4daa-915e-9d6f9cc156eb,"approach to improve performance on a speciﬁc task, we"
Language Models are Unsupervised Multitask Learners,2019-02-14,cd06b3cf-02b9-4813-b7a4-ab302d616ae3,want to avoid making assumptions about the tasks to be
Language Models are Unsupervised Multitask Learners,2019-02-14,e6826221-af45-4c0d-8915-85d02ab257ad,performed ahead of time.
Language Models are Unsupervised Multitask Learners,2019-02-14,6d40c5b9-945e-442b-a953-fcad604208f0,"Instead, we created a new web scrape which emphasizes"
Language Models are Unsupervised Multitask Learners,2019-02-14,554a7776-521c-43cd-919e-21c8a85dd99d,document quality. To do this we only scraped web pages
Language Models are Unsupervised Multitask Learners,2019-02-14,a6fb10c3-e18c-4663-ba50-aee94120d649,which have been curated/ﬁltered by humans. Manually
Language Models are Unsupervised Multitask Learners,2019-02-14,18c68ade-db53-4962-a9ea-355a7390fce9,ﬁltering a full web scrape would be exceptionally expensive
Language Models are Unsupervised Multitask Learners,2019-02-14,e9ed1fad-d0cf-4f99-a24c-5be30bcc0a2b,"so as a starting point, we scraped all outbound links from"
Language Models are Unsupervised Multitask Learners,2019-02-14,cd381a2a-460a-498f-8a63-14d1c6bc89ce,"Reddit, a social media platform, which received at least 3"
Language Models are Unsupervised Multitask Learners,2019-02-14,ebdc12d9-647d-4b69-b8d2-24e415d8a2c4,karma. This can be thought of as a heuristic indicator for
Language Models are Unsupervised Multitask Learners,2019-02-14,a67f327b-e637-47cb-959d-1f48e53f0177,"whether other users found the link interesting, educational,"
Language Models are Unsupervised Multitask Learners,2019-02-14,3fbf01ce-3a3a-4857-9b01-0ebe8c7e9f05,or just funny.
Language Models are Unsupervised Multitask Learners,2019-02-14,a6b05905-9cff-47c1-941b-751ca0a3a133,"The resulting dataset, WebText, contains the text subset"
Language Models are Unsupervised Multitask Learners,2019-02-14,ef297f76-9022-4345-824b-91ffeabc8ab6,of these 45 million links. To extract the text from HTML
Language Models are Unsupervised Multitask Learners,2019-02-14,2e969a61-dbc3-4e9e-89f0-7ded7fbf4dff,responses we use a combination of the Dragnet (Peters &
Language Models are Unsupervised Multitask Learners,2019-02-14,689f959c-2347-4d88-8b0c-99e1b04c3d64,"Lecocq, 2013) and Newspaper1content extractors. All re-"
Language Models are Unsupervised Multitask Learners,2019-02-14,02374b4c-7a63-4d2b-8941-f1b58b526add,sults presented in this paper use a preliminary version of
Language Models are Unsupervised Multitask Learners,2019-02-14,5ea499f8-97e6-4333-8988-cf31acd46ed3,WebText which does not include links created after Dec
Language Models are Unsupervised Multitask Learners,2019-02-14,c0a820b8-5aeb-4a15-b37f-6123583a316b,2017 and which after de-duplication and some heuristic
Language Models are Unsupervised Multitask Learners,2019-02-14,652521b3-eb92-4435-aec4-ce106d4510aa,based cleaning contains slightly over 8 million documents
Language Models are Unsupervised Multitask Learners,2019-02-14,f9d6cd38-5c1a-42f1-82dc-cd6937bccce9,for a total of 40 GB of text. We removed all Wikipedia
Language Models are Unsupervised Multitask Learners,2019-02-14,e1a32ca1-5f81-436c-9940-80332c7f57a7,documents from WebText since it is a common data source
Language Models are Unsupervised Multitask Learners,2019-02-14,1077b7cb-cbc0-46f0-a7af-4c6813c8903d,for other datasets and could complicate analysis due to over-
Language Models are Unsupervised Multitask Learners,2019-02-14,bb71119b-3a88-4343-b3c4-4519741d9542,1https://github.com/codelucas/newspaper
Language Models are Unsupervised Multitask Learners,2019-02-14,11ad2b38-3ecc-4c59-8b2f-c62a47c38d75,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,5c944e17-158c-4776-91d7-8afc9e2b65eb,lapping training data with test evaluation tasks.
Language Models are Unsupervised Multitask Learners,2019-02-14,379cd8c1-17b8-4955-aac6-1245ff410b8a,2.2. Input Representation
Language Models are Unsupervised Multitask Learners,2019-02-14,0a00aba6-a291-4f73-8cfb-24fe5a2eb552,A general language model (LM) should be able to compute
Language Models are Unsupervised Multitask Learners,2019-02-14,8663eec5-a8d3-4811-8613-79480b5c22cc,the probability of (and also generate) any string. Current
Language Models are Unsupervised Multitask Learners,2019-02-14,12765d0b-4c51-4c50-a521-51431900af23,large scale LMs include pre-processing steps such as lower-
Language Models are Unsupervised Multitask Learners,2019-02-14,ef7b90f4-4ca0-4f4f-a627-c2a0034b8edd,"casing, tokenization, and out-of-vocabulary tokens which"
Language Models are Unsupervised Multitask Learners,2019-02-14,d014226b-912f-438c-9e2c-23a40ce58944,restrict the space of model-able strings. While processing
Language Models are Unsupervised Multitask Learners,2019-02-14,6aad8c45-429e-4315-8c2d-b2f9885b4c43,Unicode strings as a sequence of UTF-8 bytes elegantly ful-
Language Models are Unsupervised Multitask Learners,2019-02-14,6399708b-2394-4ec6-9ccd-0809447ee99b,ﬁlls this requirement as exempliﬁed in work such as Gillick
Language Models are Unsupervised Multitask Learners,2019-02-14,ff4d53c5-bf17-4969-a86d-92f31959b50a,"et al. (2015), current byte-level LMs are not competitive"
Language Models are Unsupervised Multitask Learners,2019-02-14,ad2f6e99-663c-4908-a445-d7f6e5d30a40,with word-level LMs on large scale datasets such as the
Language Models are Unsupervised Multitask Learners,2019-02-14,fbfb98d1-2d6e-4bcb-8c1b-ee992fccf70b,"One Billion Word Benchmark (Al-Rfou et al., 2018). We"
Language Models are Unsupervised Multitask Learners,2019-02-14,1edfc3c4-f0e5-4477-ad44-c0eeeda8682e,observed a similar performance gap in our own attempts to
Language Models are Unsupervised Multitask Learners,2019-02-14,7325efa8-e4c6-4b66-b229-9a0b31cfe7b6,train standard byte-level LMs on WebText.
Language Models are Unsupervised Multitask Learners,2019-02-14,4d599095-541b-4f51-aded-5e991c842880,"Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a"
Language Models are Unsupervised Multitask Learners,2019-02-14,8d3f92e2-f9ff-495c-8786-a13d63b93e4f,practical middle ground between character and word level
Language Models are Unsupervised Multitask Learners,2019-02-14,9f1622b7-e219-4bbc-97f2-fcbb0e4a0968,language modeling which effectively interpolates between
Language Models are Unsupervised Multitask Learners,2019-02-14,38b7cffd-2624-4948-95ae-a00d93d049d5,word level inputs for frequent symbol sequences and char-
Language Models are Unsupervised Multitask Learners,2019-02-14,63d506f6-a8d2-427e-823c-65400f3ae4fe,acter level inputs for infrequent symbol sequences. Despite
Language Models are Unsupervised Multitask Learners,2019-02-14,c9b341ed-aa51-402f-b106-43af0119bb50,"its name, reference BPE implementations often operate on"
Language Models are Unsupervised Multitask Learners,2019-02-14,ceafbf1d-c617-4f42-84d5-29397829c3f5,Unicode code points and not byte sequences. These imple-
Language Models are Unsupervised Multitask Learners,2019-02-14,dfebea65-ddf5-4264-8740-22c5e6df93a1,mentations would require including the full space of Uni-
Language Models are Unsupervised Multitask Learners,2019-02-14,8207c970-149d-4102-bf68-4df5ab07d5d1,code symbols in order to model all Unicode strings. This
Language Models are Unsupervised Multitask Learners,2019-02-14,0b78fe15-474c-4aba-a444-fea155b4b6c2,"would result in a base vocabulary of over 130,000 before"
Language Models are Unsupervised Multitask Learners,2019-02-14,ad2e124f-c202-4fa7-9760-df94c226773a,any multi-symbol tokens are added. This is prohibitively
Language Models are Unsupervised Multitask Learners,2019-02-14,6a1ab023-7aa5-4c84-a561-02f11ae796a8,"large compared to the 32,000 to 64,000 token vocabularies"
Language Models are Unsupervised Multitask Learners,2019-02-14,8691db83-0425-4682-bef3-b5e6bb90077d,"often used with BPE. In contrast, a byte-level version of"
Language Models are Unsupervised Multitask Learners,2019-02-14,632fd9b8-3ea9-43dc-b5d9-03b0092d9310,"BPE only requires a base vocabulary of size 256. However,"
Language Models are Unsupervised Multitask Learners,2019-02-14,2bc90be8-4b94-4b80-8f66-a4f8782abba0,directly applying BPE to the byte sequence results in sub-
Language Models are Unsupervised Multitask Learners,2019-02-14,6eecc793-a773-4385-9dec-3064e0b8e845,optimal merges due to BPE using a greedy frequency based
Language Models are Unsupervised Multitask Learners,2019-02-14,56fd9c31-b40d-4768-bcf4-e94f81e20f12,heuristic for building the token vocabulary. We observed
Language Models are Unsupervised Multitask Learners,2019-02-14,be959d79-60a2-4aef-81e0-a30563e333ab,BPE including many versions of common words like dog
Language Models are Unsupervised Multitask Learners,2019-02-14,15ba8c32-35f3-49ca-b782-03fb9bdcbb56,since they occur in many variations such as dog. dog!
Language Models are Unsupervised Multitask Learners,2019-02-14,82e5969b-909e-4551-b368-adcf37aa30c2,dog? . This results in a sub-optimal allocation of limited
Language Models are Unsupervised Multitask Learners,2019-02-14,1aa0b8f3-4e91-46eb-b786-8357fe5f03a9,"vocabulary slots and model capacity. To avoid this, we pre-"
Language Models are Unsupervised Multitask Learners,2019-02-14,91570fb1-5817-4c03-8466-16e5b034e5db,vent BPE from merging across character categories for any
Language Models are Unsupervised Multitask Learners,2019-02-14,0020fb4d-5f44-462e-99c3-100d6425fc99,byte sequence. We add an exception for spaces which sig-
Language Models are Unsupervised Multitask Learners,2019-02-14,19e34a72-093d-461a-ba8c-82e19f3bb7af,niﬁcantly improves the compression efﬁciency while adding
Language Models are Unsupervised Multitask Learners,2019-02-14,1c083a29-892d-4426-bdb7-554cb98d2a1d,only minimal fragmentation of words across multiple vocab
Language Models are Unsupervised Multitask Learners,2019-02-14,a46219ed-ccb2-4291-b294-49c0baf7de09,tokens.
Language Models are Unsupervised Multitask Learners,2019-02-14,171edfaa-c3b2-4dcc-80f4-c75a7c24ac7a,This input representation allows us to combine the empirical
Language Models are Unsupervised Multitask Learners,2019-02-14,de17fadf-274c-4009-a017-b76bd7e68c3d,beneﬁts of word-level LMs with the generality of byte-level
Language Models are Unsupervised Multitask Learners,2019-02-14,fe0ed0e6-5102-4d33-a660-004a30affab3,approaches. Since our approach can assign a probability to
Language Models are Unsupervised Multitask Learners,2019-02-14,de91b966-c1fc-4f29-b2ff-39e9d2f53b37,"any Unicode string, this allows us to evaluate our LMs on"
Language Models are Unsupervised Multitask Learners,2019-02-14,4d9e3099-7445-4742-867a-b21856e429b1,"any dataset regardless of pre-processing, tokenization, or"
Language Models are Unsupervised Multitask Learners,2019-02-14,e5260c2b-c4e0-4994-b8b2-69c3d554daba,vocab size.
Language Models are Unsupervised Multitask Learners,2019-02-14,68a55265-339d-44b1-9df1-e8da87e08368,2.3. Model
Language Models are Unsupervised Multitask Learners,2019-02-14,9a3ac026-1480-4bc2-8659-34fcdbc8c681,"We use a Transformer (Vaswani et al., 2017) based archi-"
Language Models are Unsupervised Multitask Learners,2019-02-14,8f26a4c8-215b-4580-887b-5ee25ce15a6c,tecture for our LMs. The model largely follows the details
Language Models are Unsupervised Multitask Learners,2019-02-14,b8c5b1d7-f980-490b-a4fe-7c6f39cf0a72,"of the OpenAI GPT model (Radford et al., 2018) with aParameters Layers dmodel"
Language Models are Unsupervised Multitask Learners,2019-02-14,2e5a4e42-6052-4789-91c0-dba64d3b1a27,117M 12 768
Language Models are Unsupervised Multitask Learners,2019-02-14,05958cdf-6a74-4a17-b57b-08821b8eb1ec,345M 24 1024
Language Models are Unsupervised Multitask Learners,2019-02-14,da97450f-cdb6-45f3-9b86-b44e77021f7f,762M 36 1280
Language Models are Unsupervised Multitask Learners,2019-02-14,5813c53e-0aa4-4ca5-9434-a6a4e6466459,1542M 48 1600
Language Models are Unsupervised Multitask Learners,2019-02-14,24368d65-d075-4ca5-88a9-e72dfcd1b5c9,Table 2. Architecture hyperparameters for the 4 model sizes.
Language Models are Unsupervised Multitask Learners,2019-02-14,675150b4-ffbb-4fb8-877b-33ef91c86f32,"few modiﬁcations. Layer normalization (Ba et al., 2016)"
Language Models are Unsupervised Multitask Learners,2019-02-14,1de280f2-c804-4ddf-b2df-6b0cb88ae2b3,"was moved to the input of each sub-block, similar to a"
Language Models are Unsupervised Multitask Learners,2019-02-14,e67531eb-d894-4ca6-b99d-82c248d2cd5a,"pre-activation residual network (He et al., 2016) and an"
Language Models are Unsupervised Multitask Learners,2019-02-14,cd9ea88a-69d3-49cf-beeb-0a35fdb49606,additional layer normalization was added after the ﬁnal self-
Language Models are Unsupervised Multitask Learners,2019-02-14,83bfda15-84d9-448a-9769-94642cb86e60,attention block. A modiﬁed initialization which accounts
Language Models are Unsupervised Multitask Learners,2019-02-14,0aaf2cc3-de1c-4039-b677-7715e81a4154,for the accumulation on the residual path with model depth
Language Models are Unsupervised Multitask Learners,2019-02-14,4e281f79-1050-4935-8f92-ddfcc9e10d24,is used. We scale the weights of residual layers at initial-
Language Models are Unsupervised Multitask Learners,2019-02-14,610171d3-ee93-43d9-8155-91c55c93bf99,ization by a factor of 1=p
Language Models are Unsupervised Multitask Learners,2019-02-14,dc5004db-4454-402d-b7cc-d5dc49087735,Nwhere Nis the number of
Language Models are Unsupervised Multitask Learners,2019-02-14,e2fe29d4-5d6a-4046-a70a-84ca1d87ec5a,"residual layers. The vocabulary is expanded to 50,257. We"
Language Models are Unsupervised Multitask Learners,2019-02-14,6fe53d41-c7ba-44f5-8dd0-3d655c3f9719,also increase the context size from 512 to 1024 tokens and
Language Models are Unsupervised Multitask Learners,2019-02-14,1d3b9630-b1ac-47a7-b3ef-7a68603132f8,a larger batchsize of 512 is used.
Language Models are Unsupervised Multitask Learners,2019-02-14,866b746d-22ef-4789-8791-b2ffccd5ece4,3. Experiments
Language Models are Unsupervised Multitask Learners,2019-02-14,c1469761-0613-4768-8af0-6f984dea0098,We trained and benchmarked four LMs with approximately
Language Models are Unsupervised Multitask Learners,2019-02-14,24bac14c-1ae9-4b0d-95de-97dc2e1f246d,log-uniformly spaced sizes. The architectures are summa-
Language Models are Unsupervised Multitask Learners,2019-02-14,fc442a44-1f0b-40e7-9b16-2deaad5422d6,rized in Table 2. The smallest model is equivalent to the
Language Models are Unsupervised Multitask Learners,2019-02-14,c0d7b6f9-67dc-444d-958f-2c925dac0e9a,"original GPT, and the second smallest equivalent to the"
Language Models are Unsupervised Multitask Learners,2019-02-14,a2334f27-55dc-4932-9970-1e1a1ea8bc79,"largest model from BERT (Devlin et al., 2018). Our largest"
Language Models are Unsupervised Multitask Learners,2019-02-14,c4a8968c-3029-4036-bf14-282300100384,"model, which we call GPT-2, has over an order of magni-"
Language Models are Unsupervised Multitask Learners,2019-02-14,270ca32a-1ac1-4101-b264-3b3191c790e7,tude more parameters than GPT. The learning rate of each
Language Models are Unsupervised Multitask Learners,2019-02-14,ec682a15-cc99-45e0-a8cf-e02da8ee3cca,model was manually tuned for the best perplexity on a 5%
Language Models are Unsupervised Multitask Learners,2019-02-14,d4dc7a73-b044-4a13-8616-be733fa78d56,held-out sample of WebText. All models still underﬁt Web-
Language Models are Unsupervised Multitask Learners,2019-02-14,34d36d45-b4e2-49e1-ae29-b35e4f7df2dd,Text and held-out perplexity has as of yet improved given
Language Models are Unsupervised Multitask Learners,2019-02-14,d2563a6e-36ce-44e2-a682-d8fedc645deb,more training time.
Language Models are Unsupervised Multitask Learners,2019-02-14,be15378f-a570-49d9-9d22-805cc6dc9640,3.1. Language Modeling
Language Models are Unsupervised Multitask Learners,2019-02-14,c38bee31-79d5-4fd4-a279-03dfbb1ebf3c,"As an initial step towards zero-shot task transfer, we are"
Language Models are Unsupervised Multitask Learners,2019-02-14,82869ba8-7d52-4d4a-ac63-4dce5c91ef56,interested in understanding how WebText LM’s perform
Language Models are Unsupervised Multitask Learners,2019-02-14,ed8b905c-2e21-48e5-85ff-d9927bf87e8c,at zero-shot domain transfer on the primary task they are
Language Models are Unsupervised Multitask Learners,2019-02-14,76903762-3ed5-4897-910d-49fee11151a6,trained for – language modeling. Since our model operates
Language Models are Unsupervised Multitask Learners,2019-02-14,a0ef1c74-3f1d-493b-8e37-dd12343365ab,on a byte level and does not require lossy pre-processing
Language Models are Unsupervised Multitask Learners,2019-02-14,8773c858-ac2f-4b6f-ab5a-a4ff8d696f27,"or tokenization, we can evaluate it on any language model"
Language Models are Unsupervised Multitask Learners,2019-02-14,b72e2aa8-b624-42bb-bf3a-ce3d93690641,benchmark. Results on language modeling datasets are
Language Models are Unsupervised Multitask Learners,2019-02-14,8294e5ef-215e-4e8a-9ae4-438e1e3cd7c3,commonly reported in a quantity which is a scaled or ex-
Language Models are Unsupervised Multitask Learners,2019-02-14,2828154f-27e3-4d23-be00-3d3174c105ae,ponentiated version of the average negative log probability
Language Models are Unsupervised Multitask Learners,2019-02-14,8593e25e-58dd-480c-985a-d987180060e3,"per canonical prediction unit - usually a character, a byte, or"
Language Models are Unsupervised Multitask Learners,2019-02-14,62a6dd8d-0a87-40a3-a181-7c1e095028a5,a word. We evaluate the same quantity by computing the
Language Models are Unsupervised Multitask Learners,2019-02-14,53399df2-6d6e-445f-b71e-dd8fb134782d,log-probability of a dataset according to a WebText LM and
Language Models are Unsupervised Multitask Learners,2019-02-14,7d9b6fd3-d79c-421d-9ec5-16ba6ec6a2bc,dividing by the number of canonical units. For many of these
Language Models are Unsupervised Multitask Learners,2019-02-14,4cf02287-fd2b-4e66-9c35-ed78374178b7,"datasets, WebText LMs would be tested signiﬁcantly out-"
Language Models are Unsupervised Multitask Learners,2019-02-14,73af8145-a7e3-4dae-add0-12de8bad9778,"of-distribution, having to predict aggressively standardized"
Language Models are Unsupervised Multitask Learners,2019-02-14,cf1595d2-9ce9-4f88-bd0c-89195f7759e1,"text, tokenization artifacts such as disconnected punctuation"
Language Models are Unsupervised Multitask Learners,2019-02-14,6c399d42-33d9-46af-8558-0cedce520fb0,"and contractions, shufﬂed sentences, and even the string"
Language Models are Unsupervised Multitask Learners,2019-02-14,e63b9383-59b8-4eb2-a424-432443d234c7,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,92e5b678-00c5-4901-8799-9849235bf3d5,LAMBADA LAMBADA CBT-CN CBT-NE WikiText2 PTB enwik8 text8 WikiText103 1BW
Language Models are Unsupervised Multitask Learners,2019-02-14,f23bde30-48b6-4f5c-b548-fdcb5bef4a96,(PPL) (ACC) (ACC) (ACC) (PPL) (PPL) (BPB) (BPC) (PPL) (PPL)
Language Models are Unsupervised Multitask Learners,2019-02-14,2a01a8b7-bf29-4310-af03-741b440d17ef,SOTA 99.8 59.23 85.7 82.3 39.14 46.54 0.99 1.08 18.3 21.8
Language Models are Unsupervised Multitask Learners,2019-02-14,01f712c1-569d-4937-881f-1f992786e12a,117M 35.13 45.99 87.65 83.4 29.41 65.85 1.16 1.17 37.50 75.20
Language Models are Unsupervised Multitask Learners,2019-02-14,af3cc6d8-d4be-4786-96a2-54709ae12b13,345M 15.60 55.48 92.35 87.1 22.76 47.33 1.01 1.06 26.37 55.72
Language Models are Unsupervised Multitask Learners,2019-02-14,a74c212f-3730-4e25-983b-add98e4aafaa,762M 10.87 60.12 93.45 88.0 19.93 40.31 0.97 1.02 22.05 44.575
Language Models are Unsupervised Multitask Learners,2019-02-14,527e16ac-7ac5-43d4-b54c-0c3930edba60,1542M 8.63 63.24 93.30 89.05 18.34 35.76 0.93 0.98 17.48 42.16
Language Models are Unsupervised Multitask Learners,2019-02-14,bbfea576-0fee-470c-bcac-c8029274ad59,Table 3. Zero-shot results on many datasets. No training or ﬁne-tuning was performed for any of these results. PTB and WikiText-2
Language Models are Unsupervised Multitask Learners,2019-02-14,7cfd39ac-0555-4971-98fc-dc7de6ce73cc,"results are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018)"
Language Models are Unsupervised Multitask Learners,2019-02-14,a6dceae4-cf09-4d94-99af-df6fb94e6eec,"and LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019)."
Language Models are Unsupervised Multitask Learners,2019-02-14,1404619e-8102-4ad4-ad9b-6780a2775a1c,<UNK> which is extremely rare in WebText - occurring
Language Models are Unsupervised Multitask Learners,2019-02-14,b0b00523-8f93-434b-93df-d6c49313404e,only 26 times in 40 billion bytes. We report our main re-
Language Models are Unsupervised Multitask Learners,2019-02-14,450f0ae6-fe8e-40b1-a8a6-ae1eed58cb99,sults in Table 3 using invertible de-tokenizers which remove
Language Models are Unsupervised Multitask Learners,2019-02-14,155f7e8a-2094-4b69-8369-e6cdcf4a14cb,as many of these tokenization / pre-processing artifacts as
Language Models are Unsupervised Multitask Learners,2019-02-14,e7741a4b-73cd-4616-aa37-9d595aaa0490,"possible. Since these de-tokenizers are invertible, we can"
Language Models are Unsupervised Multitask Learners,2019-02-14,7be63096-7892-41b0-ac44-9003ed0d6d69,still calculate the log probability of a dataset and they can
Language Models are Unsupervised Multitask Learners,2019-02-14,0ae1158b-c7cb-4a22-b526-cf24836fd14a,be thought of as a simple form of domain adaptation. We
Language Models are Unsupervised Multitask Learners,2019-02-14,5ce58489-5b55-4903-b7fe-df43917c7d32,observe gains of 2.5 to 5 perplexity for GPT-2 with these
Language Models are Unsupervised Multitask Learners,2019-02-14,fc23e5b6-6929-4ace-987f-11948eb5d06e,de-tokenizers.
Language Models are Unsupervised Multitask Learners,2019-02-14,2aedbadb-e6a3-4a5f-a5e6-ddbefa3449de,"WebText LMs transfer well across domains and datasets,"
Language Models are Unsupervised Multitask Learners,2019-02-14,bb0efa64-de03-4ea7-9649-c6a591ed4b21,improving the state of the art on 7 out of the 8 datasets in a
Language Models are Unsupervised Multitask Learners,2019-02-14,126c91ea-7375-43cc-8fd5-ba7f2cc6f570,zero-shot setting. Large improvements are noticed on small
Language Models are Unsupervised Multitask Learners,2019-02-14,7cb9087a-2c5f-4b45-9006-3fa05a5aa2ab,datasets such as Penn Treebank and WikiText-2 which have
Language Models are Unsupervised Multitask Learners,2019-02-14,1295107e-5ff3-422a-a43d-89e36a563ae5,only 1 to 2 million training tokens. Large improvements
Language Models are Unsupervised Multitask Learners,2019-02-14,0723a602-effe-49c2-98a9-fe2e26d91c07,are also noticed on datasets created to measure long-term
Language Models are Unsupervised Multitask Learners,2019-02-14,377b4e14-ae35-4ae2-a991-3a5268925e19,"dependencies like LAMBADA (Paperno et al., 2016) and"
Language Models are Unsupervised Multitask Learners,2019-02-14,9dd3db36-8042-4589-b6f1-cadd875114de,"the Children’s Book Test (Hill et al., 2015). Our model is"
Language Models are Unsupervised Multitask Learners,2019-02-14,05a5c2ec-becc-4656-834e-c0a8c2a840f2,still signiﬁcantly worse than prior work on the One Billion
Language Models are Unsupervised Multitask Learners,2019-02-14,59904063-1873-42b3-ae32-a0cdf05ad8dc,"Word Benchmark (Chelba et al., 2013). This is likely due"
Language Models are Unsupervised Multitask Learners,2019-02-14,4e63d868-c08b-45d4-a79a-ab92356c5022,to a combination of it being both the largest dataset and
Language Models are Unsupervised Multitask Learners,2019-02-14,eb616465-92bb-4a95-8780-4ec7f0f69e0f,having some of the most destructive pre-processing - 1BW’s
Language Models are Unsupervised Multitask Learners,2019-02-14,c813f91d-0198-499c-88e0-d21a24476248,sentence level shufﬂing removes all long-range structure.
Language Models are Unsupervised Multitask Learners,2019-02-14,fe63b4ad-1c01-4c94-8eb9-440efe4ea4e2,3.2. Children’s Book Test
Language Models are Unsupervised Multitask Learners,2019-02-14,4273a5da-01d1-4da5-a88f-923045eec422,Figure 2. Performance on the Children’s Book Test as a function of
Language Models are Unsupervised Multitask Learners,2019-02-14,5fc5e800-4f0a-489e-bceb-065d7c02697f,"model capacity. Human performance are from Bajgar et al. (2016),"
Language Models are Unsupervised Multitask Learners,2019-02-14,2fb689c0-f6d6-4918-970a-84f5bf6f9fc7,"instead of the much lower estimates from the original paper.The Children’s Book Test (CBT) (Hill et al., 2015) was"
Language Models are Unsupervised Multitask Learners,2019-02-14,4fe5ef9b-85e2-4920-8d2c-a8761bda8496,created to examine the performance of LMs on different cat-
Language Models are Unsupervised Multitask Learners,2019-02-14,a5102248-a2c3-4283-840f-f04e29965f1e,"egories of words: named entities, nouns, verbs, and preposi-"
Language Models are Unsupervised Multitask Learners,2019-02-14,02cfd997-bb8b-43a3-8ea0-37d0a54bd839,tions. Rather than reporting perplexity as an evaluation met-
Language Models are Unsupervised Multitask Learners,2019-02-14,129a9c77-c6d3-48d3-83bc-bddb71599bb0,"ric, CBT reports accuracy on an automatically constructed"
Language Models are Unsupervised Multitask Learners,2019-02-14,2a1b65ee-e080-449b-a503-2d28b5b97a5c,cloze test where the task is to predict which of 10 possible
Language Models are Unsupervised Multitask Learners,2019-02-14,5574bf41-1972-45a7-8fd0-b1bc2c988071,choices for an omitted word is correct. Following the LM
Language Models are Unsupervised Multitask Learners,2019-02-14,fee01b98-2ee8-4e7c-b816-da01f70da106,"approach introduced in the original paper, we compute the"
Language Models are Unsupervised Multitask Learners,2019-02-14,2f53cf50-0ed0-4d53-89a1-509bd935d914,probability of each choice and the rest of the sentence con-
Language Models are Unsupervised Multitask Learners,2019-02-14,50604159-0129-413c-a4dd-e462d76136e4,"ditioned on this choice according to the LM, and predict"
Language Models are Unsupervised Multitask Learners,2019-02-14,cc87a618-2853-45fe-b12d-729ac4cf5c79,the one with the highest probability. As seen in Figure 2
Language Models are Unsupervised Multitask Learners,2019-02-14,6e0e0db7-63cf-4ce5-9305-346d705005b0,performance steadily improves as model size is increased
Language Models are Unsupervised Multitask Learners,2019-02-14,c61028fa-6f90-48ea-ab7c-daf03d849dca,and closes the majority of the gap to human performance
Language Models are Unsupervised Multitask Learners,2019-02-14,2b9abde9-8e06-4a3e-a675-454c86dbfc74,on this test. Data overlap analysis showed one of the CBT
Language Models are Unsupervised Multitask Learners,2019-02-14,21259f4f-1adf-4c8e-b10d-6f72a966c855,"test set books, The Jungle Book by Rudyard Kipling, is in"
Language Models are Unsupervised Multitask Learners,2019-02-14,7bca3a96-9ba0-4a3a-ae68-f969ae968d1d,"WebText, so we report results on the validation set which"
Language Models are Unsupervised Multitask Learners,2019-02-14,42be10a0-59cf-49f7-ba55-0386a7cb6b7e,has no signiﬁcant overlap. GPT-2 achieves new state of the
Language Models are Unsupervised Multitask Learners,2019-02-14,93486969-e9a1-485b-91e4-bbc778067d26,art results of 93.3% on common nouns and 89.1% on named
Language Models are Unsupervised Multitask Learners,2019-02-14,29a58c1a-2962-4625-b05b-0c0b24508731,entities. A de-tokenizer was applied to remove PTB style
Language Models are Unsupervised Multitask Learners,2019-02-14,406b215f-6bae-4c35-809e-73479f340abc,tokenization artifacts from CBT.
Language Models are Unsupervised Multitask Learners,2019-02-14,ee6191f5-f0b6-4482-9046-63c34080b237,3.3. LAMBADA
Language Models are Unsupervised Multitask Learners,2019-02-14,c6134bdd-5a3a-4d0e-a5aa-10f9c81ac5ff,"The LAMBADA dataset (Paperno et al., 2016) tests the"
Language Models are Unsupervised Multitask Learners,2019-02-14,5a0cc61c-2f25-408d-ac09-cbd6840ca76c,ability of systems to model long-range dependencies in
Language Models are Unsupervised Multitask Learners,2019-02-14,b3a1c643-0a40-41da-9590-8bd7f733aa27,text. The task is to predict the ﬁnal word of sentences
Language Models are Unsupervised Multitask Learners,2019-02-14,d7713666-81d1-4849-ae72-98ba581d2476,which require at least 50 tokens of context for a human to
Language Models are Unsupervised Multitask Learners,2019-02-14,23bd81e9-7034-455d-9b84-8c09ca1f8e92,successfully predict. GPT-2 improves the state of the art
Language Models are Unsupervised Multitask Learners,2019-02-14,3be970a7-a820-467c-8a53-f775ef7d11ec,"from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases"
Language Models are Unsupervised Multitask Learners,2019-02-14,9f6fa6ca-afba-4f77-9f34-6d04862422cf,"the accuracy of LMs on this test from 19% (Dehghani et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,1a44c398-9a35-417e-beec-9e795557bc2a,2018) to 52.66%. Investigating GPT-2’s errors showed most
Language Models are Unsupervised Multitask Learners,2019-02-14,c8f19055-5116-4ebb-bf3d-2c5a9dd549da,"predictions are valid continuations of the sentence, but are"
Language Models are Unsupervised Multitask Learners,2019-02-14,e0b1739b-2885-4e60-b1d4-9cd4ef43a712,not valid ﬁnal words. This suggests that the LM is not
Language Models are Unsupervised Multitask Learners,2019-02-14,a711493c-16f1-4d32-9fe7-8fc0282cbbf1,using the additional useful constraint that the word must be
Language Models are Unsupervised Multitask Learners,2019-02-14,774291e1-8819-4213-866e-b96adf4b6489,the ﬁnal of the sentence. Adding a stop-word ﬁlter as an
Language Models are Unsupervised Multitask Learners,2019-02-14,782c29ad-161c-4727-9b48-2075be1edf2f,"approximation to this further increases accuracy to 63.24%,"
Language Models are Unsupervised Multitask Learners,2019-02-14,c0d52233-90b4-4958-b34e-4fb639f94987,improving the overall state of the art on this task by 4%. The
Language Models are Unsupervised Multitask Learners,2019-02-14,a20bd68c-75fc-42a8-8985-80e0541139be,"previous state of the art (Hoang et al., 2018) used a different"
Language Models are Unsupervised Multitask Learners,2019-02-14,f5d57b95-50c3-49f1-a19f-b682410e04c1,restricted prediction setting where the outputs of the model
Language Models are Unsupervised Multitask Learners,2019-02-14,8c1d2f6e-328a-4f84-8f8c-5cdbb9749b50,were constrained to only words that appeared in the context.
Language Models are Unsupervised Multitask Learners,2019-02-14,ce050d27-9fde-4c10-b3d6-c63ba6767125,"For GPT-2, this restriction is harmful rather than helpful"
Language Models are Unsupervised Multitask Learners,2019-02-14,adaa6b0a-a802-4aaf-af32-522bfc49e3df,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,cda38eaf-6e44-4ba7-993a-e712e22f268c,since 19% of answers are not in context. We use a version
Language Models are Unsupervised Multitask Learners,2019-02-14,a704c52c-1ab5-4bff-8d39-18931bc5dcf0,of the dataset without preprocessing.
Language Models are Unsupervised Multitask Learners,2019-02-14,3fa60d71-8458-4193-8557-b9be567e2166,3.4. Winograd Schema Challenge
Language Models are Unsupervised Multitask Learners,2019-02-14,65bfde4c-9a19-4946-8f8e-33d87997b4b4,Figure 3. Performance on the Winograd Schema Challenge as a
Language Models are Unsupervised Multitask Learners,2019-02-14,13c2fc4e-c804-47b2-bd41-f72cb7674f24,function of model capacity.
Language Models are Unsupervised Multitask Learners,2019-02-14,eef5c11f-c643-4106-8467-fc16d2713a3f,"The Winograd Schema challenge (Levesque et al., 2012)"
Language Models are Unsupervised Multitask Learners,2019-02-14,796b507d-6c83-4c55-8108-10e4b81cc9e1,was constructed to measure the capability of a system to
Language Models are Unsupervised Multitask Learners,2019-02-14,1a46e08d-b5f2-469c-8b63-82a15ca4984f,perform commonsense reasoning by measuring its ability
Language Models are Unsupervised Multitask Learners,2019-02-14,fa7b4afc-5831-4c67-9ed2-012ccc02e595,to resolve ambiguities in text. Recently Trinh & Le (2018)
Language Models are Unsupervised Multitask Learners,2019-02-14,e46df0f9-ed07-40ce-99ba-983099f3adc7,demonstrated signiﬁcant progress on this challenge using
Language Models are Unsupervised Multitask Learners,2019-02-14,e0545f3a-81d8-446d-ae9f-a510cc170566,"LMs, by predicting the resolution of the ambiguity with"
Language Models are Unsupervised Multitask Learners,2019-02-14,6b5ea5d5-7d6d-4c7b-90b3-db3d6f9cbc8b,higher probability. We follow their problem formulation and
Language Models are Unsupervised Multitask Learners,2019-02-14,d0e55144-b21c-4449-92e8-a4c0b174d1fe,visualize the performance of our models with both full and
Language Models are Unsupervised Multitask Learners,2019-02-14,177513d1-de48-443c-bc73-cc104207d250,partial scoring techniques in Figure 3. GPT-2 improves state
Language Models are Unsupervised Multitask Learners,2019-02-14,ca0ec8ca-b20e-4d83-b56f-423a2fef4346,"of the art accuracy by 7%, achieving 70.70%. The dataset"
Language Models are Unsupervised Multitask Learners,2019-02-14,cc1a4ccf-b01a-458f-976d-677ada7a0b68,is quite small with only 273 examples so we recommend
Language Models are Unsupervised Multitask Learners,2019-02-14,bc58aa41-107a-433a-82b9-1f77a8842c2b,reading Trichelair et al. (2018) to help contextualize this
Language Models are Unsupervised Multitask Learners,2019-02-14,b16d78d3-a1ce-4e81-b6cf-20f0a3fda920,result.
Language Models are Unsupervised Multitask Learners,2019-02-14,8a5dbc25-3927-4059-a05c-2260e75ba1af,3.5. Reading Comprehension
Language Models are Unsupervised Multitask Learners,2019-02-14,ee5780f7-597e-451e-ba1b-1fdb007de9e5,The Conversation Question Answering dataset (CoQA)
Language Models are Unsupervised Multitask Learners,2019-02-14,f055e1ab-9567-4b8c-afa1-0338535f25ee,Reddy et al. (2018) consists of documents from 7 different
Language Models are Unsupervised Multitask Learners,2019-02-14,1ab5b780-f054-48ed-94ed-0b7bbbf675fd,domains paired with natural language dialogues between a
Language Models are Unsupervised Multitask Learners,2019-02-14,f057f6d8-5350-4079-9674-a8e750762a70,question asker and a question answerer about the document.
Language Models are Unsupervised Multitask Learners,2019-02-14,6cec7770-1cb1-4e34-be1d-a3367d64ca27,CoQA tests reading comprehension capabilities and also
Language Models are Unsupervised Multitask Learners,2019-02-14,b5df9ab8-5a9e-4b11-9a26-015d1f4b9db9,the ability of models to answer questions that depend on
Language Models are Unsupervised Multitask Learners,2019-02-14,21344b99-d018-4d67-8c04-9ca5ffca5e43,conversation history (such as “Why?”).
Language Models are Unsupervised Multitask Learners,2019-02-14,83f5641b-cde1-4483-8ec2-fb5cf0726888,Greedy decoding from GPT-2 when conditioned on a doc-
Language Models are Unsupervised Multitask Learners,2019-02-14,b66ea24d-9896-43d8-b903-272439f2f1ea,"ument, the history of the associated conversation, and a"
Language Models are Unsupervised Multitask Learners,2019-02-14,3a4cc651-e330-4195-92b3-0e81bf06e337,ﬁnal token A:achieves 55 F1 on the development set. This
Language Models are Unsupervised Multitask Learners,2019-02-14,fbe47780-8e0a-45da-ad93-09d4f72653d7,matches or exceeds the performance of 3 out of 4 base-
Language Models are Unsupervised Multitask Learners,2019-02-14,0e4764d9-64b9-40fd-9fce-5edbd42bfab6,"line systems without using the 127,000+ manually collected"
Language Models are Unsupervised Multitask Learners,2019-02-14,38b0e562-23ff-4b4f-bd17-31c55a1eabf9,question answer pairs those baselines were trained on. The
Language Models are Unsupervised Multitask Learners,2019-02-14,b76f2e2e-ba2a-47e6-81ef-93ed6648348c,"supervised SOTA, a BERT based system (Devlin et al.,R-1 R-2 R-L R-A VG"
Language Models are Unsupervised Multitask Learners,2019-02-14,0f4e30a0-b62d-4000-9260-3050e4e832c1,Bottom-Up Sum 41.22 18.68 38.34 32.75
Language Models are Unsupervised Multitask Learners,2019-02-14,f16d2fe2-684f-4496-a1b5-e74832d03acd,Lede-3 40.38 17.66 36.62 31.55
Language Models are Unsupervised Multitask Learners,2019-02-14,be6e7cf0-02be-469d-8da5-09c945dc7156,Seq2Seq + Attn 31.33 11.81 28.83 23.99
Language Models are Unsupervised Multitask Learners,2019-02-14,78e34b64-1e8d-4b16-a3c1-d0f4477b90fa,GPT-2 TL;DR: 29.34 8.27 26.58 21.40
Language Models are Unsupervised Multitask Learners,2019-02-14,b83e6e59-bb52-48d7-b253-8d79341ecd70,Random-3 28.78 8.63 25.52 20.98
Language Models are Unsupervised Multitask Learners,2019-02-14,9b3e89e5-e55f-4f85-ab19-848b10b9188f,GPT-2 no hint 21.58 4.03 19.47 15.03
Language Models are Unsupervised Multitask Learners,2019-02-14,ebe97dcf-402e-46f4-829c-ea86f56ba82d,Table 4. Summarization performance as measured by ROUGE F1
Language Models are Unsupervised Multitask Learners,2019-02-14,cf5e9549-5671-4f29-91e3-e28e502a5fe4,metrics on the CNN and Daily Mail dataset. Bottom-Up Sum is
Language Models are Unsupervised Multitask Learners,2019-02-14,11e7c60f-7d57-465d-8f2b-b37e735bb16b,"the SOTA model from (Gehrmann et al., 2018)"
Language Models are Unsupervised Multitask Learners,2019-02-14,3360ef31-3b75-4b41-9a09-12084a2c1462,"2018), is nearing the 89 F1 performance of humans. While"
Language Models are Unsupervised Multitask Learners,2019-02-14,959ae7a1-2565-42b5-8bb3-e15bb9084b28,GPT-2’s performance is exciting for a system without any su-
Language Models are Unsupervised Multitask Learners,2019-02-14,1c54d837-fa74-4a5a-8322-9636f515f038,"pervised training, some inspection of its answers and errors"
Language Models are Unsupervised Multitask Learners,2019-02-14,21091085-6bd8-4672-8ca2-3ebd44d75ea3,suggests GPT-2 often uses simple retrieval based heuristics
Language Models are Unsupervised Multitask Learners,2019-02-14,f198be68-8e54-4d4c-80fa-62eb8480550e,such as answer with a name from the document in response
Language Models are Unsupervised Multitask Learners,2019-02-14,51c901cf-6f79-412f-8746-81569d5c744f,to a who question .
Language Models are Unsupervised Multitask Learners,2019-02-14,a822d392-28a5-4ff0-bd81-0b294ac1a0f2,3.6. Summarization
Language Models are Unsupervised Multitask Learners,2019-02-14,fb13497b-1f2d-43ed-9455-76f655403c53,We test GPT-2’s ability to perform summarization on the
Language Models are Unsupervised Multitask Learners,2019-02-14,fbb915cb-0264-4222-a71b-49a6e7d9bc0c,"CNN and Daily Mail dataset (Nallapati et al., 2016). To in-"
Language Models are Unsupervised Multitask Learners,2019-02-14,f21dcc45-25d8-4cde-a070-479b6a343a1e,duce summarization behavior we add the text TL;DR: after
Language Models are Unsupervised Multitask Learners,2019-02-14,6459ab0e-be62-43d1-806a-e838ce30a806,the article and generate 100 tokens with Top- krandom sam-
Language Models are Unsupervised Multitask Learners,2019-02-14,3cb9b53c-e252-452b-a340-0a7ba41932f4,"pling (Fan et al., 2018) with k= 2which reduces repetition"
Language Models are Unsupervised Multitask Learners,2019-02-14,d7dadc9e-6f8b-4481-87a8-59d78adf5c54,and encourages more abstractive summaries than greedy de-
Language Models are Unsupervised Multitask Learners,2019-02-14,b7b7b5bc-012c-44f7-9b54-155a7963162e,coding. We use the ﬁrst 3 generated sentences in these 100
Language Models are Unsupervised Multitask Learners,2019-02-14,95628ae3-9777-42b8-8aed-2c6ea9598030,tokens as the summary. While qualitatively the generations
Language Models are Unsupervised Multitask Learners,2019-02-14,c03b191a-e785-40ca-8daf-9f8b07d287ac,"resemble summaries, as shown in Table 14, they often focus"
Language Models are Unsupervised Multitask Learners,2019-02-14,d90fe5fa-507c-4154-b4a9-c394765e3a31,on recent content from the article or confuse speciﬁc details
Language Models are Unsupervised Multitask Learners,2019-02-14,5275dadc-d012-40a7-9281-39b632035ed9,such as how many cars were involved in a crash or whether
Language Models are Unsupervised Multitask Learners,2019-02-14,12e2b5d7-ee78-4b82-a27d-7dde4b3e7285,a logo was on a hat or shirt. On the commonly reported
Language Models are Unsupervised Multitask Learners,2019-02-14,bdc32379-d28d-416f-9968-4aaa4ea9c42d,"ROUGE 1,2,L metrics the generated summaries only begin"
Language Models are Unsupervised Multitask Learners,2019-02-14,76387461-2414-4b71-a4ee-f81014608b15,to approach the performance of classic neural baselines and
Language Models are Unsupervised Multitask Learners,2019-02-14,3fc76c03-551d-4874-8955-62bc266854de,just barely outperforms selecting 3 random sentences from
Language Models are Unsupervised Multitask Learners,2019-02-14,4002467b-e1fe-4724-bc52-576ae8cd3982,the article. GPT-2’s performance drops by 6.4 points on
Language Models are Unsupervised Multitask Learners,2019-02-14,9c755377-3c0c-438c-a8f8-0498833ded67,the aggregate metric when the task hint is removed which
Language Models are Unsupervised Multitask Learners,2019-02-14,c971a5cc-2840-40ee-8428-4af8066c4f80,demonstrates the ability to invoke task speciﬁc behavior in
Language Models are Unsupervised Multitask Learners,2019-02-14,e05e88a8-04f5-43a6-bf8b-e8d52d48becc,a language model with natural language.
Language Models are Unsupervised Multitask Learners,2019-02-14,c69a6031-d15b-4265-9631-d0780bf31dea,3.7. Translation
Language Models are Unsupervised Multitask Learners,2019-02-14,4be80025-2ba5-4107-9392-26d3a25abade,We test whether GPT-2 has begun to learn how to translate
Language Models are Unsupervised Multitask Learners,2019-02-14,3bdba750-9dab-4d31-98ef-b8699a900a23,from one language to another. In order to help it infer that
Language Models are Unsupervised Multitask Learners,2019-02-14,7a0401ba-7b38-401a-876a-5fcf7e712e95,"this is the desired task, we condition the language model"
Language Models are Unsupervised Multitask Learners,2019-02-14,742e8696-ecd6-46aa-aa9a-21e57725768f,on a context of example pairs of the format english
Language Models are Unsupervised Multitask Learners,2019-02-14,5d3216f2-b8e6-4d10-9ede-8accbf5ee47a,sentence = french sentence and then after a ﬁ-
Language Models are Unsupervised Multitask Learners,2019-02-14,fabf0f3e-19fd-491d-9d17-dd28ac0f22a3,nal prompt of english sentence = we sample from
Language Models are Unsupervised Multitask Learners,2019-02-14,be6c5e42-7b1e-4e8d-90ed-a57bb9f29ece,the model with greedy decoding and use the ﬁrst generated
Language Models are Unsupervised Multitask Learners,2019-02-14,8a3034ad-b13e-4136-aaa3-20e2ec226ab7,sentence as the translation. On the WMT-14 English-French
Language Models are Unsupervised Multitask Learners,2019-02-14,8fef4add-4d49-464c-8b2b-a610786356ff,"test set, GPT-2 gets 5 BLEU, which is slightly worse than"
Language Models are Unsupervised Multitask Learners,2019-02-14,414b0869-3bf3-4c4f-813b-f1316d3a1b40,a word-by-word substitution with a bilingual lexicon in-
Language Models are Unsupervised Multitask Learners,2019-02-14,6005f96a-efe2-4911-8240-040bfafc15df,ferred in previous work on unsupervised word translation
Language Models are Unsupervised Multitask Learners,2019-02-14,4f6757d0-974d-4e39-b8e5-2a5eb1bf6aca,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,77e75e77-1fdf-4af7-a4eb-514ccc932446,Question Generated Answer Correct Probability
Language Models are Unsupervised Multitask Learners,2019-02-14,a19fe85f-6f2e-407d-be57-3b51c088a415,Who wrote the book the origin of species? Charles Darwin 3 83.4%
Language Models are Unsupervised Multitask Learners,2019-02-14,ffb077e0-4ab1-486f-bcdd-431db49cc008,Who is the founder of the ubuntu project? Mark Shuttleworth 3 82.0%
Language Models are Unsupervised Multitask Learners,2019-02-14,5a767049-ef77-4ad6-8a9e-9012d0109d7e,Who is the quarterback for the green bay packers? Aaron Rodgers 3 81.1%
Language Models are Unsupervised Multitask Learners,2019-02-14,19264190-e695-4b44-b928-cd27107d89cd,Panda is a national animal of which country? China 3 76.8%
Language Models are Unsupervised Multitask Learners,2019-02-14,7c898041-a7fb-408d-8fe5-97b1fa2c848b,Who came up with the theory of relativity? Albert Einstein 3 76.4%
Language Models are Unsupervised Multitask Learners,2019-02-14,02e7f45f-d473-4cf7-8da5-597d047242ed,When was the ﬁrst star wars ﬁlm released? 1977 3 71.4%
Language Models are Unsupervised Multitask Learners,2019-02-14,d0a8c3f8-b68b-48b8-a505-153439d98da5,What is the most common blood type in sweden? A 7 70.6%
Language Models are Unsupervised Multitask Learners,2019-02-14,20f5139e-d40e-4d46-8196-b5984309847f,Who is regarded as the founder of psychoanalysis? Sigmund Freud 3 69.3%
Language Models are Unsupervised Multitask Learners,2019-02-14,75e78358-e636-4b1b-b5ba-84813c4cb22e,Who took the ﬁrst steps on the moon in 1969? Neil Armstrong 3 66.8%
Language Models are Unsupervised Multitask Learners,2019-02-14,13acec44-8343-4c78-8bc5-ccca9003ac84,Who is the largest supermarket chain in the uk? Tesco 3 65.3%
Language Models are Unsupervised Multitask Learners,2019-02-14,5baefcc9-81a4-4a5d-9f84-0142a0637973,What is the meaning of shalom in english? peace 3 64.0%
Language Models are Unsupervised Multitask Learners,2019-02-14,20194934-9a8f-49ae-93ca-5e25b0ccef62,Who was the author of the art of war? Sun Tzu 3 59.6%
Language Models are Unsupervised Multitask Learners,2019-02-14,cb8828b3-9389-49ec-bdf2-3be764842e94,Largest state in the us by land mass? California 7 59.2%
Language Models are Unsupervised Multitask Learners,2019-02-14,a54c7b18-6ee6-4ebe-b934-c1c74e48e065,Green algae is an example of which type of reproduction? parthenogenesis 7 56.5%
Language Models are Unsupervised Multitask Learners,2019-02-14,e802f6d5-0293-4a1a-9bec-21fbe117846f,Vikram samvat calender is ofﬁcial in which country? India 3 55.6%
Language Models are Unsupervised Multitask Learners,2019-02-14,d2c8d9d7-a337-4144-a4c0-029a5db0d155,Who is mostly responsible for writing the declaration of independence? Thomas Jefferson 3 53.3%
Language Models are Unsupervised Multitask Learners,2019-02-14,b8c7e898-023d-46c2-a8b2-95e5d4c09398,What us state forms the western boundary of montana? Montana 7 52.3%
Language Models are Unsupervised Multitask Learners,2019-02-14,54dbacd4-c495-4bdd-9765-8ac93f3b8236,Who plays ser davos in game of thrones? Peter Dinklage 7 52.1%
Language Models are Unsupervised Multitask Learners,2019-02-14,3a782844-d101-475f-955d-ebc3d0625e48,Who appoints the chair of the federal reserve system? Janet Yellen 7 51.5%
Language Models are Unsupervised Multitask Learners,2019-02-14,d6ef240b-9062-44be-a37c-62727c24572d,State the process that divides one nucleus into two genetically identical nuclei? mitosis 3 50.7%
Language Models are Unsupervised Multitask Learners,2019-02-14,9f23a737-6b41-4205-b494-1e807c43de36,Who won the most mvp awards in the nba? Michael Jordan 7 50.2%
Language Models are Unsupervised Multitask Learners,2019-02-14,98ae87f8-8471-4a24-a3b2-bc995e47c124,What river is associated with the city of rome? the Tiber 3 48.6%
Language Models are Unsupervised Multitask Learners,2019-02-14,c3cebf7a-319a-42ed-9b1c-ca6d0597cce2,Who is the ﬁrst president to be impeached? Andrew Johnson 3 48.3%
Language Models are Unsupervised Multitask Learners,2019-02-14,ed13f128-d886-484b-b123-77d750ffb8c6,Who is the head of the department of homeland security 2017? John Kelly 3 47.0%
Language Models are Unsupervised Multitask Learners,2019-02-14,418adf14-0d11-43c7-9ec0-8155c44c2290,What is the name given to the common currency to the european union? Euro 3 46.8%
Language Models are Unsupervised Multitask Learners,2019-02-14,94160cba-d706-46e1-a59e-31b151dd5305,What was the emperor name in star wars? Palpatine 3 46.5%
Language Models are Unsupervised Multitask Learners,2019-02-14,a1f71b0a-42aa-46c0-9323-2fafebb92501,Do you have to have a gun permit to shoot at a range? No 3 46.4%
Language Models are Unsupervised Multitask Learners,2019-02-14,1271e1d5-1261-4bc6-b68d-764e2e2b3658,Who proposed evolution in 1859 as the basis of biological development? Charles Darwin 3 45.7%
Language Models are Unsupervised Multitask Learners,2019-02-14,7894685a-3f1c-4298-89de-e80fc9c1e000,Nuclear power plant that blew up in russia? Chernobyl 3 45.7%
Language Models are Unsupervised Multitask Learners,2019-02-14,c405292e-b66b-4759-ac5c-dc6089bdadd0,Who played john connor in the original terminator? Arnold Schwarzenegger 7 45.2%
Language Models are Unsupervised Multitask Learners,2019-02-14,d2f21c2f-afb4-4bb7-80ac-3607dd63ed6b,Table 5. The 30 most conﬁdent answers generated by GPT-2 on the development set of Natural Questions sorted by their probability
Language Models are Unsupervised Multitask Learners,2019-02-14,68ed302a-3288-483b-9ae6-de10de52a136,according to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4.
Language Models are Unsupervised Multitask Learners,2019-02-14,4a3c4299-9dfb-454d-b344-bf38199fa7e8,"(Conneau et al., 2017b). On the WMT-14 French-English"
Language Models are Unsupervised Multitask Learners,2019-02-14,69537cf9-2c3d-4ae8-8537-d8a4d42e6733,"test set, GPT-2 is able to leverage its very strong English"
Language Models are Unsupervised Multitask Learners,2019-02-14,2e3ec3ed-7208-4b6e-81bf-1cc05536e9e4,"language model to perform signiﬁcantly better, achieving"
Language Models are Unsupervised Multitask Learners,2019-02-14,867f817d-eb3c-46f9-a59e-b27b56ea4727,11.5 BLEU. This outperforms several unsupervised machine
Language Models are Unsupervised Multitask Learners,2019-02-14,c097e0eb-91a1-4991-812d-69fbb8c40a6e,"translation baselines from (Artetxe et al., 2017) and (Lample"
Language Models are Unsupervised Multitask Learners,2019-02-14,beebcfe6-a646-499e-9ada-db18e7b380d0,"et al., 2017) but is still much worse than the 33.5 BLEU of"
Language Models are Unsupervised Multitask Learners,2019-02-14,c3d6ad2f-4251-4839-a568-71b180675aed,the current best unsupervised machine translation approach
Language Models are Unsupervised Multitask Learners,2019-02-14,3987ec38-f078-41fd-8add-a4dd6f62a264,"(Artetxe et al., 2019). Performance on this task was sur-"
Language Models are Unsupervised Multitask Learners,2019-02-14,0cf30d6c-7f79-4e3c-9932-2455e4ee2aa8,"prising to us, since we deliberately removed non-English"
Language Models are Unsupervised Multitask Learners,2019-02-14,9cff82ea-3ac1-4ed3-8232-7489236a8a67,webpages from WebText as a ﬁltering step. In order to con-
Language Models are Unsupervised Multitask Learners,2019-02-14,56447d7c-2491-41bf-940b-be50f42b22d9,"ﬁrm this, we ran a byte-level language detector2on WebText"
Language Models are Unsupervised Multitask Learners,2019-02-14,5df00b02-bc55-4b3f-8712-6daee62f5d99,which detected only 10MB of data in the French language
Language Models are Unsupervised Multitask Learners,2019-02-14,9dc205e0-64f9-429f-ba59-dfb60904b7a2,which is approximately 500x smaller than the monolingual
Language Models are Unsupervised Multitask Learners,2019-02-14,e270393e-0d50-41ec-97d4-f7231da0883c,French corpus common in prior unsupervised machine trans-
Language Models are Unsupervised Multitask Learners,2019-02-14,4ac82d6d-b31e-4cb9-933f-2f6504b6b3b1,lation research.
Language Models are Unsupervised Multitask Learners,2019-02-14,96ba42c5-fdd7-4b3e-b1ab-b0fd0da3c002,3.8. Question Answering
Language Models are Unsupervised Multitask Learners,2019-02-14,5f284982-73ac-4c74-887c-d836809b6c57,A potential way to test what information is contained within
Language Models are Unsupervised Multitask Learners,2019-02-14,03107978-139b-444c-98f2-30dcb6cbc029,a language model is to evaluate how often it generates the
Language Models are Unsupervised Multitask Learners,2019-02-14,d3c918a9-f84a-451b-be4c-aa515ad473e2,correct answer to factoid-style questions. Previous showcas-
Language Models are Unsupervised Multitask Learners,2019-02-14,928ef99e-3daf-4614-afd9-5bb8b64229fd,ing of this behavior in neural systems where all information
Language Models are Unsupervised Multitask Learners,2019-02-14,0f68c3e4-6662-42dc-8021-465946cbc442,is stored in parameters such as A Neural Conversational
Language Models are Unsupervised Multitask Learners,2019-02-14,65cef789-5902-4831-8eb5-81cd7d4e39c5,"Model (Vinyals & Le, 2015) reported qualitative results due"
Language Models are Unsupervised Multitask Learners,2019-02-14,be176371-d4dd-4a5b-869e-a5a1bb3345fb,to the lack of high-quality evaluation datasets. The recently
Language Models are Unsupervised Multitask Learners,2019-02-14,ad928d01-baa2-4a56-9cb8-e4cd3f2eb3f2,"introduced Natural Questions dataset (Kwiatkowski et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,7a4a3232-4ec8-4162-aca8-987523a7cfee,2https://github.com/CLD2Owners/cld22019) is a promising resource to test this more quantita-
Language Models are Unsupervised Multitask Learners,2019-02-14,6fcab412-f0dd-479e-8142-8295fce26138,"tively. Similar to translation, the context of the language"
Language Models are Unsupervised Multitask Learners,2019-02-14,d364d9ef-cfa8-48af-aa23-de006faa9fda,model is seeded with example question answer pairs which
Language Models are Unsupervised Multitask Learners,2019-02-14,eeb49ff5-f50d-41a5-83c8-5b75a23d861e,helps the model infer the short answer style of the dataset.
Language Models are Unsupervised Multitask Learners,2019-02-14,3f7fcfaf-7693-46f8-bf30-7d06d32b3431,GPT-2 answers 4.1% of questions correctly when evalu-
Language Models are Unsupervised Multitask Learners,2019-02-14,6fc624eb-574c-49fd-b649-8239e454cf09,ated by the exact match metric commonly used on reading
Language Models are Unsupervised Multitask Learners,2019-02-14,b5a3a50e-5c70-4244-976b-d547120b2251,comprehension datasets like SQUAD.3As a comparison
Language Models are Unsupervised Multitask Learners,2019-02-14,4ee86e75-7ebc-49d1-83e9-7dcf5e3df820,"point, the smallest model does not exceed the 1.0% accu-"
Language Models are Unsupervised Multitask Learners,2019-02-14,8ba1f207-811a-4a1d-a054-5b36feb9b6a7,racy of an incredibly simple baseline which returns the most
Language Models are Unsupervised Multitask Learners,2019-02-14,7c9eaf9a-49cc-476e-99a2-5bca606c287f,"common answer for each question type (who, what, where,"
Language Models are Unsupervised Multitask Learners,2019-02-14,d839f629-d526-497c-8140-981a421587fd,"etc...). GPT-2 answers 5.3 times more questions correctly,"
Language Models are Unsupervised Multitask Learners,2019-02-14,501124e9-4f12-4368-b340-6b80365e427f,suggesting that model capacity has been a major factor in
Language Models are Unsupervised Multitask Learners,2019-02-14,47fa0b08-18b8-4096-9bd8-4864c3a119d4,the poor performance of neural systems on this kind of task
Language Models are Unsupervised Multitask Learners,2019-02-14,cfa2db1d-f082-4fca-bdcc-efa53c260de7,as of yet. The probability GPT-2 assigns to its generated
Language Models are Unsupervised Multitask Learners,2019-02-14,f1f06333-7a76-434f-9e08-d7ecccdcc560,answers is well calibrated and GPT-2 has an accuracy of
Language Models are Unsupervised Multitask Learners,2019-02-14,ee1aa5ca-e652-4488-b0a1-e56e2c6f2937,63.1% on the 1% of questions it is most conﬁdent in. The
Language Models are Unsupervised Multitask Learners,2019-02-14,963b3794-252c-4dab-b41a-6327f9d08b42,30 most conﬁdent answers generated by GPT-2 on develop-
Language Models are Unsupervised Multitask Learners,2019-02-14,2b4ebe9b-06d3-4232-8e9a-707e7a515c8d,ment set questions are shown in Table 5. The performance
Language Models are Unsupervised Multitask Learners,2019-02-14,b46961b0-6ab1-4d88-b009-16af442fce76,"of GPT-2 is still much, much, worse than the 30 to 50%"
Language Models are Unsupervised Multitask Learners,2019-02-14,77213b0b-a3de-42af-911c-7fc459d0d035,range of open domain question answering systems which
Language Models are Unsupervised Multitask Learners,2019-02-14,fb7f7001-92e0-44ad-af77-8e1bc0d15683,hybridize information retrieval with extractive document
Language Models are Unsupervised Multitask Learners,2019-02-14,e15a6c5a-e91c-4868-a973-e32509fa68e6,"question answering (Alberti et al., 2019)."
Language Models are Unsupervised Multitask Learners,2019-02-14,0f7a05a1-c996-40e5-ae09-5280c8dce0af,"3Alec, who previously thought of himself as good at random"
Language Models are Unsupervised Multitask Learners,2019-02-14,5a099c75-0f81-4640-a40e-e1134a8c2434,"trivia, answered 17 of 100 randomly sampled examples correctly"
Language Models are Unsupervised Multitask Learners,2019-02-14,ab6be931-a9f8-47a6-bc23-c8f23a7e3534,when tested in the same setting as GPT-2. He actually only got 14 right but he
Language Models are Unsupervised Multitask Learners,2019-02-14,d6bdbcda-5460-4ba2-9d72-d56e3c24eaf7,should have gotten those other 3
Language Models are Unsupervised Multitask Learners,2019-02-14,bfff2f35-2b77-4064-bd4f-e8b5ef9431f3,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,ea2b5095-ea51-4754-a43c-9bb48eea6b17,PTB WikiText-2 enwik8 text8 Wikitext-103 1BW
Language Models are Unsupervised Multitask Learners,2019-02-14,147206d2-ebc4-4b96-aeb8-4331834d3e40,Dataset train 2.67% 0.66% 7.50% 2.34% 9.09% 13.19%
Language Models are Unsupervised Multitask Learners,2019-02-14,8d78ae81-0b9c-4d00-bd84-144af34ef4e3,WebText train 0.88% 1.63% 6.31% 3.94% 2.42% 3.75%
Language Models are Unsupervised Multitask Learners,2019-02-14,df9d0dd2-37d0-4e5a-82da-9ac7600bf72b,Table 6. Percentage of test set 8 grams overlapping with training sets.
Language Models are Unsupervised Multitask Learners,2019-02-14,23d2d576-ba51-45e2-9342-6f113e64aad6,4. Generalization vs Memorization
Language Models are Unsupervised Multitask Learners,2019-02-14,0d7bab4c-13ef-46ca-86ec-282df8c844a7,Recent work in computer vision has shown that common im-
Language Models are Unsupervised Multitask Learners,2019-02-14,bbbb0b9a-e398-41ae-9aca-28a2e41f5894,age datasets contain a non-trivial amount of near-duplicate
Language Models are Unsupervised Multitask Learners,2019-02-14,d27ac41d-efed-49f2-88a3-a1c5d4e713d4,images. For instance CIFAR-10 has 3.3% overlap between
Language Models are Unsupervised Multitask Learners,2019-02-14,1f1c52d9-1005-46c4-aa6a-4d4cb30ffa76,"train and test images (Barz & Denzler, 2019). This results in"
Language Models are Unsupervised Multitask Learners,2019-02-14,397fdc31-4a1e-4661-a709-c6605e4b1d75,an over-reporting of the generalization performance of ma-
Language Models are Unsupervised Multitask Learners,2019-02-14,0517ac70-d0ab-4b69-9039-441e2867f6e9,chine learning systems. As the size of datasets increases this
Language Models are Unsupervised Multitask Learners,2019-02-14,465e2396-f312-406b-99b4-d65a0b8671a1,issue becomes increasingly likely which suggests a similar
Language Models are Unsupervised Multitask Learners,2019-02-14,9b00b257-788c-4a52-9bb3-d5827c6eb60c,phenomena could be happening with WebText. Therefore it
Language Models are Unsupervised Multitask Learners,2019-02-14,c4c195fa-97ab-4e37-a123-14b4d0fc68ac,is important to analyze how much test data also shows up in
Language Models are Unsupervised Multitask Learners,2019-02-14,acb3f915-67bc-4cab-be27-86806c39a676,the training data.
Language Models are Unsupervised Multitask Learners,2019-02-14,57c55461-fed1-4d3f-aa2e-68400ceebef0,To study this we created Bloom ﬁlters containing 8-grams
Language Models are Unsupervised Multitask Learners,2019-02-14,c479ec09-1d94-4283-95ef-e4ea82628550,"of WebText training set tokens. To improve recall, strings"
Language Models are Unsupervised Multitask Learners,2019-02-14,7c618ccb-eee2-4324-8f12-7a809adcece8,were normalized to contain only lower-cased alphanumeric
Language Models are Unsupervised Multitask Learners,2019-02-14,92145f09-543f-4d88-bdc8-a2df3060b9b5,words with a single space as a delimiter. The Bloom ﬁlters
Language Models are Unsupervised Multitask Learners,2019-02-14,6a463e9e-9311-4faf-83c7-72aae84ae9f5,were constructed such that the false positive rate is upper
Language Models are Unsupervised Multitask Learners,2019-02-14,0d1a30c2-e4ff-43da-b888-bd3ce1a4f759,bounded by1
Language Models are Unsupervised Multitask Learners,2019-02-14,f26965f5-f399-4b5e-93c4-1c6a6126fb21,108. We further veriﬁed the low false positive
Language Models are Unsupervised Multitask Learners,2019-02-14,9488b456-0759-4d99-8421-16b8f180f132,"rate by generating 1M strings, of which zero were found by"
Language Models are Unsupervised Multitask Learners,2019-02-14,e7ac8459-ff8e-49c0-a622-a41ebeb8195e,the ﬁlter.
Language Models are Unsupervised Multitask Learners,2019-02-14,7b6bdf2d-1637-4341-9a01-02b494971925,"These Bloom ﬁlters let us calculate, given a dataset, the"
Language Models are Unsupervised Multitask Learners,2019-02-14,7cfb07a6-fe6a-4e99-855f-6a9e011e573b,percentage of 8-grams from that dataset that are also found
Language Models are Unsupervised Multitask Learners,2019-02-14,2d2378b9-db8f-405b-ad69-40c1aa26878d,in the WebText training set. Table 6 shows this overlap anal-
Language Models are Unsupervised Multitask Learners,2019-02-14,44e40c71-089d-48ea-ad90-4b37a6be0792,ysis for the test sets of common LM benchmarks. Common
Language Models are Unsupervised Multitask Learners,2019-02-14,6d09c7ce-f964-4422-9952-a5485cdfbf2d,LM datasets’ test sets have between 1-6% overlap with Web-
Language Models are Unsupervised Multitask Learners,2019-02-14,20fa81d7-1a6a-45e0-bb7a-816b2bc59fd7,"Text train, with an average of overlap of 3.2%. Somewhat"
Language Models are Unsupervised Multitask Learners,2019-02-14,035e5198-995b-4eaa-8d5b-f8ec667c9f8a,"surprisingly, many datasets have larger overlaps with their"
Language Models are Unsupervised Multitask Learners,2019-02-14,1135965b-6ffd-4c0c-8d7c-9c0fefe0c329,"own training splits, with an average of 5.9% overlap."
Language Models are Unsupervised Multitask Learners,2019-02-14,f2e61a4c-dea7-4db6-ada4-3743814496d4,"Our approach optimizes for recall, and while manual inspec-"
Language Models are Unsupervised Multitask Learners,2019-02-14,abd07a5a-b9e8-4531-899c-7f92b0bdd8cd,"tion of the overlaps shows many common phrases, there are"
Language Models are Unsupervised Multitask Learners,2019-02-14,1bf86ea7-1c4e-48a9-b3de-e202c630dca3,many longer matches that are due to duplicated data. This is
Language Models are Unsupervised Multitask Learners,2019-02-14,a91e4c9e-a820-4e47-aa32-3024677a8545,"not unique to WebText. For instance, we discovered that the"
Language Models are Unsupervised Multitask Learners,2019-02-14,4ff49d5c-c22c-48b6-aa66-610f45400358,test set of WikiText-103 has an article which is also in the
Language Models are Unsupervised Multitask Learners,2019-02-14,dcfa31a9-8069-4490-b528-0c41448d2bde,training dataset. Since there are only 60 articles in the test
Language Models are Unsupervised Multitask Learners,2019-02-14,f1b643ac-b5ab-4097-997b-ea599591b0f2,set there is at least an overlap of 1.6%.4Potentially more
Language Models are Unsupervised Multitask Learners,2019-02-14,a2186f7e-b84e-4095-9e9b-41a094635645,"worryingly, 1BW has an overlap of nearly 13.2% with its"
Language Models are Unsupervised Multitask Learners,2019-02-14,fdf2e618-80f4-4e7d-87a7-f858fcfc7b7f,own training set according to our procedure.
Language Models are Unsupervised Multitask Learners,2019-02-14,5748e6f3-1948-4271-8b44-2ae1f539457c,"For the Winograd Schema Challenge, we found only 10"
Language Models are Unsupervised Multitask Learners,2019-02-14,70437172-d1f0-4dec-891d-8bd4251b7f37,schemata which had any 8-gram overlaps with the WebText
Language Models are Unsupervised Multitask Learners,2019-02-14,bd866719-0bdf-4bb1-becb-c3de4134604a,"training set. Of these, 2 were spurious matches. Of the"
Language Models are Unsupervised Multitask Learners,2019-02-14,356edb38-5b94-4d06-ab16-2ec9308ab980,"remaining 8, only 1 schema appeared in any contexts that"
Language Models are Unsupervised Multitask Learners,2019-02-14,e1e89d3f-fdcb-4f44-add7-11ed6286ad3c,4A signiﬁcant portion of additional overlap is due to editors
Language Models are Unsupervised Multitask Learners,2019-02-14,ada63d36-4b06-41e3-a4cb-eccb388b19f3,reusing some paragraphs across multiple articles with a shared
Language Models are Unsupervised Multitask Learners,2019-02-14,9aad19d1-582b-45f1-aa79-2afc95328aea,theme such as various battles in the Korean War.gave away the answer.
Language Models are Unsupervised Multitask Learners,2019-02-14,3c7c8d69-c454-4fa7-a093-07baa8048905,"For CoQA, about 15% of documents in the news domain"
Language Models are Unsupervised Multitask Learners,2019-02-14,4cf1c3d6-951f-4328-8a15-18741cb782dd,are already in WebText and the model performs about 3
Language Models are Unsupervised Multitask Learners,2019-02-14,edd9145a-8301-4820-bcbd-efcb368a6ff4,F1 better on these. CoQA’s development set metric reports
Language Models are Unsupervised Multitask Learners,2019-02-14,9d041548-a2c1-4c02-b0e2-cd0076282e58,the average performance over 5 different domains and we
Language Models are Unsupervised Multitask Learners,2019-02-14,7b0b361f-69e0-4321-8cfc-5a4bed009768,measure a gain of about 0.5-1.0 F1 due to overlap across the
Language Models are Unsupervised Multitask Learners,2019-02-14,51f4c229-ae25-478e-87d7-88a88919886f,"various domains. However, no actual training questions or"
Language Models are Unsupervised Multitask Learners,2019-02-14,81a5200f-c8fe-4bf0-8113-3ad776244f57,answers are in WebText since CoQA was released after the
Language Models are Unsupervised Multitask Learners,2019-02-14,b8278cef-fdc9-4faf-ba08-c745716ea814,cutoff date for links in WebText.
Language Models are Unsupervised Multitask Learners,2019-02-14,2a3526bb-4c2f-4f86-b0f1-9ec59a715238,"On LAMBADA, the average overlap is 1.2%. GPT-2 per-"
Language Models are Unsupervised Multitask Learners,2019-02-14,f5eb8885-1688-4152-a549-0e5e095efb42,forms about 2 perplexity better on examples with greater
Language Models are Unsupervised Multitask Learners,2019-02-14,6b88076e-5b1d-4161-9dac-877825d336e3,than 15% overlap. Recalculating metrics when excluding
Language Models are Unsupervised Multitask Learners,2019-02-14,5c8b8f3c-58a4-4932-985d-3d5f96f7143d,all examples with any overlap shifts results from 8.6 to 8.7
Language Models are Unsupervised Multitask Learners,2019-02-14,30a11c98-b584-4eca-b560-4c450fdaa089,perplexity and reduces accuracy from 63.2% to 62.9%. This
Language Models are Unsupervised Multitask Learners,2019-02-14,03ada928-5175-40db-be2f-1bc868532686,very small change in overall results is likely due to only 1
Language Models are Unsupervised Multitask Learners,2019-02-14,93884ad6-9a4f-467f-a5bc-9a2a8014e8c8,in 200 examples having signiﬁcant overlap.
Language Models are Unsupervised Multitask Learners,2019-02-14,4f71a836-2001-45d9-b5a8-cb0f21c44650,"Overall, our analysis suggests that data overlap between"
Language Models are Unsupervised Multitask Learners,2019-02-14,1fc33a18-fecb-4806-a29c-c8d8ebe11eb1,WebText training data and speciﬁc evaluation datasets pro-
Language Models are Unsupervised Multitask Learners,2019-02-14,18cf7888-62eb-40fe-bb08-ee1199016d8d,vides a small but consistent beneﬁt to reported results. How-
Language Models are Unsupervised Multitask Learners,2019-02-14,07ff580f-b7be-4f40-8677-93016c8beafa,"ever, for most datasets we do not notice signiﬁcantly larger"
Language Models are Unsupervised Multitask Learners,2019-02-14,422851ff-13c2-4862-bbb6-7e64ed0f68e8,overlaps than those already existing between standard train-
Language Models are Unsupervised Multitask Learners,2019-02-14,c3cda442-9065-4973-9ed0-74049fe1bcbe,"ing and test sets, as Table 6 highlights."
Language Models are Unsupervised Multitask Learners,2019-02-14,7892fe22-714b-48a5-95e8-8aa56f8f9ac1,Understanding and quantifying how highly similar text im-
Language Models are Unsupervised Multitask Learners,2019-02-14,e568f631-41ab-41ab-9b0d-b86048024043,pacts performance is an important research question. Better
Language Models are Unsupervised Multitask Learners,2019-02-14,d8316352-47b8-40a0-83eb-6aab45987871,de-duplication techniques such as scalable fuzzy matching
Language Models are Unsupervised Multitask Learners,2019-02-14,01919576-2526-40c2-b8d0-dd4c13d4316e,"could also help better answer these questions. For now, we"
Language Models are Unsupervised Multitask Learners,2019-02-14,47085b76-8683-46a6-8f0e-48ce2229a21a,recommend the use of n-gram overlap based de-duplication
Language Models are Unsupervised Multitask Learners,2019-02-14,0e06cbd8-5865-487a-b0e9-0970339aeb4e,as an important veriﬁcation step and sanity check during the
Language Models are Unsupervised Multitask Learners,2019-02-14,048442d5-689b-4245-8f37-3a7b1db4f3c1,creation of training and test splits for new NLP datasets.
Language Models are Unsupervised Multitask Learners,2019-02-14,77325ff1-aadc-4b11-9b14-c49cd351f47c,Another potential way of determining whether the perfor-
Language Models are Unsupervised Multitask Learners,2019-02-14,6f6ac1ef-386e-4cf8-bbc5-d52f1d5a5412,mance of WebText LMs is attributable to memorization is
Language Models are Unsupervised Multitask Learners,2019-02-14,36c55b1c-5241-4089-a8fd-aa6f420eeacc,inspecting their performance on their own held-out set. As
Language Models are Unsupervised Multitask Learners,2019-02-14,4d5f06ba-0027-4cbf-9052-048aa4414551,"shown in Figure 4, performance on both the training and"
Language Models are Unsupervised Multitask Learners,2019-02-14,6e0b950c-cd35-46ea-8006-ec2aeb9171d7,test sets of WebText are similar and improve together as
Language Models are Unsupervised Multitask Learners,2019-02-14,eda7c095-4c2f-4d71-80d3-35798672aac0,model size is increased. This suggests even GPT-2 is still
Language Models are Unsupervised Multitask Learners,2019-02-14,3f14eaa2-1d18-434b-86ba-7de0af2c28bc,underﬁtting on WebText in many ways.
Language Models are Unsupervised Multitask Learners,2019-02-14,e130f99c-90d3-4811-8075-1b2eb4754337,GPT-2 is also able to write news articles about the discovery
Language Models are Unsupervised Multitask Learners,2019-02-14,0b0cfcea-cba1-4960-92eb-3d0b70c863b3,of talking unicorns. An example is provided in Table 13.
Language Models are Unsupervised Multitask Learners,2019-02-14,0426fe5a-40d1-4dd6-b57f-67d9c7fbdd49,5. Related Work
Language Models are Unsupervised Multitask Learners,2019-02-14,369dcb44-e753-4990-85ea-314994a1b910,A signiﬁcant portion of this work measured the performance
Language Models are Unsupervised Multitask Learners,2019-02-14,85f994db-651e-44eb-b4d4-b8dc83430e54,of larger language models trained on larger datasets. This
Language Models are Unsupervised Multitask Learners,2019-02-14,4758a2d6-2f17-4510-a670-e34d7a42c685,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,d0ed252b-ff29-45e4-9573-aed42f4713d8,Figure 4. The performance of LMs trained on WebText as a func-
Language Models are Unsupervised Multitask Learners,2019-02-14,e9ab0662-3aa7-4c96-b32f-c36a1e144137,tion of model size.
Language Models are Unsupervised Multitask Learners,2019-02-14,c67b433b-95b4-497e-b498-0f9f8dcfa2ef,is similar to the work of Jozefowicz et al. (2016) which
Language Models are Unsupervised Multitask Learners,2019-02-14,907c2b96-b837-42fe-baf0-de1d0359a8dc,scaled RNN based language models on the 1 Billion Word
Language Models are Unsupervised Multitask Learners,2019-02-14,96f8655a-f0c3-4c8c-8a38-0c88e6e55d88,Benchmark. Bajgar et al. (2016) also previously improved
Language Models are Unsupervised Multitask Learners,2019-02-14,381e4c89-7e60-44a5-837b-ff1be4c47e33,results on the Children’s Book Test by creating a much larger
Language Models are Unsupervised Multitask Learners,2019-02-14,d15b0589-ed8e-4c78-b536-f0dd6c435f68,training dataset out of Project Gutenberg to supplement the
Language Models are Unsupervised Multitask Learners,2019-02-14,5929ac21-916a-405a-95e6-1b012d01657f,standard training dataset. Hestness et al. (2017) conducted
Language Models are Unsupervised Multitask Learners,2019-02-14,49a6fbea-7395-4de0-8a5d-a87fab8261dd,a thorough analysis of how the performance of various deep
Language Models are Unsupervised Multitask Learners,2019-02-14,5a203f6f-c385-4efa-9fde-14060ba7447d,learning models changes as a function of both model capac-
Language Models are Unsupervised Multitask Learners,2019-02-14,781be53c-4445-476f-8293-1763484f8048,"ity and dataset size. Our experiments, while much noisier"
Language Models are Unsupervised Multitask Learners,2019-02-14,bea60a22-1258-4782-becf-24d84e8df946,"across tasks, suggest similar trends hold for sub-tasks of an"
Language Models are Unsupervised Multitask Learners,2019-02-14,39f77745-d321-421d-a19b-00fb8b4d682b,objective and continue into the 1B+ parameter regime.
Language Models are Unsupervised Multitask Learners,2019-02-14,8ec3e60f-9a0b-4932-b897-3c74ce7b75e1,Interesting learned functionality in generative models
Language Models are Unsupervised Multitask Learners,2019-02-14,b373f7ae-c5f3-46c8-a444-54c292cddffc,has been documented before such as the cells in an
Language Models are Unsupervised Multitask Learners,2019-02-14,1bfdfe43-29c4-4ce0-8985-c0e8fde8dbb4,RNN language model performing line-width tracking and
Language Models are Unsupervised Multitask Learners,2019-02-14,c299ceae-806c-4f0f-a306-04165559809d,quote/comment detection Karpathy et al. (2015). More in-
Language Models are Unsupervised Multitask Learners,2019-02-14,1e86afc5-8bbc-43a2-b7ed-b4dc04de9ad4,spirational to our work was the observation of Liu et al.
Language Models are Unsupervised Multitask Learners,2019-02-14,5b825d4c-d237-4e2d-9bc2-99f36d67b700,(2018) that a model trained to generate Wikipedia articles
Language Models are Unsupervised Multitask Learners,2019-02-14,5205f1d6-871f-4e24-9610-8de45c172b68,also learned to translate names between languages.
Language Models are Unsupervised Multitask Learners,2019-02-14,593ff999-48f8-4825-83be-f413135095ec,Previous work has explored alternative approaches to ﬁlter-
Language Models are Unsupervised Multitask Learners,2019-02-14,c282fcb0-fe55-47ac-801e-30a38e7bd839,"ing and constructing a large text corpus of web pages, such"
Language Models are Unsupervised Multitask Learners,2019-02-14,15cbf2a1-0f31-491f-807c-c16af4729fe5,"as the iWeb Corpus (Davies, 2018)."
Language Models are Unsupervised Multitask Learners,2019-02-14,c60aaf85-1243-4a54-8a2e-85e575d83bd2,There has been extensive work on pre-training methods
Language Models are Unsupervised Multitask Learners,2019-02-14,e50fe9a8-405b-41e2-878b-bcbfce9da8ca,for language tasks. In addition to those mentioned in the
Language Models are Unsupervised Multitask Learners,2019-02-14,232da95d-689a-4452-afea-cb273741355c,"introduction, GloVe (Pennington et al., 2014) scaled word"
Language Models are Unsupervised Multitask Learners,2019-02-14,fe871697-5627-43dd-a147-562c29cbe575,vector representation learning to all of Common Crawl. An
Language Models are Unsupervised Multitask Learners,2019-02-14,a23f3d7f-6205-4829-82fd-cec1734edc5b,inﬂuential early work on deep representation learning for
Language Models are Unsupervised Multitask Learners,2019-02-14,6218b214-0ce0-4e4a-b1c0-cba780279835,"text was Skip-thought Vectors (Kiros et al., 2015). McCann"
Language Models are Unsupervised Multitask Learners,2019-02-14,c49a76ac-f7de-4111-816c-5e18d2ae415d,et al. (2017) explored the use of representations derived from
Language Models are Unsupervised Multitask Learners,2019-02-14,d5e4a6a8-6fda-4ce6-bfd6-728e23629895,machine translation models and Howard & Ruder (2018)improved the RNN based ﬁne-tuning approaches of (Dai
Language Models are Unsupervised Multitask Learners,2019-02-14,7c9b2c89-127e-46d2-8ae2-b1369f338915,"& Le, 2015). (Conneau et al., 2017a) studied the transfer"
Language Models are Unsupervised Multitask Learners,2019-02-14,e7143915-aa07-41ef-8f29-a0b329ae44e0,performance of representations learned by natural language
Language Models are Unsupervised Multitask Learners,2019-02-14,7bca70e0-54b5-4044-bf79-dddd15fd0533,"inference models and (Subramanian et al., 2018) explored"
Language Models are Unsupervised Multitask Learners,2019-02-14,f532c5c8-2e85-4706-9bdd-9ba110baa3e5,large-scale multitask training.
Language Models are Unsupervised Multitask Learners,2019-02-14,791a3ca7-5c16-45a2-803f-f8b4d655bc01,"(Ramachandran et al., 2016) demonstrated that seq2seq mod-"
Language Models are Unsupervised Multitask Learners,2019-02-14,73e1d148-adf9-4ae0-9e07-abcb1568f0af,els beneﬁt from being initialized with pre-trained language
Language Models are Unsupervised Multitask Learners,2019-02-14,4b5d7581-0f5c-4e19-baf6-af790c9f5b40,models as encoders and decoders. More recent work has
Language Models are Unsupervised Multitask Learners,2019-02-14,5c4ee51e-3c3a-4315-96fe-e568baec5280,shown that LM pre-training is helpful when ﬁne-tuned for
Language Models are Unsupervised Multitask Learners,2019-02-14,ba6ba829-3d44-41d7-86ba-1503241b172a,difﬁcult generation tasks like chit-chat dialog and dialog
Language Models are Unsupervised Multitask Learners,2019-02-14,ce0f25bd-2dbb-45ea-afc6-d5b2b13c9639,"based question answering systems as well (Wolf et al., 2019)"
Language Models are Unsupervised Multitask Learners,2019-02-14,66372e9b-bd1a-4eb2-b377-8993923d2a41,"(Dinan et al., 2018)."
Language Models are Unsupervised Multitask Learners,2019-02-14,84316516-3da5-4a45-88fb-ce1d4f987e04,6. Discussion
Language Models are Unsupervised Multitask Learners,2019-02-14,a267391b-b4cb-444b-b02a-15c52c2f87dc,"Much research has been dedicated to learning (Hill et al.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f7f0870f-ee3b-4d0d-8ef8-10b32e16a90f,"2016), understanding (Levy & Goldberg, 2014), and criti-"
Language Models are Unsupervised Multitask Learners,2019-02-14,dbd9abdf-33df-4009-9fa4-8e9b77cc4460,"cally evaluating (Wieting & Kiela, 2019) the representations"
Language Models are Unsupervised Multitask Learners,2019-02-14,097ebcf6-468f-46b7-bfd9-2e63c4d90300,of both supervised and unsupervised pre-training methods.
Language Models are Unsupervised Multitask Learners,2019-02-14,a44500f0-949a-4bb5-ac64-8a0a65c50e89,Our results suggest that unsupervised task learning is an
Language Models are Unsupervised Multitask Learners,2019-02-14,39b828f5-9bc7-4db3-8a90-649499a4a2e6,additional promising area of research to explore. These
Language Models are Unsupervised Multitask Learners,2019-02-14,8d2f2f81-b073-4bf4-b24b-9709d1fd8e9d,ﬁndings potentially help explain the widespread success of
Language Models are Unsupervised Multitask Learners,2019-02-14,6e2bd5ab-7d97-4e92-9711-7ceaabcb0112,pre-training techniques for down-stream NLP tasks as we
Language Models are Unsupervised Multitask Learners,2019-02-14,69807984-31bb-4c15-ab42-2204944568cc,"show that, in the limit, one of these pre-training techniques"
Language Models are Unsupervised Multitask Learners,2019-02-14,f7225427-2a56-4650-9020-0dd30e05e4e1,begins to learn to perform tasks directly without the need
Language Models are Unsupervised Multitask Learners,2019-02-14,13eabd9f-446c-4f4a-83c8-89b44ca831b0,for supervised adaption or modiﬁcation.
Language Models are Unsupervised Multitask Learners,2019-02-14,b3ae60e9-73a8-46ac-ac5c-55c4a8671253,On reading comprehension the performance of GPT-2 is
Language Models are Unsupervised Multitask Learners,2019-02-14,432747c0-0c4e-4206-8daf-01a0f4202371,competitive with supervised baselines in a zero-shot setting.
Language Models are Unsupervised Multitask Learners,2019-02-14,37bb95c3-ba71-4f14-86b5-5b76d60ab540,"However, on other tasks such as summarization, while it"
Language Models are Unsupervised Multitask Learners,2019-02-14,c9925f75-ecc6-4f8c-b0bb-b6aef82c0eb6,"is qualitatively performing the task, its performance is still"
Language Models are Unsupervised Multitask Learners,2019-02-14,e9711d26-768a-4550-907a-6c944e2e6e20,only rudimentary according to quantitative metrics. While
Language Models are Unsupervised Multitask Learners,2019-02-14,f8557a69-1e77-4749-861e-19ad6a382d50,"suggestive as a research result, in terms of practical applica-"
Language Models are Unsupervised Multitask Learners,2019-02-14,8abab429-4308-4dc0-ba3a-7f3ad09692b3,"tions, the zero-shot performance of GPT-2 is still far from"
Language Models are Unsupervised Multitask Learners,2019-02-14,6c11a143-9f1c-4c0f-a4fc-f8e03e3ab606,use-able.
Language Models are Unsupervised Multitask Learners,2019-02-14,124682e0-befb-4fc8-8448-ee9dbcf2f3ea,We have studied the zero-shot performance of WebText
Language Models are Unsupervised Multitask Learners,2019-02-14,21b3cec1-8a9b-4f6a-a73c-00c9531a7925,"LMs on many canonical NLP tasks, but there are many addi-"
Language Models are Unsupervised Multitask Learners,2019-02-14,a7b0aa57-840a-4d84-b555-6061ab182093,tional tasks that could be evaluated. There are undoubtedly
Language Models are Unsupervised Multitask Learners,2019-02-14,959c5506-8875-4d5e-af18-7df2a82a0d0e,many practical tasks where the performance of GPT-2 is
Language Models are Unsupervised Multitask Learners,2019-02-14,288fb252-19dd-44fe-b147-4204dbeb5f89,still no better than random. Even on common tasks that we
Language Models are Unsupervised Multitask Learners,2019-02-14,2d288218-6148-4e90-9fc9-c8debc9e30c3,"evaluated on, such as question answering and translation,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f1f35b58-1e99-44e4-9ab1-5840465c105a,language models only begin to outperform trivial baselines
Language Models are Unsupervised Multitask Learners,2019-02-14,8ea42673-b4d5-4e00-affe-a55b95a0164a,when they have sufﬁcient capacity.
Language Models are Unsupervised Multitask Learners,2019-02-14,0f8a9ef5-1476-432a-bd4f-7bf668c12222,While zero-shot performance establishes a baseline of the
Language Models are Unsupervised Multitask Learners,2019-02-14,9e64d04b-ad42-49a8-80ca-016b4a9e70df,"potential performance of GPT-2 on many tasks, it is not"
Language Models are Unsupervised Multitask Learners,2019-02-14,f45a0098-15f6-4865-aaa2-819b2f6eeb2f,"clear where the ceiling is with ﬁnetuning. On some tasks,"
Language Models are Unsupervised Multitask Learners,2019-02-14,a9372ed9-5559-4cb6-b761-b9b03d6f3911,GPT-2’s fully abstractive output is a signiﬁcant departure
Language Models are Unsupervised Multitask Learners,2019-02-14,5bd380a8-76ef-4b9f-9d94-8442711c3a30,"from the extractive pointer network (Vinyals et al., 2015)"
Language Models are Unsupervised Multitask Learners,2019-02-14,ee17734e-e62d-4f29-a75d-e87bab4fba2b,based outputs which are currently state of the art on many
Language Models are Unsupervised Multitask Learners,2019-02-14,e411b581-2164-4e84-89d1-5ea43fe03883,question answering and reading comprehension datasets.
Language Models are Unsupervised Multitask Learners,2019-02-14,5cd719ce-ca78-49b3-b6bc-9ff8ff6e2b76,"Given the prior success of ﬁne-tuning GPT, we plan to in-"
Language Models are Unsupervised Multitask Learners,2019-02-14,c0704781-93d8-45aa-995b-62608a8e52d2,vestigate ﬁne-tuning on benchmarks such as decaNLP and
Language Models are Unsupervised Multitask Learners,2019-02-14,28a9e90d-1c15-4c89-9f7d-30a40e13229b,"GLUE, especially since it is unclear whether the additional"
Language Models are Unsupervised Multitask Learners,2019-02-14,bcadde2e-0379-4676-be57-89d9d694d369,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,a0aa5bc5-0720-45b8-9d64-596fa61d78c2,training data and capacity of GPT-2 is sufﬁcient to over-
Language Models are Unsupervised Multitask Learners,2019-02-14,931a4535-c128-47a2-abd0-5e9aa02450ec,come the inefﬁciencies of uni-directional representations
Language Models are Unsupervised Multitask Learners,2019-02-14,b7e513d7-b6f5-4340-aeb2-bf99bef9bda5,"demonstrated by BERT (Devlin et al., 2018)."
Language Models are Unsupervised Multitask Learners,2019-02-14,c6310fea-1559-4524-848f-7e669c8c7b92,7. Conclusion
Language Models are Unsupervised Multitask Learners,2019-02-14,9f513e89-5701-4eba-895a-b88a2f8ce194,When a large language model is trained on a sufﬁciently
Language Models are Unsupervised Multitask Learners,2019-02-14,389b5871-7c66-4977-8160-a73bdfa99849,large and diverse dataset it is able to perform well across
Language Models are Unsupervised Multitask Learners,2019-02-14,4bc616df-5802-4498-a112-c2fc7dddac6a,many domains and datasets. GPT-2 zero-shots to state of
Language Models are Unsupervised Multitask Learners,2019-02-14,fc42be94-55e3-42c9-815e-eea6cb7dc4ec,the art performance on 7 out of 8 tested language model-
Language Models are Unsupervised Multitask Learners,2019-02-14,2046c502-26be-4a09-b816-649eca70b272,ing datasets. The diversity of tasks the model is able to
Language Models are Unsupervised Multitask Learners,2019-02-14,65d55260-2085-4e09-adea-ad6dc8270463,perform in a zero-shot setting suggests that high-capacity
Language Models are Unsupervised Multitask Learners,2019-02-14,f6040eeb-2f8a-46c5-a0af-d136a321d260,models trained to maximize the likelihood of a sufﬁciently
Language Models are Unsupervised Multitask Learners,2019-02-14,a53ec0ae-58fb-4a6a-9430-9504a1a3fcc0,varied text corpus begin to learn how to perform a surprising
Language Models are Unsupervised Multitask Learners,2019-02-14,0c154492-6468-489c-825b-ecde9b6ec1ee,amount of tasks without the need for explicit supervision.5
Language Models are Unsupervised Multitask Learners,2019-02-14,5cc19b00-35fd-463c-852e-5fc807f8b589,Acknowledgements
Language Models are Unsupervised Multitask Learners,2019-02-14,587c5b59-041b-4cc7-a49d-cc07ed047275,"Thanks to everyone who wrote the text, shared the links,"
Language Models are Unsupervised Multitask Learners,2019-02-14,2bad13de-07f4-4988-aafd-543d3a8499d8,and upvoted the content in WebText. Many millions of
Language Models are Unsupervised Multitask Learners,2019-02-14,6af99365-2597-4bf4-89eb-18bb512cce4c,people were involved in creating the data that GPT-2 was
Language Models are Unsupervised Multitask Learners,2019-02-14,63ae50de-a561-4c53-b42f-ece6a4e3ae69,trained on. Also thanks to all the Googlers who helped us
Language Models are Unsupervised Multitask Learners,2019-02-14,2027548a-99a8-4025-975b-a283723b8769,"with training infrastructure, including Zak Stone, JS Riehl,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f75a0e68-bb0b-4bf1-a54f-7b0091b02439,"Jonathan Hseu, Russell Power, Youlong Cheng, Noam"
Language Models are Unsupervised Multitask Learners,2019-02-14,73432b74-0da5-431b-a967-275f0fc2d484,"Shazeer, Solomon Boulos, Michael Banﬁeld, Aman Gupta,"
Language Models are Unsupervised Multitask Learners,2019-02-14,9af54f33-340f-4aa7-a156-5100edafe001,"Daniel Sohn, and many more. Finally thanks to the people"
Language Models are Unsupervised Multitask Learners,2019-02-14,4ee1eae7-2457-49c0-b2d1-cf66107c3d4a,"who gave feedback on drafts of the paper: Jacob Steinhardt,"
Language Models are Unsupervised Multitask Learners,2019-02-14,94d402e1-0ed0-481d-8cce-d0b4e721296b,"Sam Bowman, Geoffrey Irving, and Madison May."
Language Models are Unsupervised Multitask Learners,2019-02-14,97804e5d-7a89-4b22-a854-0bf089ffbb6a,References
Language Models are Unsupervised Multitask Learners,2019-02-14,5a503d05-1bc5-4e98-ac6b-d4147acde316,"Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L."
Language Models are Unsupervised Multitask Learners,2019-02-14,b242cbc4-02c6-4703-b6f8-55678d09a2c9,Character-level language modeling with deeper self-attention.
Language Models are Unsupervised Multitask Learners,2019-02-14,231d1926-8239-4c06-baed-af9002be8543,"arXiv preprint arXiv:1808.04444 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,a4d0f298-e82e-4016-9a9b-89f90aed34d7,"Alberti, C., Lee, K., and Collins, M. A bert baseline for the natural"
Language Models are Unsupervised Multitask Learners,2019-02-14,89c8bbab-7b9e-4d12-a10a-a4133790570b,"questions. arXiv preprint arXiv:1901.08634 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,e91ef595-00a0-483b-9604-b6aa350da170,"Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and"
Language Models are Unsupervised Multitask Learners,2019-02-14,39e64de9-a31a-4fb9-be37-c363187444a1,"Nguyen, A. Strike (with) a pose: Neural networks are easily"
Language Models are Unsupervised Multitask Learners,2019-02-14,9aeb5ae0-f27f-4551-8ce5-7c80b3c61efd,fooled by strange poses of familiar objects. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,f062babb-426c-4720-9708-9d47f870dea7,"arXiv:1811.11553 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,ce15c63e-43b6-42a2-a2a3-8c4db9f6c076,"Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Batten-"
Language Models are Unsupervised Multitask Learners,2019-02-14,c4148187-283e-4469-914a-cd684a902ea7,"berg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen,"
Language Models are Unsupervised Multitask Learners,2019-02-14,863f00ad-5c6c-4396-8df6-5120962f519b,"G., et al. Deep speech 2: End-to-end speech recognition in"
Language Models are Unsupervised Multitask Learners,2019-02-14,1c38a0e1-f33a-43f2-88cf-3f3d2d412872,english and mandarin. In International Conference on Machine
Language Models are Unsupervised Multitask Learners,2019-02-14,dbe00975-50e0-413a-be75-5b4a2fc66712,"Learning , pp. 173–182, 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,0e9377cf-3154-4721-b54d-c60cf1d62a2b,"Artetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised"
Language Models are Unsupervised Multitask Learners,2019-02-14,d6f1a96d-ef2e-4047-90e1-f600052ddec7,"neural machine translation. arXiv preprint arXiv:1710.11041 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,1e3e8c1a-314c-45cb-8bf6-1305dbd391f1,2017.
Language Models are Unsupervised Multitask Learners,2019-02-14,47c01726-11cd-4a1a-b088-f5a3a7457c3d,"Artetxe, M., Labaka, G., and Agirre, E. An effective ap-"
Language Models are Unsupervised Multitask Learners,2019-02-14,17fb6b13-495d-4e78-b224-3d8e4cc8114f,proach to unsupervised machine translation. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,97d0a4c7-fb11-44d3-adb1-ac81e9dc0710,"arXiv:1902.01313 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,d83a61a5-fe0c-46d5-b84c-251e1865f75e,5Preliminary code for downloading and using the small model
Language Models are Unsupervised Multitask Learners,2019-02-14,6ff64555-e45e-48bc-be01-9780edc99e4b,"is available at https://github.com/openai/gpt-2Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization."
Language Models are Unsupervised Multitask Learners,2019-02-14,b2477dff-45f6-47ca-b687-549dd81ca005,"arXiv preprint arXiv:1607.06450 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,14d8f439-20de-4c80-a729-cad29aa46fa3,"Bajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abun-"
Language Models are Unsupervised Multitask Learners,2019-02-14,672ab7fe-88b9-4a7b-a5f9-563f73f668ef,dance: Booktest dataset for reading comprehension. arXiv
Language Models are Unsupervised Multitask Learners,2019-02-14,75a2d13a-a4d8-45db-9771-fe316df1a15c,"preprint arXiv:1610.00956 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,004d5b99-f296-4451-8ec9-6ed6a044c9de,"Barz, B. and Denzler, J. Do we train on test data? purging cifar of"
Language Models are Unsupervised Multitask Learners,2019-02-14,89f62267-38bd-4ac4-9349-43fd8de97040,"near-duplicates. arXiv preprint arXiv:1902.00423 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,74c94165-1a01-4c1c-8752-f35f63ff468a,"Bengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A neural"
Language Models are Unsupervised Multitask Learners,2019-02-14,d0505113-65d7-4eed-a874-adf2c8255857,probabilistic language model. Journal of machine learning
Language Models are Unsupervised Multitask Learners,2019-02-14,1fefe267-4a26-4364-b11b-af28059005fb,"research , 3(Feb):1137–1155, 2003."
Language Models are Unsupervised Multitask Learners,2019-02-14,855a53ec-5e69-4dea-93a0-d6b52286c814,"Bowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,78cd2572-8212-4fbc-8486-0bf93125bdab,"Hula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al."
Language Models are Unsupervised Multitask Learners,2019-02-14,4facda29-4dd1-417f-bcf4-beb2cde8a533,Looking for elmo’s friends: Sentence-level pretraining beyond
Language Models are Unsupervised Multitask Learners,2019-02-14,9469129f-dad8-4467-a7a4-a5980bd7eec5,"language modeling. arXiv preprint arXiv:1812.10860 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,b2c99555-b298-4231-b7e0-09f5901b3a1c,"Caruana, R. Multitask learning. Machine learning , 28(1):41–75,"
Language Models are Unsupervised Multitask Learners,2019-02-14,5c9bceb8-df0a-4ce2-a520-ab3ab2f2b2de,1997.
Language Models are Unsupervised Multitask Learners,2019-02-14,385a6d04-3fc5-4151-9095-a6a528aa6554,"Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn,"
Language Models are Unsupervised Multitask Learners,2019-02-14,03417602-28a2-4bb3-a882-b60aaeba3feb,"P., and Robinson, T. One billion word benchmark for measur-"
Language Models are Unsupervised Multitask Learners,2019-02-14,d1ffc221-99ff-49f0-bd8d-d9ca7214e67c,ing progress in statistical language modeling. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,e72880e8-05c4-4335-a263-dfa5264cfdb8,"arXiv:1312.3005 , 2013."
Language Models are Unsupervised Multitask Learners,2019-02-14,3f052b9c-e89c-4238-a875-f4271d12a588,"Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,"
Language Models are Unsupervised Multitask Learners,2019-02-14,1edd629c-95f2-488f-9fad-809a152e2050,"K., and Kuksa, P. Natural language processing (almost) from"
Language Models are Unsupervised Multitask Learners,2019-02-14,90c4a08f-efa5-43dc-8d78-b9ca23041bef,"scratch. Journal of Machine Learning Research , 12(Aug):2493–"
Language Models are Unsupervised Multitask Learners,2019-02-14,ad2a95d5-0d3c-4dc3-ad5d-9690c92fcdb7,"2537, 2011."
Language Models are Unsupervised Multitask Learners,2019-02-14,8aac82df-1768-464c-bdd1-bdfdd2aefcb6,"Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bor-"
Language Models are Unsupervised Multitask Learners,2019-02-14,b4f12670-b484-494d-b354-3f78bd8cfb7a,"des, A. Supervised learning of universal sentence represen-"
Language Models are Unsupervised Multitask Learners,2019-02-14,58a2982d-cd97-407b-b97d-d6ac13bbafa6,tations from natural language inference data. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,d1787ace-34c0-4c99-922f-b487433643d4,"arXiv:1705.02364 , 2017a."
Language Models are Unsupervised Multitask Learners,2019-02-14,f9e00595-acb2-4514-b700-c0cb637a3cbb,"Conneau, A., Lample, G., Ranzato, M., Denoyer, L., and J ´egou,"
Language Models are Unsupervised Multitask Learners,2019-02-14,8d207e8f-9616-4902-9520-bf99a85b2c92,H. Word translation without parallel data. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,9b0b516f-c079-41ca-b85d-feb7955a24b9,"arXiv:1710.04087 , 2017b."
Language Models are Unsupervised Multitask Learners,2019-02-14,26f62a9c-557b-494d-8c78-86346b36cfdf,"Dai, A. M. and Le, Q. V . Semi-supervised sequence learning. In"
Language Models are Unsupervised Multitask Learners,2019-02-14,6b0f021f-a614-458f-bbcf-c0300fbe0683,"Advances in neural information processing systems , pp. 3079–"
Language Models are Unsupervised Multitask Learners,2019-02-14,1c34fe36-08b6-4987-b95a-085e6f3d52a1,"3087, 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,4b652f33-ae4b-467b-8b5d-1bc441973c92,"Dai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le,"
Language Models are Unsupervised Multitask Learners,2019-02-14,08aeb982-03ab-4420-8897-30b87f023c6f,"Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive lan-"
Language Models are Unsupervised Multitask Learners,2019-02-14,1b3818db-9140-449f-b56d-5ef9920e75f1,guage models beyond a ﬁxed-length context. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,6f04f8bd-5dbe-402b-90b1-7c75e3565e62,"arXiv:1901.02860 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,f64d68fd-89fc-40d8-b001-0f8e9986fb6e,"Davies, M. The 14 billion word iweb corpus."
Language Models are Unsupervised Multitask Learners,2019-02-14,b273c59d-b345-466c-9f1a-e03f4428c81d,"https://corpus.byu.edu/iWeb/ , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,2d779518-ef1f-4f86-9759-31aa5e87519e,"Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser,"
Language Models are Unsupervised Multitask Learners,2019-02-14,b84ae582-6ae5-47f2-8cd2-d4f5d4536e17,"Ł. Universal transformers. arXiv preprint arXiv:1807.03819 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,4937ca4b-8622-4026-aa03-f6645ee8eb8a,2018.
Language Models are Unsupervised Multitask Learners,2019-02-14,7bb7e0e9-6134-4a87-bb54-ce718a4a7aeb,"Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-"
Language Models are Unsupervised Multitask Learners,2019-02-14,6faaf3c1-2ed1-4d89-838d-eccb4fb4c130,training of deep bidirectional transformers for language under-
Language Models are Unsupervised Multitask Learners,2019-02-14,43a7907d-b522-4509-abce-90de993fa992,"standing. arXiv preprint arXiv:1810.04805 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,22abe65c-bf08-4c74-a909-fb1a58938bca,"Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston,"
Language Models are Unsupervised Multitask Learners,2019-02-14,20625d35-19eb-4514-b195-3a7148c4f031,J. Wizard of wikipedia: Knowledge-powered conversational
Language Models are Unsupervised Multitask Learners,2019-02-14,f5ed1d8e-b509-454d-a0b0-56e08c2f6c09,"agents. arXiv preprint arXiv:1811.01241 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,6eb2521b-573d-4ea2-9ae6-2532a07ae361,"Fan, A., Lewis, M., and Dauphin, Y . Hierarchical neural story"
Language Models are Unsupervised Multitask Learners,2019-02-14,f2edc5b1-dd1d-4b9b-8f77-6b1b522ff9fd,"generation. arXiv preprint arXiv:1805.04833 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,e483bc74-8f04-42a7-b028-1ba76d8209d5,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,b567ba00-4df6-48cb-9205-d02db4d5772d,"Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-"
Language Models are Unsupervised Multitask Learners,2019-02-14,7dcc8edf-e3df-4868-a421-6353349dc687,learning for fast adaptation of deep networks. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,24627ce2-2741-4c75-8c65-7052b2b8698b,"arXiv:1703.03400 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,4dfb426c-8572-43c5-b0bb-93a961b0e8f0,"Gehrmann, S., Deng, Y ., and Rush, A. M. Bottom-up abstractive"
Language Models are Unsupervised Multitask Learners,2019-02-14,94b79a3a-1039-440a-a92f-2c3cabc0112f,"summarization. arXiv preprint arXiv:1808.10792 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,6468b947-0977-46d2-bd34-ba40bedbeb63,"Gillick, D., Brunk, C., Vinyals, O., and Subramanya, A. Mul-"
Language Models are Unsupervised Multitask Learners,2019-02-14,58bc442e-fd40-4812-851e-ebb92e6796e5,tilingual language processing from bytes. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,21a14a70-09c7-4af8-b39b-4492a8ecda70,"arXiv:1512.00103 , 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,4bc1d87c-bbc6-4d5f-a00b-ca10c493ed76,"Gong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y . Frage:"
Language Models are Unsupervised Multitask Learners,2019-02-14,d103dc2d-754b-46ed-82bf-ccd034ca3416,frequency-agnostic word representation. In Advances in Neural
Language Models are Unsupervised Multitask Learners,2019-02-14,39e0e28b-0bf8-4f62-8e31-11c93d6ac86a,"Information Processing Systems , pp. 1341–1352, 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,906e4a67-9c21-45f6-a666-bf175714f0f6,"Grave, E., Joulin, A., and Usunier, N. Improving neural"
Language Models are Unsupervised Multitask Learners,2019-02-14,a2361f17-ee22-4797-9bf6-f908394f2a05,language models with a continuous cache. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,8e33778d-9b11-4c9a-9ae0-cdfda248a624,"arXiv:1612.04426 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,0df796b7-a155-48c7-be13-c83492ba7e97,"He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep"
Language Models are Unsupervised Multitask Learners,2019-02-14,f37e4e2a-a307-4ced-aae8-4f1178355e28,"residual networks. In European conference on computer vision ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,e59194f1-3047-4bb7-9f28-90fc37eab8bc,"pp. 630–645. Springer, 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,1e57cfef-c979-47dc-9417-a7d5f307c4f2,"Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kian-"
Language Models are Unsupervised Multitask Learners,2019-02-14,327e1ab6-102e-41b7-91d1-80bd3e2224b9,"inejad, H., Patwary, M., Ali, M., Yang, Y ., and Zhou, Y . Deep"
Language Models are Unsupervised Multitask Learners,2019-02-14,cb69419f-1eb4-42cb-a377-54af08287892,"learning scaling is predictable, empirically. arXiv preprint"
Language Models are Unsupervised Multitask Learners,2019-02-14,51077161-841d-4232-8bd4-423417322164,"arXiv:1712.00409 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,2fbc6edc-67aa-4e3d-8556-09edf6265abf,"Hill, F., Bordes, A., Chopra, S., and Weston, J. The goldilocks"
Language Models are Unsupervised Multitask Learners,2019-02-14,5759b0a1-4141-43a4-ba92-2491cf279059,principle: Reading children’s books with explicit memory rep-
Language Models are Unsupervised Multitask Learners,2019-02-14,93339308-9f8d-44c3-97c3-299e536d6613,"resentations. arXiv preprint arXiv:1511.02301 , 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,382f2d17-f0c9-4406-8be7-5f33bcaf75b8,"Hill, F., Cho, K., and Korhonen, A. Learning distributed repre-"
Language Models are Unsupervised Multitask Learners,2019-02-14,95704d27-a0aa-4424-b6ff-25b938e2f03a,sentations of sentences from unlabelled data. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,ea07877c-ee79-4788-b12f-42dfc300cee1,"arXiv:1602.03483 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,520b28c8-db9f-426c-b327-3349e9fbddb1,"Hoang, L., Wiseman, S., and Rush, A. M. Entity tracking im-"
Language Models are Unsupervised Multitask Learners,2019-02-14,1af73d85-f64e-4a11-ad09-6ffce4d50b00,proves cloze-style reading comprehension. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,39fd9c96-068d-45f8-a1e8-a99038b9add1,"arXiv:1810.02891 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,ea69388f-49ee-4e59-920b-68301859e105,"Howard, J. and Ruder, S. Universal language model ﬁne-tuning for"
Language Models are Unsupervised Multitask Learners,2019-02-14,2e4e09af-3f43-45de-9692-251340148a32,text classiﬁcation. In Proceedings of the 56th Annual Meeting
Language Models are Unsupervised Multitask Learners,2019-02-14,e93de481-0f43-4c96-86b4-23cac181c3e7,of the Association for Computational Linguistics (Volume 1:
Language Models are Unsupervised Multitask Learners,2019-02-14,92999465-9c4b-4198-9195-43f49993d5d6,"Long Papers) , volume 1, pp. 328–339, 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,302e2b03-7634-44f5-8be4-5b2c4124b271,"Jelinek, F. and Mercer, R. L. Interpolated estimation of markov"
Language Models are Unsupervised Multitask Learners,2019-02-14,955ece07-cfb0-45a9-b310-d430104e33f7,source parameters from sparse data. In Proceedings of the
Language Models are Unsupervised Multitask Learners,2019-02-14,8dbde567-2d80-4483-bf7e-c83e203076d7,"Workshop on Pattern Recognition in Practice, Amsterdam, The"
Language Models are Unsupervised Multitask Learners,2019-02-14,615b7338-a02e-42f9-ad16-2de8b9bf5e04,"Netherlands: North-Holland, May. , 1980."
Language Models are Unsupervised Multitask Learners,2019-02-14,77302869-485c-4c44-b3aa-28f100cc1607,"Jia, R. and Liang, P. Adversarial examples for evaluating read-"
Language Models are Unsupervised Multitask Learners,2019-02-14,e8388d7e-0588-4640-9e6a-a9528f8fc73c,"ing comprehension systems. arXiv preprint arXiv:1707.07328 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,fb0dce26-2813-4e1a-b98c-0c46be207962,2017.
Language Models are Unsupervised Multitask Learners,2019-02-14,a54724ba-f705-4663-bba3-3ad45aa80327,"Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu,"
Language Models are Unsupervised Multitask Learners,2019-02-14,541eb695-b1ab-4c25-8d32-955c06051a27,Y . Exploring the limits of language modeling. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,0b821f9a-7edc-4268-9d20-3697e270fbc3,"arXiv:1602.02410 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,f6c90ce3-3dc5-4114-be13-3164e958f3d7,"Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,21630572-2019-41d2-9302-716f2fc562e3,"Jones, L., and Uszkoreit, J. One model to learn them all. arXiv"
Language Models are Unsupervised Multitask Learners,2019-02-14,8081cb73-e760-406d-b2d4-e57cd424b459,"preprint arXiv:1706.05137 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,8ef26445-dbd5-41d9-b584-c17643232666,"Karpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and under-"
Language Models are Unsupervised Multitask Learners,2019-02-14,213aefe1-26d7-408c-b6bd-6489010b5110,"standing recurrent networks. arXiv preprint arXiv:1506.02078 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,e5b67642-367b-4114-83b4-7ef4e92f9ec3,"2015.Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins,"
Language Models are Unsupervised Multitask Learners,2019-02-14,0c8ac993-406c-432c-b402-9f73113df564,"G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-"
Language Models are Unsupervised Multitask Learners,2019-02-14,78380771-54c2-4f3e-8be3-b27397cc1885,"Barwinska, A., et al. Overcoming catastrophic forgetting in"
Language Models are Unsupervised Multitask Learners,2019-02-14,ed383f52-cb2d-495e-b689-77ec787d4611,neural networks. Proceedings of the national academy of sci-
Language Models are Unsupervised Multitask Learners,2019-02-14,02bc3f6a-1a95-42ce-9b70-cc085b473449,"ences , pp. 201611835, 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,52c020d8-2a1b-4647-a6b8-2be8453237da,"Kiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun, R.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,af900971-5149-47fc-a3ae-d1a01e1cf3ff,"Torralba, A., and Fidler, S. Skip-thought vectors. In Advances"
Language Models are Unsupervised Multitask Learners,2019-02-14,7ab9f6b1-6c4d-49f9-99fe-e0f202764ce0,"in neural information processing systems , pp. 3294–3302, 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,b4534f93-28b5-45f9-b9f8-e117eb39036a,"Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁ-"
Language Models are Unsupervised Multitask Learners,2019-02-14,12a177e3-51fb-41cd-8777-64a41997fdba,cation with deep convolutional neural networks. In Advances in
Language Models are Unsupervised Multitask Learners,2019-02-14,467dc542-a895-4490-8cd6-1a74d8323c4c,"neural information processing systems , pp. 1097–1105, 2012."
Language Models are Unsupervised Multitask Learners,2019-02-14,2cf67fc5-9f2c-4f39-af37-30a43c8943aa,"Kwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh,"
Language Models are Unsupervised Multitask Learners,2019-02-14,decce1ae-b87f-4a09-b77d-754e2dbc56b5,"A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin,"
Language Models are Unsupervised Multitask Learners,2019-02-14,37e1e945-2484-43b4-916b-26e9db46bf31,"J., et al. Natural questions: a benchmark for question answering"
Language Models are Unsupervised Multitask Learners,2019-02-14,b9a8b74d-9388-4086-bc4f-5eb67161682c,research. 2019.
Language Models are Unsupervised Multitask Learners,2019-02-14,f433c22c-e002-446e-8747-470811214526,"Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J."
Language Models are Unsupervised Multitask Learners,2019-02-14,cc123092-e33d-445b-972f-9d6d31be51ae,Building machines that learn and think like people. Behavioral
Language Models are Unsupervised Multitask Learners,2019-02-14,c006df1c-add4-4ed4-b25a-1071933c2732,"and Brain Sciences , 40, 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,48b501e3-9fec-4e90-835f-783ae6b3a2a5,"Lample, G., Conneau, A., Denoyer, L., and Ranzato, M. Unsu-"
Language Models are Unsupervised Multitask Learners,2019-02-14,426feb02-f5f9-4bb2-9ced-e36cff5e14b2,pervised machine translation using monolingual corpora only.
Language Models are Unsupervised Multitask Learners,2019-02-14,79cc84a6-35ad-4035-9e91-8d3ea844b79a,"arXiv preprint arXiv:1711.00043 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,db8413f2-cde9-43d9-87ca-36fd348458ea,"Levesque, H., Davis, E., and Morgenstern, L. The winograd"
Language Models are Unsupervised Multitask Learners,2019-02-14,9752eed1-fb6f-4842-abf3-bec71faa90f1,schema challenge. In Thirteenth International Conference on
Language Models are Unsupervised Multitask Learners,2019-02-14,fc66d264-0471-4d32-83ef-8c3dd968bc3c,"the Principles of Knowledge Representation and Reasoning ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,d424d86b-dc6e-4d89-b520-b0dba695f7a1,2012.
Language Models are Unsupervised Multitask Learners,2019-02-14,f818354f-92bd-4ddd-ae9a-526b3b47fa8b,"Levy, O. and Goldberg, Y . Neural word embedding as implicit ma-"
Language Models are Unsupervised Multitask Learners,2019-02-14,6ae98e8b-2364-4f72-bdde-6a1d09344ba6,trix factorization. In Advances in neural information processing
Language Models are Unsupervised Multitask Learners,2019-02-14,a8c15461-dcc2-48c7-b14e-5a9272dfc8a4,"systems , pp. 2177–2185, 2014."
Language Models are Unsupervised Multitask Learners,2019-02-14,878b79c0-938d-436f-9fbf-1bd852503604,"Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,4710878d-786f-4a34-ae2c-c3d911d73a9c,"and Shazeer, N. Generating wikipedia by summarizing long"
Language Models are Unsupervised Multitask Learners,2019-02-14,0bc42419-5906-421c-a1d0-5e80974fa66d,"sequences. arXiv preprint arXiv:1801.10198 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,a3c0abc7-1adf-4676-8408-9347ebe0631a,"McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned"
Language Models are Unsupervised Multitask Learners,2019-02-14,91efa375-fafe-4369-9c72-a2cd57662015,in translation: Contextualized word vectors. In Advances in
Language Models are Unsupervised Multitask Learners,2019-02-14,cd1c589d-450c-446c-8f10-28ec268582ce,"Neural Information Processing Systems , pp. 6294–6305, 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,e14fdd32-b178-40c0-9618-888d1cffe97b,"McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural"
Language Models are Unsupervised Multitask Learners,2019-02-14,8df52626-d096-457e-89ba-b1e0a47273c5,language decathlon: Multitask learning as question answering.
Language Models are Unsupervised Multitask Learners,2019-02-14,0a12129a-7e86-4cbd-9adf-d4695757490c,"arXiv preprint arXiv:1806.08730 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,583a8a4e-06c3-47ff-8a25-d7a823af01fb,"Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel"
Language Models are Unsupervised Multitask Learners,2019-02-14,68845393-0155-4077-a3a2-8d7dcf8f6f23,"mixture models. arXiv preprint arXiv:1609.07843 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,14963185-f50e-45eb-8eab-6e8a2e80252b,"Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f965acb9-b00f-4298-b34c-559aa89f274b,J. Distributed representations of words and phrases and their
Language Models are Unsupervised Multitask Learners,2019-02-14,ac3e192a-8c77-46e5-b40f-a177e9d843ce,compositionality. In Advances in neural information processing
Language Models are Unsupervised Multitask Learners,2019-02-14,233e12ea-8b81-49ff-b4e2-a9d79e4fc402,"systems , pp. 3111–3119, 2013."
Language Models are Unsupervised Multitask Learners,2019-02-14,56990114-33ba-4a0d-baa0-e42f7508f686,"Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstrac-"
Language Models are Unsupervised Multitask Learners,2019-02-14,c4918ac1-dafc-4a3a-9af4-d0c80d8c4541,tive text summarization using sequence-to-sequence rnns and
Language Models are Unsupervised Multitask Learners,2019-02-14,223e1e77-f62a-47af-9920-5a17b1ca08cd,"beyond. arXiv preprint arXiv:1602.06023 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,f3c27270-3abb-4586-a24d-220f8f222558,"Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi,"
Language Models are Unsupervised Multitask Learners,2019-02-14,4a6dfe11-2f78-46ac-8960-9e6d8e58b90d,"R., Pezzelle, S., Baroni, M., Boleda, G., and Fern ´andez, R. The"
Language Models are Unsupervised Multitask Learners,2019-02-14,066dd079-e8af-41f4-807f-edd16fbc6c53,lambada dataset: Word prediction requiring a broad discourse
Language Models are Unsupervised Multitask Learners,2019-02-14,6c4aea4e-7187-41c4-9bbe-b28eed3027eb,"context. arXiv preprint arXiv:1606.06031 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,026e5904-e612-4b8a-94ef-1d66739b7033,"Pennington, J., Socher, R., and Manning, C. Glove: Global vectors"
Language Models are Unsupervised Multitask Learners,2019-02-14,c683b533-0efd-42de-9556-de89cc95234c,for word representation. In Proceedings of the 2014 conference
Language Models are Unsupervised Multitask Learners,2019-02-14,c9fa0055-1e1b-4fd2-adf0-e3eed683e671,"on empirical methods in natural language processing (EMNLP) ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,cfcdc8e5-353d-4174-9b76-ac7bc94386a8,"pp. 1532–1543, 2014."
Language Models are Unsupervised Multitask Learners,2019-02-14,1fc7b05e-30a0-4f22-8ea6-dc5e17cb39a8,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,bffbaa4b-b143-4270-a1ba-64dd938a5196,"Peters, M. E. and Lecocq, D. Content extraction using diverse fea-"
Language Models are Unsupervised Multitask Learners,2019-02-14,ddb7d364-e39e-4a7b-a2a7-56dbd69a3781,ture sets. In Proceedings of the 22nd International Conference
Language Models are Unsupervised Multitask Learners,2019-02-14,6ccb9193-f4a2-4b6a-842c-4bee1f7f7f25,"on World Wide Web , pp. 89–90. ACM, 2013."
Language Models are Unsupervised Multitask Learners,2019-02-14,4a78415f-b678-4b8c-837e-dd5c7669e19c,"Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,807e976d-0d3b-4f7b-be56-964e2b18b6b0,"Lee, K., and Zettlemoyer, L. Deep contextualized word repre-"
Language Models are Unsupervised Multitask Learners,2019-02-14,aa81f587-6c2d-435b-9b19-a4daadc1210e,"sentations. arXiv preprint arXiv:1802.05365 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,850a3d20-e9bf-44c7-a046-521499d0f753,"Radford, A., Jozefowicz, R., and Sutskever, I. Learning to"
Language Models are Unsupervised Multitask Learners,2019-02-14,24fb259e-9d3b-4912-8742-aa5a3f36c1c9,generate reviews and discovering sentiment. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,30f62048-a437-4675-b0c0-2627ebf21734,"arXiv:1704.01444 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,fbef0919-d11d-43c6-a4dd-8113b8106344,"Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I."
Language Models are Unsupervised Multitask Learners,2019-02-14,ced949ec-9e5f-44dc-8bc5-b7f72bfe978c,Improving language understanding by generative pre-training.
Language Models are Unsupervised Multitask Learners,2019-02-14,b34eb065-2a24-452d-9ecf-9e459604e20d,2018.
Language Models are Unsupervised Multitask Learners,2019-02-14,c415ed5e-8633-4793-b8c5-b0c75cc85591,"Ramachandran, P., Liu, P. J., and Le, Q. V . Unsupervised pre-"
Language Models are Unsupervised Multitask Learners,2019-02-14,99b9b923-9d07-4182-a9bd-74e3a6c8dfc3,training for sequence to sequence learning. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,eaebc6e7-b55a-4c44-9ca1-a7608e8476b2,"arXiv:1611.02683 , 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,18068830-104a-459c-94f5-f7ef8b25e480,"Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do"
Language Models are Unsupervised Multitask Learners,2019-02-14,ef616ad6-c691-4df0-9e3f-de77b189645a,cifar-10 classiﬁers generalize to cifar-10? arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,9a8c0c08-b359-4c7d-ab2b-854690d103e5,"arXiv:1806.00451 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,a667e47c-684e-4bb7-9e39-e54098dcbf7d,"Reddy, S., Chen, D., and Manning, C. D. Coqa: A conversational"
Language Models are Unsupervised Multitask Learners,2019-02-14,4c129eb6-6625-413d-85df-ee2bab2a578c,"question answering challenge. arXiv preprint arXiv:1808.07042 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,7953a41e-b412-4144-8884-4635cc9a5e04,2018.
Language Models are Unsupervised Multitask Learners,2019-02-14,e58add10-b897-4fa4-a0e5-01bc0f1e7e3e,"Schwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y ., and Smith,"
Language Models are Unsupervised Multitask Learners,2019-02-14,7fc7e5f9-5a1d-4811-838f-eafdf7c0a759,N. A. Story cloze task: Uw nlp system. In Proceedings of the
Language Models are Unsupervised Multitask Learners,2019-02-14,35840b15-13ac-4662-ac72-5104e0ede20b,"2nd Workshop on Linking Models of Lexical, Sentential and"
Language Models are Unsupervised Multitask Learners,2019-02-14,cff5ec12-4f94-4777-9b8d-10527c0bbee4,"Discourse-level Semantics , pp. 52–55, 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,3c521726-c0a4-40c9-a27f-7629431711fa,"See, A., Liu, P. J., and Manning, C. D. Get to the point: Sum-"
Language Models are Unsupervised Multitask Learners,2019-02-14,b2f0ee30-1413-4958-a335-21e0916cd7cc,marization with pointer-generator networks. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,9dac9cf9-c0c4-4ce7-95e3-3fca0f11b9e9,"arXiv:1704.04368 , 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,9f39c82f-d92f-4569-864c-121187f74387,"Sennrich, R., Haddow, B., and Birch, A. Neural machine trans-"
Language Models are Unsupervised Multitask Learners,2019-02-14,7ee8328d-732e-432a-a547-a0b181aa8559,lation of rare words with subword units. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,b8624aae-f2fc-49ae-a05e-520485df0745,"arXiv:1508.07909 , 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,49d8d4ce-7bc9-4e97-b497-479f51acb80a,"Subramanian, S., Trischler, A., Bengio, Y ., and Pal, C. J. Learning"
Language Models are Unsupervised Multitask Learners,2019-02-14,719b86ba-b5d7-4a6b-bdde-358e47a02d60,general purpose distributed sentence representations via large
Language Models are Unsupervised Multitask Learners,2019-02-14,ef1f60e0-5727-49c6-863a-137f043f1147,"scale multi-task learning. arXiv preprint arXiv:1804.00079 ,"
Language Models are Unsupervised Multitask Learners,2019-02-14,109e9aec-12f7-4c70-b5e5-cdb9f98e553c,2018.
Language Models are Unsupervised Multitask Learners,2019-02-14,56080ca8-451e-47ed-b737-3e4c45ba97a1,"Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to sequence"
Language Models are Unsupervised Multitask Learners,2019-02-14,89e49e50-4b91-422c-b995-91293d1a8bc9,learning with neural networks. In Advances in neural informa-
Language Models are Unsupervised Multitask Learners,2019-02-14,a066e0ce-89e7-4e27-8782-7e2d4994ab0b,"tion processing systems , pp. 3104–3112, 2014."
Language Models are Unsupervised Multitask Learners,2019-02-14,c79bed3f-80d1-4d9b-9485-6d94dec0dec7,"Sutskever, I., Jozefowicz, R., Gregor, K., Rezende, D., Lillicrap,"
Language Models are Unsupervised Multitask Learners,2019-02-14,207fb623-173a-45d9-b822-0dbd907fc248,"T., and Vinyals, O. Towards principled unsupervised learning."
Language Models are Unsupervised Multitask Learners,2019-02-14,a84035d8-05a6-413f-8817-e839d0f80dd2,"arXiv preprint arXiv:1511.06440 , 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,421da31d-9333-4286-8ac2-10b683434b56,"Trichelair, P., Emami, A., Cheung, J. C. K., Trischler, A., Sule-"
Language Models are Unsupervised Multitask Learners,2019-02-14,d4fae411-ef8a-4d83-b18f-f5368abbe55c,"man, K., and Diaz, F. On the evaluation of common-sense"
Language Models are Unsupervised Multitask Learners,2019-02-14,d3f5b85b-1e51-4e9a-9eda-0a27409bafc0,reasoning in natural language understanding. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,85dfc3fd-d87a-421f-a9da-af6b9bb597ff,"arXiv:1811.01778 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,7bd02bd5-c140-42e0-9149-7dd9c7217ba2,"Trinh, T. H. and Le, Q. V . A simple method for commonsense"
Language Models are Unsupervised Multitask Learners,2019-02-14,c3c6484a-bc4a-47e5-851d-0f57597b26d0,"reasoning. arXiv preprint arXiv:1806.02847 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,b8de55ae-3d32-4f20-a9c1-7328927ebd80,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,baa0d2f7-2bbb-4096-be53-62e0f8d59348,"Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is"
Language Models are Unsupervised Multitask Learners,2019-02-14,251572f1-0aa9-401f-8890-19684c0578f2,all you need. In Advances in Neural Information Processing
Language Models are Unsupervised Multitask Learners,2019-02-14,c63113de-d173-44fc-a95d-d6772fbfeb2f,"Systems , pp. 5998–6008, 2017."
Language Models are Unsupervised Multitask Learners,2019-02-14,e53c2ec9-3b1e-4a63-a4d6-894922093769,"Vinyals, O. and Le, Q. A neural conversational model. arXiv"
Language Models are Unsupervised Multitask Learners,2019-02-14,42927809-6947-4f20-bf51-a5653ab68aa3,"preprint arXiv:1506.05869 , 2015.Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In"
Language Models are Unsupervised Multitask Learners,2019-02-14,f303fece-81a4-4d3d-a90a-bfad064a5f8b,"Advances in Neural Information Processing Systems , pp. 2692–"
Language Models are Unsupervised Multitask Learners,2019-02-14,37f19100-6c98-4deb-ae7e-1e9815658c8b,"2700, 2015."
Language Models are Unsupervised Multitask Learners,2019-02-14,40a8a883-5860-4cd3-a872-01b8500390b4,"Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-"
Language Models are Unsupervised Multitask Learners,2019-02-14,96c59f89-2879-44b9-bda9-5d6985e022a8,"man, S. R. Glue: A multi-task benchmark and analysis"
Language Models are Unsupervised Multitask Learners,2019-02-14,65d487e6-4bad-4a3d-ab4e-f13fccc12d98,platform for natural language understanding. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,53ba5f5c-d9cc-49d0-a69f-d9389c3297db,"arXiv:1804.07461 , 2018."
Language Models are Unsupervised Multitask Learners,2019-02-14,beb1773a-2328-4321-8d68-8141734906da,"Weston, J. E. Dialog-based language learning. In Advances in"
Language Models are Unsupervised Multitask Learners,2019-02-14,3abd5f31-0e37-4219-930f-4cf2cdaaa537,"Neural Information Processing Systems , pp. 829–837, 2016."
Language Models are Unsupervised Multitask Learners,2019-02-14,41912489-d51c-4b3d-b4e6-4a745148cebe,"Wieting, J. and Kiela, D. No training required: Exploring"
Language Models are Unsupervised Multitask Learners,2019-02-14,3212167c-efc1-46dd-9880-d731ec1ad736,random encoders for sentence classiﬁcation. arXiv preprint
Language Models are Unsupervised Multitask Learners,2019-02-14,922e7dfb-132f-4054-9393-29cd4b10da89,"arXiv:1901.10444 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,0402f316-5ce6-43fc-a3ac-581d72aba879,"Wolf, T., Sanh, V ., Chaumond, J., and Delangue, C. Transfer-"
Language Models are Unsupervised Multitask Learners,2019-02-14,6f621ff6-c870-48b2-b20b-7aac3e117bbf,transfo: A transfer learning approach for neural network based
Language Models are Unsupervised Multitask Learners,2019-02-14,b083de1a-b51c-4d4d-93ed-1ac545467c47,"conversational agents. arXiv preprint arXiv:1901.08149 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,bf0caf06-a361-45fa-a2c2-2e57712e04c0,"Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,2c0a8df8-81f0-4394-b36e-e8457773e286,"Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f0a3fa42-4145-422a-95bf-173ab1bb1688,"Dyer, C., et al. Learning and evaluating general linguistic intel-"
Language Models are Unsupervised Multitask Learners,2019-02-14,f00495bd-1981-4b72-b73a-e5d9064b1167,"ligence. arXiv preprint arXiv:1901.11373 , 2019."
Language Models are Unsupervised Multitask Learners,2019-02-14,33f6749d-eea2-4c98-8b94-7069f44949c6,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,6fe87ea7-827e-4f16-99e3-eaa7713ad6df,8. Appendix A: Samples
Language Models are Unsupervised Multitask Learners,2019-02-14,427f197d-fc18-47a9-98c8-0317a783792e,8.1. Model capacity
Language Models are Unsupervised Multitask Learners,2019-02-14,e1065f6f-41d1-4b38-9c6b-e8a44652f646,To complement the reported perplexity gains of bigger LMs on
Language Models are Unsupervised Multitask Learners,2019-02-14,79423d0c-183d-4e4c-a08b-2ea940770798,"WebText show in Figure 4, Tables 7 through 11 show side-by-side"
Language Models are Unsupervised Multitask Learners,2019-02-14,422b11b6-6d2b-48e3-89ba-e0e99b2730ee,completions of the smallest WebText LM and GPT-2 on random
Language Models are Unsupervised Multitask Learners,2019-02-14,b46d7307-a173-4280-8a13-c99871bd47d0,unseen WebText test set articles.
Language Models are Unsupervised Multitask Learners,2019-02-14,cdcdc573-e5e4-44a4-9603-28c3c49bbc10,8.2. Text Memorization
Language Models are Unsupervised Multitask Learners,2019-02-14,da9c9f11-28b9-4bf6-8ad2-6097da0d1a61,We observe some memorizing behavior in GPT-2 on longer strings
Language Models are Unsupervised Multitask Learners,2019-02-14,9edd740b-84a7-4931-a001-a44e62415d3a,that are repeated many times in the dataset such as famous quotes
Language Models are Unsupervised Multitask Learners,2019-02-14,cca9bbba-9406-40f4-8858-44daa99df915,"or speeches. For example, when conditioned on the ﬁrst sentence"
Language Models are Unsupervised Multitask Learners,2019-02-14,50b10c1b-d879-4e2a-a749-bf998c11d9df,and a half of the Gettysburg Address (which occurs approximately
Language Models are Unsupervised Multitask Learners,2019-02-14,9edbfab5-c1c7-49f5-974d-538a6e8a55bb,"40 times throughout WebText), an argmax decode from GPT-2"
Language Models are Unsupervised Multitask Learners,2019-02-14,d3c10949-2cf3-4621-aa79-e6c174b18a53,"recovers the speech. Even when sampling without truncation, we"
Language Models are Unsupervised Multitask Learners,2019-02-14,b6cc3de1-6e0d-4f2c-9fcb-0aead1a092a6,"ﬁnd that the model copies the speech for awhile before drifting,"
Language Models are Unsupervised Multitask Learners,2019-02-14,fa158aa4-09d7-4e94-a3f9-6f31a78111cb,"albeit in a similar style. It typically drifts within 100-200 tokens,"
Language Models are Unsupervised Multitask Learners,2019-02-14,8a5c8f91-c173-4713-864b-2d42c3862024,and displays widening diversity once it drifts.
Language Models are Unsupervised Multitask Learners,2019-02-14,8565e291-083c-48c9-b664-4162d91201ea,"To quantify how often exact memorization shows up in samples,"
Language Models are Unsupervised Multitask Learners,2019-02-14,988cd4a4-f0f6-4114-bf77-b4fc5c7aef78,we generated samples from GPT-2 conditioned on WebText test
Language Models are Unsupervised Multitask Learners,2019-02-14,9f522f51-97e9-4bc1-821e-ef95b2aa4227,set articles and compared the overlap rates of GPT-2’s generations
Language Models are Unsupervised Multitask Learners,2019-02-14,bdeee9d8-5137-45c2-905d-9d88cfdb63ab,to the overlap rates of the ground-truth completions. The results of
Language Models are Unsupervised Multitask Learners,2019-02-14,25e4b698-d9b1-4ff8-af8b-e9f1592ba426,this analysis are shown below and suggest that GPT-2 repeats text
Language Models are Unsupervised Multitask Learners,2019-02-14,c10bc035-961c-4ee0-baa5-f1fd00566a6d,from the training set less often then the baseline rate of held-out
Language Models are Unsupervised Multitask Learners,2019-02-14,b7bddb02-1c01-41f3-9fb1-f41d52e1e884,articles.
Language Models are Unsupervised Multitask Learners,2019-02-14,e5f3f111-0d0b-4a09-a6b2-230b5555b6a7,Figure 5. CDF of percentage 8-gram overlap with WebText train-
Language Models are Unsupervised Multitask Learners,2019-02-14,84eeaa0d-0108-4305-88ed-6d7a3b05d33c,"ing set, for both WebText test set and samples (conditioned on"
Language Models are Unsupervised Multitask Learners,2019-02-14,aae9b205-3dc7-41aa-a523-c31f013cf07b,"WebText test set, with top- ktruncated random sampling with"
Language Models are Unsupervised Multitask Learners,2019-02-14,bb6c74d4-6c64-4549-adee-1b86d8d717c1,"k= 40 ). Most samples have less than 1% overlap, including over"
Language Models are Unsupervised Multitask Learners,2019-02-14,3aa4379b-4f4c-43e7-9fcf-b196f142f776,"30% of samples with no overlap, whereas the median for test set is"
Language Models are Unsupervised Multitask Learners,2019-02-14,84e65fa5-0e46-485e-abc9-ce24ff9f52ba,2.6% overlap.8.3. Diversity
Language Models are Unsupervised Multitask Learners,2019-02-14,4405026d-b58f-4839-999f-42480ee8be13,Table 12 shows multiple completions of the same random WebText
Language Models are Unsupervised Multitask Learners,2019-02-14,c7a95f18-d24c-4aff-ad9a-29d753678f94,"test set context, showing the diversity of completions with standard"
Language Models are Unsupervised Multitask Learners,2019-02-14,f51e90a8-01bf-4f0b-929e-06535dc91469,sampling settings.
Language Models are Unsupervised Multitask Learners,2019-02-14,2f5d53ff-7c44-4e1a-846b-232b85945374,8.4. Robustness
Language Models are Unsupervised Multitask Learners,2019-02-14,5de1a496-bcf2-471d-a48b-967a7b30feea,Table 13 shows the previously mentioned talking unicorns news
Language Models are Unsupervised Multitask Learners,2019-02-14,ca3ed745-494d-4eb3-9f0f-19fb293bc16b,article. We ﬁnd the model to be capable of handling out of dis-
Language Models are Unsupervised Multitask Learners,2019-02-14,9fed93bb-fee1-4bcc-9768-0c3367f4be44,"tribution contexts, but the quality of these samples is generally"
Language Models are Unsupervised Multitask Learners,2019-02-14,c4f9ca39-b284-4a90-ac2c-f2dbecbe32a9,lower.
Language Models are Unsupervised Multitask Learners,2019-02-14,e88464e6-14ce-4622-88a0-286d31396a97,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,9889a61d-e2a1-4eb1-ac2b-83229b51ab98,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,6f40dfae-bb57-4689-acc3-ed6e160374fe,Outﬁt 1: Typical
Language Models are Unsupervised Multitask Learners,2019-02-14,39bee4e9-d7b6-4d0d-9d83-9ca994ca9fa6,This pairing was the ﬁrst outﬁt I thought of when I bought the shoes. It’s like a summer version of this
Language Models are Unsupervised Multitask Learners,2019-02-14,379bc6b0-a130-4f5c-b925-631cc53c9117,"Jake Grantham outﬁt; in fact, my shoes are close to the colors of his Nike Racers! Instead of a heavy Harris Tweed"
Language Models are Unsupervised Multitask Learners,2019-02-14,098861f3-c3e0-4246-a2c9-7c0f169651a2,"jacket and denim shirt, I’m wearing a cotton DB jacket and and a linen shirt. Both fabrics (in these colors) are an"
Language Models are Unsupervised Multitask Learners,2019-02-14,9a327214-e79a-4627-a6e9-d5789d157326,"absolute must for summer, as they go with both dark and and light pants! As you can see, they pair wonderfully"
Language Models are Unsupervised Multitask Learners,2019-02-14,e0ffb862-f9e5-4e5b-a5d1-fd8d8d5d83e2,with the dark jeans and shoes. It’s a pseudo menswear/prep outﬁt.
Language Models are Unsupervised Multitask Learners,2019-02-14,246d2a77-8b50-418d-a004-32622fa4cd8b,"Overall, this is a very casual outﬁt which is why I paired my sneakers with it. I’m not about wearing a"
Language Models are Unsupervised Multitask Learners,2019-02-14,42453d89-7872-4ed6-b1a3-92eae41b2d4c,"full wool suit with sneakers (as GQ shows a lot) but I’m deﬁnitely open to keeping things casual, like this cotton"
Language Models are Unsupervised Multitask Learners,2019-02-14,56b17cff-92d7-4bcc-80c6-5806f4813698,DB. Casual fabrics are key to pulling off your sneakers in a dressed down menswear outﬁt. I’d even suggest to
Language Models are Unsupervised Multitask Learners,2019-02-14,fb306ed9-e898-41b0-8b49-6f7f8341b82f,wear these sneakers with a khaki chino suit or a white linen suit. Just be sure to ditch the tie or wear a tee or polo;
Language Models are Unsupervised Multitask Learners,2019-02-14,211f86be-8590-4e38-b71b-423280439f64,wearing a tie with sneakers is a bit too much
Language Models are Unsupervised Multitask Learners,2019-02-14,30e168f0-777a-4556-919d-79bb8cd28a30,Small model completion
Language Models are Unsupervised Multitask Learners,2019-02-14,74c838e7-c266-4304-95ea-3896dcbffb7d,.
Language Models are Unsupervised Multitask Learners,2019-02-14,873a5087-a9ff-4657-873a-373e5284dd54,Outﬁt 2: High End
Language Models are Unsupervised Multitask Learners,2019-02-14,bae45543-5859-4c28-97a7-8f7cfd0f7d7a,This outﬁt was just in the eye of the storm. It’s a
Language Models are Unsupervised Multitask Learners,2019-02-14,bc543c3e-f71d-4367-ada4-f3299bea75b3,hybrid of what ASICS does with streetwear (like J. Crew)
Language Models are Unsupervised Multitask Learners,2019-02-14,fe78f6d4-e2e7-466c-967c-8a25e54c51d9,and what J. Crew does with casual pieces that are a bit
Language Models are Unsupervised Multitask Learners,2019-02-14,59c5ed8d-dec2-4b2a-9058-b41850486582,"more traditional. Again, as I mentioned before, it’s a low"
Language Models are Unsupervised Multitask Learners,2019-02-14,e278ffc6-39e1-4d32-bb2c-71dbf56ae414,budget item and they can be very expensive. These will
Language Models are Unsupervised Multitask Learners,2019-02-14,ef133cde-74b5-4e19-b5a0-e2ae6d2c9704,deﬁnitely be an issue if you’re trying to work as a casual
Language Models are Unsupervised Multitask Learners,2019-02-14,9a9ec06b-476f-4170-8db6-5d35434d305e,"sneakermaker, and that’s OK but don’t hesitate if you"
Language Models are Unsupervised Multitask Learners,2019-02-14,1412ba1d-e399-4399-b7c6-e8448e20c402,want to try the lower price versions of their outﬁts!
Language Models are Unsupervised Multitask Learners,2019-02-14,fa4d5f3f-485a-4bbf-8ff1-6837feb8e15a,This version features a very low cut-cut neckline
Language Models are Unsupervised Multitask Learners,2019-02-14,f79d3680-18f8-4949-91b7-3782d44a38ec,"(which is not a bad thing), slightly longer legs, and a pair"
Language Models are Unsupervised Multitask Learners,2019-02-14,8b7c7ac6-8a82-4aa9-bdff-66b6291f9473,of long legs to keep things nice. My preferred color of
Language Models are Unsupervised Multitask Learners,2019-02-14,aae27d32-85b4-444e-8581-ba710eb8287e,"color these items are blue-gray (which is my ﬁrst colors,"
Language Models are Unsupervised Multitask Learners,2019-02-14,1ab6adc0-4be2-4124-ac23-7feb67ea4261,"as they are a light white), or black (which is my favorite"
Language Models are Unsupervised Multitask Learners,2019-02-14,cf739e04-061f-4ceb-9826-641a1dd437af,"of my designs). As always, my pick of colors from this"
Language Models are Unsupervised Multitask Learners,2019-02-14,056f5e74-d114-4ac8-b86a-e3f5c8a1ef68,store have nothing to do with that other than the de-
Language Models are Unsupervised Multitask Learners,2019-02-14,2b44d1af-490d-44f1-98f1-e38a10e54f14,"sign. For a high end, I would recommend this to everyone!"
Language Models are Unsupervised Multitask Learners,2019-02-14,6978eb6f-55db-4c52-9236-3e5cb921ee41,Outﬁt 3: High Light
Language Models are Unsupervised Multitask Learners,2019-02-14,2fc69627-f1a7-4097-8115-f00b75a9d139,This was a ”high light” look while I had a chance
Language Models are Unsupervised Multitask Learners,2019-02-14,74b71247-8988-446c-ab2c-014cd77a3b0c,to get some color. I think the color is based onGPT-2 completion
Language Models are Unsupervised Multitask Learners,2019-02-14,7432d292-7dee-4525-b60f-25a1ae1646ac,for me!
Language Models are Unsupervised Multitask Learners,2019-02-14,d05aefd9-f6da-4bba-a66b-03e445fd8b36,Outﬁt 2: Casual Jacket
Language Models are Unsupervised Multitask Learners,2019-02-14,835ddb47-9b62-4478-bc3a-70998f666eb5,I know that some people might be opposed to wearing
Language Models are Unsupervised Multitask Learners,2019-02-14,183e1fe4-4d11-4cfc-93c1-e598f0adf3ea,"sneakers in a turtleneck, but I wanted to be true to myself,"
Language Models are Unsupervised Multitask Learners,2019-02-14,e1a11e28-4123-4b7b-a088-820914c1f326,so I went with a slim ﬁtting turtleneck in a color more
Language Models are Unsupervised Multitask Learners,2019-02-14,e77a87a9-55fd-4b8b-bb22-5437b6f64ebf,"similar to my favorite color of the day. Overall, it’s a"
Language Models are Unsupervised Multitask Learners,2019-02-14,d3097823-ab46-478b-a73b-0edd5d12ab50,"classic turtleneck, with nothing too ﬂashy. I did swap my"
Language Models are Unsupervised Multitask Learners,2019-02-14,b9091596-aaee-497e-9aac-168bdc06f61a,shoes for these ASICS Tiger Killshots and I wish I hadn’t
Language Models are Unsupervised Multitask Learners,2019-02-14,cb8326ca-28a9-48a7-909d-1ce52811fed0,"did that. The shoe itself is very comfortable, however,"
Language Models are Unsupervised Multitask Learners,2019-02-14,9c8ee8ac-806e-413d-b8e1-6fcf97934b8a,I found that wearing them in these sneakers made my
Language Models are Unsupervised Multitask Learners,2019-02-14,f09f4f54-d16f-4bbd-85ca-3e9d7aca64ab,"feet look a bit larger. I’d wear them with denim, chinos,"
Language Models are Unsupervised Multitask Learners,2019-02-14,9504a72a-ed2c-43cf-8bbb-64bdfc2b44d7,"whatever, but would never recommend wearing them"
Language Models are Unsupervised Multitask Learners,2019-02-14,9fd74ce9-851a-4e29-807c-4f336d3e72c5,alone. There are just too many variables involved.
Language Models are Unsupervised Multitask Learners,2019-02-14,27d9dd8f-95d9-4be3-9e85-f7b1f5c5de9b,Outﬁt 3: Dress
Language Models are Unsupervised Multitask Learners,2019-02-14,61cec461-fa29-4908-910f-f4aa4403abcf,This is the ﬁnal outﬁt I wore the ASICS Tiger
Language Models are Unsupervised Multitask Learners,2019-02-14,ec92a025-25b0-41b1-bd71-8a34facce965,Killshots. This version of the ASICS is a much more
Language Models are Unsupervised Multitask Learners,2019-02-14,e1960741-4aa5-4ed5-91d6-9cd893c75471,modern and streamlined version. The shoe has the most
Language Models are Unsupervised Multitask Learners,2019-02-14,e067b526-5c93-4d02-9560-f2c1ec796aa7,modern and minimal design that I can think of! With a lot
Language Models are Unsupervised Multitask Learners,2019-02-14,b5a69685-3722-41c7-969a-ad651d14e40c,"of black, these sneakers look very cool. They’ll deﬁnitely"
Language Models are Unsupervised Multitask Learners,2019-02-14,04193e7d-d1e8-4c1b-af70-874cc57617b9,be very comfortable for summer.
Language Models are Unsupervised Multitask Learners,2019-02-14,ad246604-6c34-49c6-ad95-72dbc4f07a74,"To me, wearing them with a suit is the easiest way"
Language Models are Unsupervised Multitask Learners,2019-02-14,91cfc9d4-b6b4-43b0-bd1f-d78535764d02,to have a good pair of sneaker shoes. That’s pretty much
Language Models are Unsupervised Multitask Learners,2019-02-14,3dfd374c-ea86-49e2-8370-13903822bde1,it! If
Language Models are Unsupervised Multitask Learners,2019-02-14,6f27903b-d4ca-484f-a901-a667e608f417,"Table 7. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models."
Language Models are Unsupervised Multitask Learners,2019-02-14,820d75ca-8c00-462d-9657-bc576ca855cb,"Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top- k"
Language Models are Unsupervised Multitask Learners,2019-02-14,23428554-998e-4707-b6a6-52a8ed2c25d1,random sampling with k= 40 was used for generation.
Language Models are Unsupervised Multitask Learners,2019-02-14,2c3c4ed4-353e-4c15-991f-2dbd3a73b63c,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,780aee91-6d72-4c27-ba12-cfbfa8d69589,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,5aee526f-7155-4218-b18e-2cfff9f9a024,"Some of the most glorious historical attractions in Spain date from the period of Muslim rule, including The"
Language Models are Unsupervised Multitask Learners,2019-02-14,ca167451-0426-44b2-9ab5-d3e2bfb375ed,"Mezquita, built as the Great Mosque of C ´ordoba and the Medina Azahara, also in C ´ordoba and now in ruins but"
Language Models are Unsupervised Multitask Learners,2019-02-14,193a591e-376e-4463-bb40-3b15f9e2982f,"still visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a"
Language Models are Unsupervised Multitask Learners,2019-02-14,0807d4e8-d22e-4a6e-9f69-437091006bfb,"splendid, intact palace. There are also two synagogues still standing that were built during the era of Muslim Spain:"
Language Models are Unsupervised Multitask Learners,2019-02-14,3a0560dc-2c96-4c87-9a75-b2b03177efc5,"Santa Mar ´ıa la Blanca in Toledo and the Synagogue of C ´ordoba, in the Old City."
Language Models are Unsupervised Multitask Learners,2019-02-14,02728af8-2ad6-45d7-abf7-94241013f343,Reconquista and Imperial era [ edit ]
Language Models are Unsupervised Multitask Learners,2019-02-14,ff9f68b1-372d-481a-81b3-8444f4de7371,Small model completion
Language Models are Unsupervised Multitask Learners,2019-02-14,9856033a-5624-4714-816d-0dd5aab6574a,The era of Muslim Spain is very different from that of
Language Models are Unsupervised Multitask Learners,2019-02-14,dd0c248e-e06d-451e-bfd1-94eacbd51062,"other parts of the world, with a rather extensive medieval"
Language Models are Unsupervised Multitask Learners,2019-02-14,3236d576-60bb-4271-a145-ae3e48735cce,history. While the Spanish Empire existed during the
Language Models are Unsupervised Multitask Learners,2019-02-14,e233ef3e-6e28-4a93-ae41-010e7de61af1,second half of the 9th century BC it was quite slowly
Language Models are Unsupervised Multitask Learners,2019-02-14,47c8d274-1aef-46db-a57b-8b41e837019b,"transformed to a small, secularized empire by 1150 A.D.,"
Language Models are Unsupervised Multitask Learners,2019-02-14,ee588284-a19d-4a76-8f15-914f492cf86d,in a much more prosperous and diverse state. In 1050 CE
Language Models are Unsupervised Multitask Learners,2019-02-14,a26cd131-95af-49cb-950f-997c2a81680e,"the emperor of Naples, at least partially in characterised by"
Language Models are Unsupervised Multitask Learners,2019-02-14,6e6687e6-b7b6-4c69-82d3-4cf20b53d132,a series of wars and invasions from the north-western coast
Language Models are Unsupervised Multitask Learners,2019-02-14,39e2c5b4-7cf5-4708-aeab-cbedede8dd64,"to the western coast of the region (e.g., on the Danube"
Language Models are Unsupervised Multitask Learners,2019-02-14,bffddf35-57db-4367-a7eb-f9e5c6cac5b1,River which extended all the way from Gibraltar into the
Language Models are Unsupervised Multitask Learners,2019-02-14,e3727139-7480-46a3-a51f-c68fcf482e14,"south of Spain, but was quickly destroyed on the same"
Language Models are Unsupervised Multitask Learners,2019-02-14,c6d94cdd-b978-4404-97a4-e364aed9eb86,"day by the Roman army), assumed the title of Emperor"
Language Models are Unsupervised Multitask Learners,2019-02-14,0f39fce0-e18e-43ff-ae10-b539596b41d1,of Naples. He made a large contribution over the last few
Language Models are Unsupervised Multitask Learners,2019-02-14,9a5a2be3-a754-4d90-a2ad-2833252574a8,"decades to restore and maintain the empire, the ﬁrst of"
Language Models are Unsupervised Multitask Learners,2019-02-14,3c2784c1-226c-43e0-bf6d-d99a795d2c6d,"which to the west. His successors were Ferdinand, who"
Language Models are Unsupervised Multitask Learners,2019-02-14,a56f46ff-8fbe-424e-9825-ace1fd24f56d,"took power from the Papal Throne in 1148, while the ﬁrst"
Language Models are Unsupervised Multitask Learners,2019-02-14,005bf414-cb76-4749-999a-2186588e8dab,imperial successor was King Alfred II of Sweden. The
Language Models are Unsupervised Multitask Learners,2019-02-14,588a7ddb-d387-424c-a00f-83ca44197ff5,empire was again made one of the few independent states
Language Models are Unsupervised Multitask Learners,2019-02-14,1c18dbc4-4164-4ae8-b714-5ce76ff88ad6,in the European Continent and although no more than 200
Language Models are Unsupervised Multitask Learners,2019-02-14,d29efa37-7a9b-4862-ae26-f675d9fc17ad,million people lived on its territory during this period the
Language Models are Unsupervised Multitask Learners,2019-02-14,da6a44c9-cdec-47af-a834-54fce555081b,people had much to say to the Emperor. From 1720 on the
Language Models are Unsupervised Multitask Learners,2019-02-14,7320f80f-250d-41bb-9294-caa0685f8c95,Muslims made up half the population of Spain.GPT-2 completion
Language Models are Unsupervised Multitask Learners,2019-02-14,13f83c90-e0a3-4683-a982-c606f13cf2bf,"The Reconquista (literally, ”The return to the country”)"
Language Models are Unsupervised Multitask Learners,2019-02-14,1deb15e6-2642-4de9-8676-d63c44d6fe1a,of 1092 and 1095 saw the establishment in the Iberian
Language Models are Unsupervised Multitask Learners,2019-02-14,78b6c16c-f47d-449f-9c13-cf7aa454a34a,peninsula of a united kingdom of Castile and Aragon in
Language Models are Unsupervised Multitask Learners,2019-02-14,a1dce8ec-57d1-4b9e-817e-248ae2a407ba,which a Muslim dynasty would exist. This was the last
Language Models are Unsupervised Multitask Learners,2019-02-14,882b1342-673b-41ee-b12f-5a6d6e8707ff,time in Europe where Muslim rule would be considered
Language Models are Unsupervised Multitask Learners,2019-02-14,0a2447b5-c813-4333-80ca-d956eba9a2c5,”native” and the last occasion where Christian-inﬂamed
Language Models are Unsupervised Multitask Learners,2019-02-14,027a3213-ce09-471e-b8b7-d38ce59280a6,and organized conﬂicts would make themselves known in
Language Models are Unsupervised Multitask Learners,2019-02-14,91f3eda4-6ea1-46b5-be6b-dbb436d0803b,Europe.
Language Models are Unsupervised Multitask Learners,2019-02-14,4d99259d-d8b3-4d4e-b373-725b43f2b8a9,The reign of Muslim Moors in Spain is often re-
Language Models are Unsupervised Multitask Learners,2019-02-14,99d4015f-303a-4904-ad64-4e87681697a0,"ferred to, especially among non-Muslims, as the"
Language Models are Unsupervised Multitask Learners,2019-02-14,68cbde30-c4e3-446c-b630-53c40d900d72,”Conquest of Granada” and Spain is sometimes referred
Language Models are Unsupervised Multitask Learners,2019-02-14,ddd47d88-fe2e-45fd-bd29-d522862af898,to as Spain in ”Granada”. The last years of Muslim rule
Language Models are Unsupervised Multitask Learners,2019-02-14,5e1c0ea9-23db-46e3-bd31-145d1afce11c,were followed by the Spanish Reconquista where the
Language Models are Unsupervised Multitask Learners,2019-02-14,42c1d990-bd3e-4a6d-8bba-d63e88c4bf1b,Muslims were driven or compelled out of the Iberian
Language Models are Unsupervised Multitask Learners,2019-02-14,bb1ea1c1-67a2-4f11-aa35-116b6e71c673,Peninsula with the victory of Ferdinand IX. in the Battle
Language Models are Unsupervised Multitask Learners,2019-02-14,ad8367be-05aa-4d47-ada8-c8b681427488,of Poitiers in March 1189 and the conquest of Jerusalem
Language Models are Unsupervised Multitask Learners,2019-02-14,b4ace8c6-31c8-4ffd-a42e-4d1be1179ae3,in 1187 by Saladin.
Language Models are Unsupervised Multitask Learners,2019-02-14,a89d7c6c-ec95-4e32-88be-0fdde10ec2f1,The Reconquista period in Spain came to an end
Language Models are Unsupervised Multitask Learners,2019-02-14,eb864212-3777-4877-bb50-eb4966275403,when the Moors retreated north. However the Moorish
Language Models are Unsupervised Multitask Learners,2019-02-14,a91b3213-41c0-43c4-99b0-ee11383631e8,rule would not be fully replaced by the Christian rule until
Language Models are Unsupervised Multitask Learners,2019-02-14,1a6c6338-54c1-4330-91ba-374f80ebdf14,the Reconquista era resumed when the Reconquista lasted
Language Models are Unsupervised Multitask Learners,2019-02-14,ff08d212-887c-476e-a705-191a7c79c6b7,from 1229 to 1492.
Language Models are Unsupervised Multitask Learners,2019-02-14,5e5a41f9-780b-4fae-8f8d-0ddd01bff8e0,"The last Muslim state in Spain, Granada, was con-"
Language Models are Unsupervised Multitask Learners,2019-02-14,25022cd2-f699-4758-9940-c84f637d8e9c,quered by the
Language Models are Unsupervised Multitask Learners,2019-02-14,56dd7949-0d45-470d-a5d6-94d865bd710c,"Table 8. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models."
Language Models are Unsupervised Multitask Learners,2019-02-14,f539320f-cb80-4afd-b2f4-4c63812a78f4,"Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top- k"
Language Models are Unsupervised Multitask Learners,2019-02-14,90bba377-3ae0-47fa-8888-fb4103c47256,random sampling with k= 40 was used for generation.
Language Models are Unsupervised Multitask Learners,2019-02-14,8689a05d-5bcc-4d52-b8c5-ae2c82288273,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,d4bf05c6-aeab-4a7b-8840-b03b1f0dcb77,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,2751b136-60c6-44c6-bbf4-dc182ef1bd9b,Pinky
Language Models are Unsupervised Multitask Learners,2019-02-14,03e8432f-292e-4855-897c-411dd4b8ea3f,The pink ghost’s AI is designed to ”feel” opposite of the red ghost’s behavior. Pinky actually attempts to
Language Models are Unsupervised Multitask Learners,2019-02-14,2fe880d9-a08b-4b94-b0af-d2b8128a5edc,get out in front of Pac-Man. This is accomplished by setting the target 4 tiles ahead of Pac-Man’s current location in
Language Models are Unsupervised Multitask Learners,2019-02-14,739e238e-5356-4c4c-92be-f0acc1bf2fb5,the direction that Pac-Man is travelling. One exception to this is when Pac-Man is traveling up. Due to an overﬂow
Language Models are Unsupervised Multitask Learners,2019-02-14,aa7ce907-39f9-4eec-8b1a-d90984e4d0d7,"bug in the code, the calculation includes a left offset equal to the expected up offset."
Language Models are Unsupervised Multitask Learners,2019-02-14,11d507f8-1c6f-4afe-a67d-069c92cafa44,The use of Pac-Man’s expected position allows the player to exploit the navigation code for Pinky. This was most
Language Models are Unsupervised Multitask Learners,2019-02-14,2b29c50e-1a6e-47b5-9f2d-a5a4b171fdbb,"often seen in situations where Pac-Man and Pinky are in close proximity to each other. For example, if they were"
Language Models are Unsupervised Multitask Learners,2019-02-14,04816be6-6ce4-449a-8ca3-c929c624a844,"headed toward one another, once they are less than 4 tiles apart, the target tile is now behind Pinky. If he encounters"
Language Models are Unsupervised Multitask Learners,2019-02-14,6bd6abec-bdb3-4484-8fd5-a492a75a3f35,"an intersection while in this conﬁguration, he will attempt to double-back (by turning) in order to reach the target tile."
Language Models are Unsupervised Multitask Learners,2019-02-14,6fa84efc-e8fb-4f4f-89c6-627222c3ab94,Player’s would often perform what was referred to as a ”head fake” by changing directions rapidly just
Language Models are Unsupervised Multitask Learners,2019-02-14,989df841-256c-42e1-9044-f3b4041a40fc,as Pinky was approaching an intersection. By displacing the target tile just as Pinky
Language Models are Unsupervised Multitask Learners,2019-02-14,6e14e21a-4e3c-4f1e-9f53-3be309c1d604,Small model completion
Language Models are Unsupervised Multitask Learners,2019-02-14,29d994a2-d0b0-47e7-aa17-3f0876bd0534,"was coming to the left, they would get the ”correct”"
Language Models are Unsupervised Multitask Learners,2019-02-14,7485b4e0-62d5-48ff-8cb8-e6199a7e5ed7,direction.
Language Models are Unsupervised Multitask Learners,2019-02-14,aa6e2600-94fe-4cda-aed5-2b2b415bb2af,If the location is actually farther from what Pac-
Language Models are Unsupervised Multitask Learners,2019-02-14,c7b71052-0e09-402a-94e6-10fe0435543c,"Man was at, the player’s will not respond to the ghost’s"
Language Models are Unsupervised Multitask Learners,2019-02-14,1c4db5bb-bba5-4d8d-9a7f-037d2ff52c13,"movement, and the ghost will instead return to the player’s"
Language Models are Unsupervised Multitask Learners,2019-02-14,93e91d64-ad75-45a5-90d8-173f7df06d5b,current location. This is because Pac-Man does not move
Language Models are Unsupervised Multitask Learners,2019-02-14,3bf0c594-93c1-4521-a6c5-7ac416a32e77,by turning.
Language Models are Unsupervised Multitask Learners,2019-02-14,0faa23b0-ddbe-46ac-9627-931af2e3823a,The ghost will have to go to another location of
Language Models are Unsupervised Multitask Learners,2019-02-14,8dab8a4c-6b06-40e7-8102-86e06e1c958d,"some kind to return where he began, and this is achieved"
Language Models are Unsupervised Multitask Learners,2019-02-14,ce7941af-ad46-4e08-9b7e-679f14c740bd,"in many ways. It will often simply leave the location, as"
Language Models are Unsupervised Multitask Learners,2019-02-14,a6ccb561-4dda-4962-88cf-fb835329c2a5,"for example if Pac-Man did jump off a cliff, he would"
Language Models are Unsupervised Multitask Learners,2019-02-14,d0c43dac-21d6-44cf-b131-518bc925f555,return to the spot where he was before vanishing into
Language Models are Unsupervised Multitask Learners,2019-02-14,4f0075c1-9ebb-4acf-b47c-9254063793c9,space. There is a good number of possibilities these ghost
Language Models are Unsupervised Multitask Learners,2019-02-14,2cf822d7-1e69-4237-8742-67023c9c3c19,"paths may be possible, but the most obvious ones are if"
Language Models are Unsupervised Multitask Learners,2019-02-14,73a04c51-c6d9-4c2d-bdb1-b31681665fa9,"the player wants to avoid getting caught up in the game,"
Language Models are Unsupervised Multitask Learners,2019-02-14,aac8c45c-4a88-45a3-9a4a-1c124ca179f9,with a goal as easy as keeping Pac-Man in the game for
Language Models are Unsupervised Multitask Learners,2019-02-14,fa530572-7819-468e-bad8-7fde576e53c0,1/3 as long as he was.
Language Models are Unsupervised Multitask Learners,2019-02-14,720a94a7-551d-4a71-a1cd-24c0ec09918b,Chasing
Language Models are Unsupervised Multitask Learners,2019-02-14,eb3cabdd-3967-4e2c-813a-b295a2a8d232,"In Pac-Man’s case, this would be similar to how"
Language Models are Unsupervised Multitask Learners,2019-02-14,222e6dce-bda5-40f5-a82b-bace1bad19b2,Pac-Man can ”catch up” in a ﬁght.
Language Models are Unsupervised Multitask Learners,2019-02-14,5820bc22-3e52-494b-a238-0d2622949b7d,A ghost could chase through enemies. This is use-
Language Models are Unsupervised Multitask Learners,2019-02-14,0d9bb832-7736-42bf-8dbf-39d2b87af31b,"ful for many reasons, both for Pac-Man and his AI."
Language Models are Unsupervised Multitask Learners,2019-02-14,7a2a8397-51d3-4ea8-aa92-ce456a21a6fb,"Unlike most Ghost games, this ghost simply travels in the"
Language Models are Unsupervised Multitask Learners,2019-02-14,6258f048-48d1-4294-8baf-ca2c549d7099,direction fromGPT-2 completion
Language Models are Unsupervised Multitask Learners,2019-02-14,ab50334f-5dfb-40eb-a0c2-00fcfddd7d93,"was about to cross the intersection, Pac-Man would be"
Language Models are Unsupervised Multitask Learners,2019-02-14,68cd18c3-528f-4389-9ae2-733c1545052e,able to dodge the ghosts’ projectiles and return to the safe
Language Models are Unsupervised Multitask Learners,2019-02-14,a63c2096-ffb9-48e1-a72e-3ab631e2f723,location of the safe house.
Language Models are Unsupervised Multitask Learners,2019-02-14,e2007620-c074-4131-9435-ef433726d9b9,Buster
Language Models are Unsupervised Multitask Learners,2019-02-14,f10cf307-d88d-487f-a37f-5d3065042fdc,The yellow ghost’s AI is speciﬁcally designed to
Language Models are Unsupervised Multitask Learners,2019-02-14,f299ed37-499e-4afd-9bf4-50bd978ab87c,try to avoid Pac-Man from leaving the safe house. The
Language Models are Unsupervised Multitask Learners,2019-02-14,a354edb3-b514-4887-9b6e-a98c3898999f,player must press the SPACE bar to reveal the safe
Language Models are Unsupervised Multitask Learners,2019-02-14,7c70d2bf-ec8c-4902-9f7e-afad0d100ef2,"house’s doors, then go into them to ﬁnd the yellow ghost"
Language Models are Unsupervised Multitask Learners,2019-02-14,217a995d-fee2-442c-9efe-a5a3b1e004c9,hiding in the next room.
Language Models are Unsupervised Multitask Learners,2019-02-14,d5b9b37d-6983-4603-98be-b0d288b19dc8,The player must also note that the ghost will not
Language Models are Unsupervised Multitask Learners,2019-02-14,f1486b64-c8f0-4ea9-ae61-9ea715c49c63,attempt to exit the safe house if it sees Pac-Man already
Language Models are Unsupervised Multitask Learners,2019-02-14,48a63c03-c173-4c09-b60c-0af231cd0bfa,"there. In fact, the ghost will not attempt an exit in the ﬁrst"
Language Models are Unsupervised Multitask Learners,2019-02-14,4a4eb304-0a27-4197-b594-30ba1027f9fd,place unless they’re already in the safe house.
Language Models are Unsupervised Multitask Learners,2019-02-14,32d72286-fa91-482f-bfa6-ff6c72ecb022,These two behaviors are based on the fact that the
Language Models are Unsupervised Multitask Learners,2019-02-14,8568a76f-6287-4ff4-91b9-564d12c8a1e2,player does not always attempt to reach the safe house
Language Models are Unsupervised Multitask Learners,2019-02-14,3aa1b3c3-9480-417c-b9ec-3909af515d99,before Pac-Man (or Blinky).
Language Models are Unsupervised Multitask Learners,2019-02-14,40e32953-586b-46e3-8ed8-db9d8fcf1cd9,The player must also beware of this glitch if a
Language Models are Unsupervised Multitask Learners,2019-02-14,054b8752-f7be-4698-8917-02900414831e,ghost (or another player) is already in the safe house
Language Models are Unsupervised Multitask Learners,2019-02-14,28256b5b-e12f-40c2-ad20-26844d2048bc,and attempting to get out as soon as Pac-Man or Blinky
Language Models are Unsupervised Multitask Learners,2019-02-14,c963dfb5-ca01-4816-934c-2c8d92d0123e,"is gone. At this point, the ghost will attempt to exit"
Language Models are Unsupervised Multitask Learners,2019-02-14,2d9d2756-6825-49aa-b429-e30b2e306f2e,the safe house in a similar manner as it is in ”Chase” mode.
Language Models are Unsupervised Multitask Learners,2019-02-14,41656da4-5250-4706-9029-24d751b04ed0,Pac-Man
Language Models are Unsupervised Multitask Learners,2019-02-14,99d3a4c9-b938-4052-85d4-09dfd303bf92,There are some ghosts that follow Pac-Man. These ghosts
Language Models are Unsupervised Multitask Learners,2019-02-14,1433d0b4-09a4-4030-94da-22241a0d6a1f,are
Language Models are Unsupervised Multitask Learners,2019-02-14,fca76cf1-b2cc-47ae-a6a6-71d032dbe436,"Table 9. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models."
Language Models are Unsupervised Multitask Learners,2019-02-14,5700399a-e785-4c1c-ac80-a311a99af1e0,"Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top- k"
Language Models are Unsupervised Multitask Learners,2019-02-14,72402766-2adf-48e0-8e76-63ef71ec39d1,random sampling with k= 40 was used for generation.
Language Models are Unsupervised Multitask Learners,2019-02-14,eae1808f-bb36-428c-a1bb-379f6cd3dcd5,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,76e56b55-1361-4bb1-9409-86ed9b2b7adc,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,4c36ed91-3b37-46a1-acaa-8bc1da6ea1b4,Corporal Michael P. Goeldin was an unskilled laborer from Ireland when he enlisted in Company A in November
Language Models are Unsupervised Multitask Learners,2019-02-14,8e0a0840-094e-41a0-998e-6bdc41a9a4cc,"1860. Goldein survived the war. Corporal Patrick O’Neal, also from Ireland, ﬁrst enlisted in 1854 and served with"
Language Models are Unsupervised Multitask Learners,2019-02-14,24b0cef6-91fd-4dc2-8ef0-a255302a3570,"Company L, 3d U.S. Artillery, in Oregon. He returned to the East Coast and enlisted in the company in 1860."
Language Models are Unsupervised Multitask Learners,2019-02-14,3663eb90-668a-4178-b127-cb599af42f69,"O’Neal served until 1874, when he was named superintendent of the National Cemetery at Willets Point, New York."
Language Models are Unsupervised Multitask Learners,2019-02-14,f08516ae-44c0-4179-9d14-69d292ac0a9d,"Corporal Benjamin Browne was a shoemaker from Orange County, New York. In August 1862, he enlisted in the"
Language Models are Unsupervised Multitask Learners,2019-02-14,579019f6-19dc-4ede-a3c9-fdc04124f054,"newly formed 124th New York V olunteers, and was one of sixty-one men who transferred into Company A that"
Language Models are Unsupervised Multitask Learners,2019-02-14,5594f6af-6e3a-4c32-b1e5-505e71f9c814,October. Browne reenlisted in the company in February 1864 while it was camped at Brandy Station. He returned
Language Models are Unsupervised Multitask Learners,2019-02-14,42501caa-dda3-4ece-aa6f-38523d372b9b,to civilian life after completing his enlistment in 1867.
Language Models are Unsupervised Multitask Learners,2019-02-14,7f1f7f95-dce8-49c6-9a14-3155dceb222a,"On 10 June, Artiﬁcer William Collins was promoted to corporal, probably to ﬁll a combat leadership"
Language Models are Unsupervised Multitask Learners,2019-02-14,5c5b677e-ce52-4c82-ae31-ab0425bb735d,void for the crossing of the James River. Collins’s service record does not reﬂect the qualities he demonstrated to
Language Models are Unsupervised Multitask Learners,2019-02-14,7d25f52d-83a1-427a-9e9a-3801e634c19a,"earn this promotion, but he had obviously overcome some serious problems. Born in Sacketts Harbor, New York,"
Language Models are Unsupervised Multitask Learners,2019-02-14,0881e6fa-baa2-41d0-a468-e26d3c6bab49,"Collins enlisted in the company in December 1853 at the age of twenty-two, and reenlisted in December 1858. Just"
Language Models are Unsupervised Multitask Learners,2019-02-14,56a412b2-469e-4336-b0a5-8f0fbf3f8a49,"a month before the war began in April 1861, Collins went ”over the hill” and was not caught until three years later."
Language Models are Unsupervised Multitask Learners,2019-02-14,32ae1dfb-63de-4848-9725-49331e13fa6f,"Returned to the company on 22 March 1864, he was tried"
Language Models are Unsupervised Multitask Learners,2019-02-14,15f67453-c9f8-452c-b2cf-34df9d1cf40d,Small model completion
Language Models are Unsupervised Multitask Learners,2019-02-14,2a68a2c2-d303-4222-8e35-677790be0450,on military charges and was discharged from the U.S.
Language Models are Unsupervised Multitask Learners,2019-02-14,c22f89b9-e6dd-429b-a7ff-9fdd5999ca1e,"Military. He returned to the company, and was assigned"
Language Models are Unsupervised Multitask Learners,2019-02-14,bb300851-4aea-4c21-af25-7694d2c483ff,to Division of Internal Affairs during the Civil War.
Language Models are Unsupervised Multitask Learners,2019-02-14,6cc7ec3c-8456-4e3f-b564-091743767b6e,"As an engineer, Collins also had training in engi-"
Language Models are Unsupervised Multitask Learners,2019-02-14,0fa1ad90-2d31-46c7-a8f8-b600f44d7c42,neering from his native New York. After graduate studies
Language Models are Unsupervised Multitask Learners,2019-02-14,c440d131-5565-4e2d-9375-fc1a554a1a8b,"in the Union College of Engineering, he was employed"
Language Models are Unsupervised Multitask Learners,2019-02-14,7accfa35-e445-4213-b45a-d68d4c943f5f,in the United States Army for two years at the same
Language Models are Unsupervised Multitask Learners,2019-02-14,1740a905-061b-4c26-b306-dad13e6ab715,time. There he worked under Captain Daniel Pfeiffer
Language Models are Unsupervised Multitask Learners,2019-02-14,50c2362d-a3ac-4e48-86b8-5e00234b5391,"and was promoted to colonel in May 1870, three years"
Language Models are Unsupervised Multitask Learners,2019-02-14,1adb6b34-c4b4-4f7e-a951-a081225dfb1b,after Captain Pfeiffer’s enlistment. Collins returned
Language Models are Unsupervised Multitask Learners,2019-02-14,9fbfbf5b-17f9-4c33-848e-c17e2921d5ba,home to England after completing his ﬁrst year in the
Language Models are Unsupervised Multitask Learners,2019-02-14,f48d077d-239c-4c4c-9028-f22fc7ef97a6,First Division to serve the next two years in the Army
Language Models are Unsupervised Multitask Learners,2019-02-14,31a5b716-f068-4039-b814-410b4f6f903e,"Corps of Engineers, and by his second year, he began to"
Language Models are Unsupervised Multitask Learners,2019-02-14,69c8f089-dad5-4143-90df-99d5e2378f57,"experience his service duty.¡—endoftext—¿MOSCOW,"
Language Models are Unsupervised Multitask Learners,2019-02-14,00e46211-4414-48ef-aeec-5406c478f412,July 5 (Reuters) - U.K.-based Russian state media and
Language Models are Unsupervised Multitask Learners,2019-02-14,5d9f4d66-ca78-438b-b9c4-53bd2572ce4a,government sources and experts say Moscow has sought
Language Models are Unsupervised Multitask Learners,2019-02-14,9da3d555-dde7-4a8e-87e4-5a93e093f441,to downplay the ”military buildup” of the Syrian oppo-
Language Models are Unsupervised Multitask Learners,2019-02-14,6870a611-99a4-4f40-bd09-c22cf0f8ef3b,sition while maintaining support for President Donald
Language Models are Unsupervised Multitask Learners,2019-02-14,6a4724ae-192a-4e73-805e-07e1a7150f13,Trump’s plan to build a wall along the U.S.-Mexico
Language Models are Unsupervised Multitask Learners,2019-02-14,81c7f8c8-7a33-476f-b90a-c6ec13215751,"border, calling it a ”military coup”."
Language Models are Unsupervised Multitask Learners,2019-02-14,3ca13ca1-70be-4500-8fa6-52d3d35b952d,U.S.-led Syria war veteran and Syria envoy Michael
Language Models are Unsupervised Multitask Learners,2019-02-14,66044e0e-e2a8-4a04-bec9-33a6f71428f9,"Flynn (R), right, gestures with U.S.-based Syrian political"
Language Models are Unsupervised Multitask Learners,2019-02-14,6a63344e-d167-4295-9c4e-21eddc64146f,"analyst Ahmet Koryat, withGPT-2 completion"
Language Models are Unsupervised Multitask Learners,2019-02-14,86fc633d-5fd3-41e2-a9f8-b5ea4200b8fd,"in June of that year. Sentenced to be shot for desertion, he"
Language Models are Unsupervised Multitask Learners,2019-02-14,04cf2a24-5643-41b8-a967-dc0ab9b34113,remained on active duty.
Language Models are Unsupervised Multitask Learners,2019-02-14,bb93bbca-1124-4d77-a1c9-405fa4476160,Captain James J. Dall was born in Virginia in 1829 during
Language Models are Unsupervised Multitask Learners,2019-02-14,669d561f-b2ba-4c7e-8504-c97f1851f540,the Revolution and joined his father in the Army in 1836.
Language Models are Unsupervised Multitask Learners,2019-02-14,2f8c546d-9b75-4c33-ab4f-5c8c67d751b9,"He served with the 2d Virginia Infantry until 1845, when"
Language Models are Unsupervised Multitask Learners,2019-02-14,0ff25aa8-1cf8-4126-baee-15f0966502e1,"he went to the Western Reserve Regiment; and, on his"
Language Models are Unsupervised Multitask Learners,2019-02-14,c5c44da8-0955-4e6d-9ae8-1fd5fd6cf267,"discharge, he reenlisted with Company I, 25th New York"
Language Models are Unsupervised Multitask Learners,2019-02-14,4175aa83-ce7f-49b9-8c4f-96c970a0a5d8,"V olunteers (which in 1845 numbered about 4,500 men)."
Language Models are Unsupervised Multitask Learners,2019-02-14,4e260a7e-a37d-4792-8208-e718a7664c3d,"One of his brothers, James, served with his company"
Language Models are Unsupervised Multitask Learners,2019-02-14,a4f2f57a-0ecb-426c-8e18-9fadc879fc32,in the same capacity in the same brigade. While the
Language Models are Unsupervised Multitask Learners,2019-02-14,c5976c79-8a15-4a95-9cd8-f21a89ae0c61,"regiment remained in Connecticut through 1846, Captain"
Language Models are Unsupervised Multitask Learners,2019-02-14,44536963-89fb-45ed-8b53-c2658fcffe2a,Dall served with Company I at various times before
Language Models are Unsupervised Multitask Learners,2019-02-14,0be1fd49-42e8-49ab-92e6-4cb0b93cef5c,"his discharge. A veteran of the Mexican War, he had"
Language Models are Unsupervised Multitask Learners,2019-02-14,fc1d8214-d31d-4bfe-9437-610df57678bc,served two years on the staff of Brigadier General John J.
Language Models are Unsupervised Multitask Learners,2019-02-14,171a462c-1c53-4e5e-9e1e-0474e9b8c184,Sullivan at San Antonio. During the Peninsula Campaign
Language Models are Unsupervised Multitask Learners,2019-02-14,d45f7b35-2da7-4950-93b1-d9338c0280b4,"of 1863, he commanded a company of twenty-three and"
Language Models are Unsupervised Multitask Learners,2019-02-14,39ae4cef-c12a-43d9-b4e1-13a1492ca34f,was in charge of the battery of ﬁfty-four heavy mortars
Language Models are Unsupervised Multitask Learners,2019-02-14,a039243f-defa-4a05-b2b9-d227f8cdc4df,"and the gunboat Pinta of the Battery, a gunboat and"
Language Models are Unsupervised Multitask Learners,2019-02-14,1b22f847-d68e-4849-a0de-69a9b4d1fc61,several other boats. Captain Dall was on active duty with
Language Models are Unsupervised Multitask Learners,2019-02-14,cb49d3b0-34fa-4759-9d3c-d70388c45d4b,"Company I in the fall of 1865. Two years later, he was"
Language Models are Unsupervised Multitask Learners,2019-02-14,ff382a23-3808-418b-952e-b50e4492a368,promoted to second lieutenant.
Language Models are Unsupervised Multitask Learners,2019-02-14,9f82e2af-2346-44a3-92ee-6bb5dabfe498,Captain John A. O’
Language Models are Unsupervised Multitask Learners,2019-02-14,71b3c327-610e-4b25-a8ba-8d6eea2ac626,"Table 10. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models."
Language Models are Unsupervised Multitask Learners,2019-02-14,013fd89b-d3d4-4b93-b52f-93331d17b117,"Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top- k"
Language Models are Unsupervised Multitask Learners,2019-02-14,2083b887-f390-4aec-ae8b-51f0a84a5ac6,random sampling with k= 40 was used for generation.
Language Models are Unsupervised Multitask Learners,2019-02-14,691d1af4-b29d-4c72-913f-80fb2ea2dd31,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,084f71ba-5bf6-4a23-bf50-e60e846774b2,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,f40be360-0c47-4c02-93b0-178f9242ab62,"Xmonad, Xresources, and Fonts"
Language Models are Unsupervised Multitask Learners,2019-02-14,1840786c-3bef-4e4c-98a0-93d9b22ebc9e,"Xresources can be an absolute pain (they were for me). So, this a bit of help:"
Language Models are Unsupervised Multitask Learners,2019-02-14,69b5627a-b30e-4ca3-9d75-a5e21508359f,"First of all, the way that fonts are generally handled in Xmonad is through XFT . Xft is the X FreeType"
Language Models are Unsupervised Multitask Learners,2019-02-14,4783d6a2-6d97-4b03-ae8b-a8baa4323114,Interface library and is a library that handles the actual rendering of fonts in Xmonad/urxvt (Xresources speciﬁes
Language Models are Unsupervised Multitask Learners,2019-02-14,a38c2844-782a-4093-bfc5-e24c7ea3aab0,a bunch of resources to things that launch under the X window manager. Settings for individual programs are
Language Models are Unsupervised Multitask Learners,2019-02-14,9b1e186e-c489-4ca6-808d-e7a191b30a17,speciﬁed in Xresources like so: Urxvt.background:background settins here ).
Language Models are Unsupervised Multitask Learners,2019-02-14,28c7df91-fd43-4f73-90c4-4c748b055515,"Anyway, fonts are set using this syntax in Xresources :"
Language Models are Unsupervised Multitask Learners,2019-02-14,d62c3e7f-e739-45b4-9562-e5906d62214f,"urxvt*font:xft:your font name goes here:size=the size of the font goes here,xft:fall-back font 1 goes here"
Language Models are Unsupervised Multitask Learners,2019-02-14,553ce011-8f47-4989-9ff3-1698a9d7fd10,"So, thats the most basic part of fonts in urxvt speciﬁed via Xresources."
Language Models are Unsupervised Multitask Learners,2019-02-14,30291888-2e5f-401f-9dda-3f128630f35a,Fixing /.ssh/ Permissions in Cygwin
Language Models are Unsupervised Multitask Learners,2019-02-14,c2afb00c-2c69-489c-aa79-a2e732227193,"In Cygwin on Windows I found that I could not set the permissions of my /.ssh/ folder to be 0600, as is"
Language Models are Unsupervised Multitask Learners,2019-02-14,db75a9c0-5142-48c0-a732-57fa8772e164,"required for ssh to allow you to use keys. The symptom I had was that I no matter what I did, it always modﬁed the"
Language Models are Unsupervised Multitask Learners,2019-02-14,60981716-d142-4103-bd90-c23098ff7b13,owner and the group
Language Models are Unsupervised Multitask Learners,2019-02-14,e46f2c23-e9dd-46f0-b422-7d40ba247a4b,Small model completion
Language Models are Unsupervised Multitask Learners,2019-02-14,b6291de2-5a8f-4b53-abf0-48bc4a6c32dd,"was set to /.ssh/ , which was the way things were. This"
Language Models are Unsupervised Multitask Learners,2019-02-14,ba622253-d73c-41ac-9470-c1aa7cf533a5,could not be ﬁxed completely. I found I could make that
Language Models are Unsupervised Multitask Learners,2019-02-14,f10266d0-233f-4612-bc08-359e27580b40,much more workable with just typing the username with
Language Models are Unsupervised Multitask Learners,2019-02-14,56c5bde5-e40c-468b-8f78-c2b0dc46069d,your mouse then trying to set permissions to be ”0600”
Language Models are Unsupervised Multitask Learners,2019-02-14,672bca1d-d9fc-46d4-b48d-6fb3058006ce,"for the group, which is what my system is now conﬁgured"
Language Models are Unsupervised Multitask Learners,2019-02-14,edee711e-ae8b-4d4d-8513-df06d5c76c38,to do.
Language Models are Unsupervised Multitask Learners,2019-02-14,e374ea4f-0e16-464a-b897-39f4ce2ade5f,"However, as of Cygwin 7 on Linux I have found"
Language Models are Unsupervised Multitask Learners,2019-02-14,ef2bb828-9030-4a65-bac7-b8d621574c71,that doing so is now ”unnecessary” for my system! I also
Language Models are Unsupervised Multitask Learners,2019-02-14,b5a658b8-6ade-42bd-9ef6-53afc0ab4d45,installed a package called chroot. It does that by allowing
Language Models are Unsupervised Multitask Learners,2019-02-14,c9e68dfe-c285-40f3-ba93-4364cf6aac1b,you to install/upgrade to your system directory.
Language Models are Unsupervised Multitask Learners,2019-02-14,2b07cde5-59a7-4c4f-8001-64d9ebddd58b,"So, as of Windows 8 and up, installing that can re-"
Language Models are Unsupervised Multitask Learners,2019-02-14,e655fc9f-c91f-4338-be88-c68eff5dd037,"quire chroot, so I tried and tried and found that it wasn’t"
Language Models are Unsupervised Multitask Learners,2019-02-14,f671191f-e96d-45ac-8b98-9f5be7af1626,so bad when installing the package on Linux and then
Language Models are Unsupervised Multitask Learners,2019-02-14,d5570891-3e83-4a25-bdfe-bbb21f88bea3,"uninstalling it. As of my last write-up on Cygwin, the"
Language Models are Unsupervised Multitask Learners,2019-02-14,9b7ceece-1a56-4766-b4cc-0f972ae9910d,"script just does its job (the sudo commands do the rest), if"
Language Models are Unsupervised Multitask Learners,2019-02-14,d15220bd-3c06-4944-9113-42ff9213d9bd,you try it you will have to log the change.
Language Models are Unsupervised Multitask Learners,2019-02-14,c92abc83-664e-4e7b-9207-c34fb96ac461,"If that wasn’t confusing enough, this is what I tried and"
Language Models are Unsupervised Multitask Learners,2019-02-14,8f2eabe4-9a80-4cbe-bbc2-bef10255d567,just went back to my /.ssh/ directory and the chroot script.
Language Models are Unsupervised Multitask Learners,2019-02-14,1b757e4a-93b5-41ab-9e84-d0402e2f50fe,I found I was ﬁnally running out of permissions (because
Language Models are Unsupervised Multitask Learners,2019-02-14,78271a4b-04b8-4b2b-a056-23891bab556b,i forgot to create a symlink to ssh from the ﬁle system andGPT-2 completion
Language Models are Unsupervised Multitask Learners,2019-02-14,497bea35-d5db-4504-8ac7-fa70136cbe9b,", making these ﬁles read-only. I tried just starting ssh"
Language Models are Unsupervised Multitask Learners,2019-02-14,36958402-0a79-4184-9436-afdc39a2994c,and using a program like passwd which modiﬁes the
Language Models are Unsupervised Multitask Learners,2019-02-14,b56ec548-9c40-4849-8f97-474a7192eae7,"owner/group to 0644 permissions, but it didn’t work. It"
Language Models are Unsupervised Multitask Learners,2019-02-14,dd82bc09-75e9-4e83-9436-eb4a3b238327,"was annoying but not nearly as bad as it could be, just"
Language Models are Unsupervised Multitask Learners,2019-02-14,e43a00d9-20c2-4bd8-b21f-892b4b6dcd4d,annoying to have to ﬁddle with permissions.
Language Models are Unsupervised Multitask Learners,2019-02-14,ad3fc928-0653-4506-ba3d-e6d0feb4e9ff,So I decided to do this as the reverse:
Language Models are Unsupervised Multitask Learners,2019-02-14,72dc1e9a-c84e-4ec9-90db-35c996b54a57,"For each .ssh folder in your Cygwin directory, set"
Language Models are Unsupervised Multitask Learners,2019-02-14,1e6a8430-c154-4a7c-946a-b48041d91de6,the permission of each ﬁle and directory to 2644. Then
Language Models are Unsupervised Multitask Learners,2019-02-14,5d91cff5-8423-4286-a191-5017917d6ccb,start ssh with: ssh-add /.ssh/
Language Models are Unsupervised Multitask Learners,2019-02-14,1ac86303-6894-443d-9f0c-5fd64fab733e,"The ﬁrst time you connect, your permissions of"
Language Models are Unsupervised Multitask Learners,2019-02-14,80b6dad6-b1d7-405a-b2ce-cc8827f6eaed,the /.ssh folder that you just had in Cygwin will be
Language Models are Unsupervised Multitask Learners,2019-02-14,16dc19c2-7e13-4a98-84cb-fe237ce00975,"set to 0600. Then every time you connect again, your"
Language Models are Unsupervised Multitask Learners,2019-02-14,5f55c59c-98b3-4cc2-99d0-deb9e8fca5b2,permissions will change to 0644.
Language Models are Unsupervised Multitask Learners,2019-02-14,1a5edfaa-b57b-4853-88c0-341f62a17118,Makeﬁle for Xresources as used in /.xsessionrc
Language Models are Unsupervised Multitask Learners,2019-02-14,0db1fd5f-8f70-48e8-b69a-e0f8bdfe35fd,These are made in my .xsessionrc :
Language Models are Unsupervised Multitask Learners,2019-02-14,5387ac40-7d7d-458f-bd06-1c530df8beee,.xproﬁle: .xsession-errors: color=””
Language Models are Unsupervised Multitask Learners,2019-02-14,16cfbcb2-b3ba-41c2-b960-bccea0d4fa12,For example:
Language Models are Unsupervised Multitask Learners,2019-02-14,f8284bfd-ac19-4cbd-b8ef-3b24cc3b0d14,.xproﬁle: color=blue
Language Models are Unsupervised Multitask Learners,2019-02-14,1cbd70ca-d8cb-42f7-b213-8ffd713fc3f0,"As the colors are stored in a separate ﬁle, the color in your"
Language Models are Unsupervised Multitask Learners,2019-02-14,cd822fb6-f05f-41fc-a49a-6d65717ce658,/.xsession-errors will look like cyan instead of blue.
Language Models are Unsupervised Multitask Learners,2019-02-14,418705aa-2d01-4022-8a9c-4a75d9f429ed,"This is mostly because, as of now, only the color"
Language Models are Unsupervised Multitask Learners,2019-02-14,65e16772-0855-4607-8aa2-1b976bd0fb81,is assigned to your
Language Models are Unsupervised Multitask Learners,2019-02-14,6e3f9d41-d0fe-49b5-bae3-f99130d7e91f,"Table 11. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models."
Language Models are Unsupervised Multitask Learners,2019-02-14,81f812a4-1497-4682-8596-8fb45bff879e,"Contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top- k"
Language Models are Unsupervised Multitask Learners,2019-02-14,4fe39abd-1a0b-46d5-bb29-37eb45cb8145,random sampling with k= 40 was used for generation.
Language Models are Unsupervised Multitask Learners,2019-02-14,d472f4b7-463f-4c3b-8331-40d004fda70e,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,3b4e9d12-ffca-4ed1-b7bf-cf42b681f2a9,Context (WebText test)
Language Models are Unsupervised Multitask Learners,2019-02-14,f6ba4031-06a7-457a-99c8-44feb2522e1a,"Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some of"
Language Models are Unsupervised Multitask Learners,2019-02-14,5182bfcd-101b-4cfa-aee9-f19d936e5f3a,those in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of
Language Models are Unsupervised Multitask Learners,2019-02-14,212cd46d-02c2-4a4f-a7c4-1b2c0cd5f5d5,"those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate"
Language Models are Unsupervised Multitask Learners,2019-02-14,7aae7925-efac-414b-85cd-14a7e44465b0,"chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the ﬁnished"
Language Models are Unsupervised Multitask Learners,2019-02-14,ce877bc9-3188-42df-8fcd-13addcaf2d15,"product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And"
Language Models are Unsupervised Multitask Learners,2019-02-14,8a6fe9fa-8d47-4203-a9a7-86aa5281848d,don’t be afraid to use a heavy hand. Texture = good.
Language Models are Unsupervised Multitask Learners,2019-02-14,e937eb6e-2cf0-4290-a67a-0f6be693ae04,"If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this"
Language Models are Unsupervised Multitask Learners,2019-02-14,62bf1422-481b-463c-a6e5-8455fcd3a3f0,GPT-2 Completion 1
Language Models are Unsupervised Multitask Learners,2019-02-14,158057a5-a512-4b3f-b78b-fbb24039b39a,"recipe. I made the longer version,"
Language Models are Unsupervised Multitask Learners,2019-02-14,d736f2c6-70c3-4e16-9bf7-5273ee6693e8,then used the pan for a layer of
Language Models are Unsupervised Multitask Learners,2019-02-14,3c25661d-8ffb-41f2-9260-ee957ef87c9a,buttercream frosting and stuck 1/3
Language Models are Unsupervised Multitask Learners,2019-02-14,a6211bf3-fa1a-4fb0-acb7-bd3b2f36cd2d,of it into the fridge for a frosting
Language Models are Unsupervised Multitask Learners,2019-02-14,373741e5-85f0-468e-9dbf-8f1963af7c99,topper. Just make sure you have all
Language Models are Unsupervised Multitask Learners,2019-02-14,79c209cd-f5d3-40b1-b680-6e6c4fa30140,of this frosting prepared earlier than
Language Models are Unsupervised Multitask Learners,2019-02-14,9cbbd6ad-3f97-4e64-8fa5-b4533897e4bf,you make this cake and you won’t
Language Models are Unsupervised Multitask Learners,2019-02-14,b0363670-8745-4500-8fce-f45862726bee,have to wait on it as your egg whites
Language Models are Unsupervised Multitask Learners,2019-02-14,e1b3d3d2-2753-4a76-a979-6e2d643f8514,sit on the bottom of your mixing
Language Models are Unsupervised Multitask Learners,2019-02-14,8d8442c5-d5d9-4117-a89d-73d96f2cef64,bowl.
Language Models are Unsupervised Multitask Learners,2019-02-14,d28cb103-1fb7-4777-8e2a-5f1a3e1eaa36,You want the egg whites whipped
Language Models are Unsupervised Multitask Learners,2019-02-14,58d694ef-cd5e-48be-984a-a9fce4484c3e,to stiff peaks? Then put your cake
Language Models are Unsupervised Multitask Learners,2019-02-14,c8bab5ec-73c8-4b77-92c7-a77acaa354c3,dish over a pot of simmering water
Language Models are Unsupervised Multitask Learners,2019-02-14,edee8a40-fdc3-4259-b1f6-f70cca137044,and heat it up until it’s just barely
Language Models are Unsupervised Multitask Learners,2019-02-14,604d248d-ec40-4c53-8e4d-215681fa2ce6,warm. It doesn’t take more than 3-5
Language Models are Unsupervised Multitask Learners,2019-02-14,ccc6713c-20a5-4f00-9e7a-297cfa96173b,minutes; just enough to get it startedGPT-2 Completion 2
Language Models are Unsupervised Multitask Learners,2019-02-14,725b631a-0c90-469a-9983-59d846b9f468,"recipe. If you do, and you know"
Language Models are Unsupervised Multitask Learners,2019-02-14,d86911fb-62bb-4b94-84b7-ba005f9db705,"how to make that cake, then your"
Language Models are Unsupervised Multitask Learners,2019-02-14,f96ad94b-63be-4283-b161-d83a24d86cea,life will never be the same.
Language Models are Unsupervised Multitask Learners,2019-02-14,1707a744-0cab-48b3-9bd5-e15e31f7b536,I have two very simple rules
Language Models are Unsupervised Multitask Learners,2019-02-14,c3d3ebf2-541e-41de-a491-8d50be27e466,when it comes to cookie crumbs
Language Models are Unsupervised Multitask Learners,2019-02-14,c31aba1d-96db-4537-b6ab-69927bc82cf6,or chocolate chips. 1) When I buy
Language Models are Unsupervised Multitask Learners,2019-02-14,e3d9b161-2b11-4948-8ae7-155894c8e27e,"raw sugar, I do not use it until it’s"
Language Models are Unsupervised Multitask Learners,2019-02-14,75b597f3-d2de-4205-9b95-cf19a0a9ec28,cooled down and has at least half a
Language Models are Unsupervised Multitask Learners,2019-02-14,53466b8f-dffb-445a-98e4-bfb198fa2468,teaspoon of sugar on it. That’s my
Language Models are Unsupervised Multitask Learners,2019-02-14,2cf1211f-def9-4e67-96cc-bc26b540d015,rule. And 2) If I have a recipe that
Language Models are Unsupervised Multitask Learners,2019-02-14,0f31d9b2-90dc-4bac-bc0a-31cda2b59d92,calls for something and I want to
Language Models are Unsupervised Multitask Learners,2019-02-14,d7c84c2e-7fb1-4483-8212-9bd1c591aaee,"use it in a recipe, I do not substitute"
Language Models are Unsupervised Multitask Learners,2019-02-14,f9b28470-f633-4ba3-be76-4e63be95f7fb,it with something else on-line. Like
Language Models are Unsupervised Multitask Learners,2019-02-14,9080cfc7-72fd-423a-92af-c89ac3fe4131,this.
Language Models are Unsupervised Multitask Learners,2019-02-14,1cccb580-fa27-4b23-85ce-cac6ab050d6e,I found this really cute card
Language Models are Unsupervised Multitask Learners,2019-02-14,bb4ebd78-7aa4-4308-a4b8-b22e08186262,with cute little kittens onGPT-2 Completion 3
Language Models are Unsupervised Multitask Learners,2019-02-14,abab0553-a528-4230-b828-1b9160340978,"recipe, which will likely yield about"
Language Models are Unsupervised Multitask Learners,2019-02-14,3415b987-b62a-40da-99ca-84299d73ad24,the same amount of cake.
Language Models are Unsupervised Multitask Learners,2019-02-14,ab764028-9477-4f1c-aba7-26675aa3a371,Chocolate Buttermilk Cake
Language Models are Unsupervised Multitask Learners,2019-02-14,e62d4f3e-6507-4704-a055-f79a41df0321,"Print Chunks of chocolate cake,"
Language Models are Unsupervised Multitask Learners,2019-02-14,17412003-31a9-439f-a6b3-bbaf9a0fdcb6,coated in buttermilk and topped
Language Models are Unsupervised Multitask Learners,2019-02-14,8400cf6a-e291-4137-9b24-692ccce827ec,"with crunchy chopped chocolate,"
Language Models are Unsupervised Multitask Learners,2019-02-14,4c478473-7955-4de7-854a-7bbf4d4bd878,drizzled with chocolate sauce.
Language Models are Unsupervised Multitask Learners,2019-02-14,3589e47f-80f1-4943-a5a2-a05003215d60,Simple but a nice change from
Language Models are Unsupervised Multitask Learners,2019-02-14,a31b7f4b-d090-4438-98ef-7cf7ee90962d,chocolate cake! Author: Momma
Language Models are Unsupervised Multitask Learners,2019-02-14,5bd6ee2b-d00d-41b4-91ff-81ccb7248186,Made Ingredients For the chocolate
Language Models are Unsupervised Multitask Learners,2019-02-14,b1edfaff-351e-4318-90ff-144cda2ca0e6,buttermilk cake 1 1/2 cup unsalted
Language Models are Unsupervised Multitask Learners,2019-02-14,0fa3adf0-dc12-42dc-a5ce-a5c275057a90,"butter , room temperature"
Language Models are Unsupervised Multitask Learners,2019-02-14,b9667038-107c-4cd8-b08e-a1c0fc99bbd2,1 1/2 cup granulated sugar
Language Models are Unsupervised Multitask Learners,2019-02-14,a0db49a3-de9d-4d89-b361-e3fe6ee8749b,3 large eggs plus 1 egg yolk For
Language Models are Unsupervised Multitask Learners,2019-02-14,446b1371-fc6f-4c96-b729-0bd7065a2645,the chocolate glaze 1/3 cup cocoa
Language Models are Unsupervised Multitask Learners,2019-02-14,7945a896-a942-49f0-8efc-b4d33a4152dc,powder
Language Models are Unsupervised Multitask Learners,2019-02-14,d065acfa-9e57-492c-823f-c35d598af422,1 3/4 cups powdered sugar
Language Models are Unsupervised Multitask Learners,2019-02-14,0cc8ec29-bc7c-4866-a967-6ff85d852059,6 ounces semis
Language Models are Unsupervised Multitask Learners,2019-02-14,46c088dc-3c91-448b-a145-1b3040aef046,GPT-2 Completion 4
Language Models are Unsupervised Multitask Learners,2019-02-14,5cc250a6-d0e4-4f43-8e13-8479dcdfc455,recipe.
Language Models are Unsupervised Multitask Learners,2019-02-14,0e261e59-7b9a-42cc-8e59-b9868f977a9c,Meringue Cookies
Language Models are Unsupervised Multitask Learners,2019-02-14,2dadf894-57e3-4ddf-8486-978ae39fd102,"1 3/4 cup butter, softened"
Language Models are Unsupervised Multitask Learners,2019-02-14,955a3849-b2bb-4f93-92ca-e462da163a80,1 cup sugar
Language Models are Unsupervised Multitask Learners,2019-02-14,5162f8d6-8f57-4d6d-bb28-be7a50be2527,1 egg yolk
Language Models are Unsupervised Multitask Learners,2019-02-14,55749dfb-9a22-4f7e-adf3-87fd89232cf7,3 T. heavy cream
Language Models are Unsupervised Multitask Learners,2019-02-14,590b1319-a9ba-4ffa-ac17-8374a77658cd,3 1/2-4 cups ﬂour
Language Models are Unsupervised Multitask Learners,2019-02-14,77b3aff4-6bb7-4a50-8d98-4c3482f10265,pinch of salt
Language Models are Unsupervised Multitask Learners,2019-02-14,4609ead2-f7af-4c56-9410-5322d395075c,Peppermint-Jojo Topping
Language Models are Unsupervised Multitask Learners,2019-02-14,4f986a53-8bbd-45da-bee2-71ecf3a419d5,1 1/4 cups powdered sugar
Language Models are Unsupervised Multitask Learners,2019-02-14,85c7a826-c020-4852-bad0-b23a7390a95b,1 cup chopped pecans
Language Models are Unsupervised Multitask Learners,2019-02-14,f46bfd7a-8f25-437e-a80b-d3f36c5cdcde,1/2 cup ﬁnely chopped mint leaves
Language Models are Unsupervised Multitask Learners,2019-02-14,bf587a96-ecf1-4502-a37e-7e9d49625846,1/2 cup chopped fresh mint (about
Language Models are Unsupervised Multitask Learners,2019-02-14,b7113127-65c2-435d-a472-7a3b02b10b94,1/2 sheet)
Language Models are Unsupervised Multitask Learners,2019-02-14,a5f6772e-162a-4c96-a37d-d6706af321a6,1 tsp. peppermint extract
Language Models are Unsupervised Multitask Learners,2019-02-14,f9e7d27e-e3b5-45c8-981f-54697b82cd6d,1/4 tsp. sea salt
Language Models are Unsupervised Multitask Learners,2019-02-14,b8934850-be6d-4aaa-8b3e-bd025ef66cf0,pinch of seaGPT-2 Completion 5
Language Models are Unsupervised Multitask Learners,2019-02-14,8d3d12d9-be58-484b-92a2-4b8cacb58474,recipe.
Language Models are Unsupervised Multitask Learners,2019-02-14,4132e73b-e425-4046-a2c3-5138a63d519b,My 2 failures at Christmas morning
Language Models are Unsupervised Multitask Learners,2019-02-14,25b3a944-2cf4-4a14-95ff-fb92762ff714,were not due to poor frosting skills.
Language Models are Unsupervised Multitask Learners,2019-02-14,ed0f6e52-fd77-4390-bec3-a81186187882,That was another success story.
Language Models are Unsupervised Multitask Learners,2019-02-14,6c46add3-3b21-4981-80bd-400af32e8c7a,"The cake was beautifully moist,"
Language Models are Unsupervised Multitask Learners,2019-02-14,5120bd04-1465-41b3-9358-ccb4105132c0,"and well, just a beautiful cake. It’s"
Language Models are Unsupervised Multitask Learners,2019-02-14,adc27b94-7f55-4c17-b5b8-461759a016f5,"got very little air in it or moisture,"
Language Models are Unsupervised Multitask Learners,2019-02-14,f8e50b4f-d0cc-463f-87a0-4ef4a34f0d7d,but all the chocolate and mint was
Language Models are Unsupervised Multitask Learners,2019-02-14,9da088a9-18c6-4652-afd0-3a7080ae4524,perfectly blended together.
Language Models are Unsupervised Multitask Learners,2019-02-14,837952ca-273d-49ba-9b9a-468d04ece98d,One thing that made me cringe a
Language Models are Unsupervised Multitask Learners,2019-02-14,5313987a-fd32-4d82-b0e6-97d29af4da68,bit was that all those ingredients-
Language Models are Unsupervised Multitask Learners,2019-02-14,1a4c86f3-3599-4e27-ab1e-c468a8b05500,"egg yolks, sugar, melted butter,"
Language Models are Unsupervised Multitask Learners,2019-02-14,a9b2ee0c-67a0-4c1d-9861-1aa5a35af30a,"chocolate chips, melted chocolate,"
Language Models are Unsupervised Multitask Learners,2019-02-14,05425be9-41a2-46b8-9aa7-70e55522b2e7,and mint- had to be mixed right
Language Models are Unsupervised Multitask Learners,2019-02-14,41ea1aea-3520-4d2a-aa39-a291e25aee69,"into the cake batter, which is"
Language Models are Unsupervised Multitask Learners,2019-02-14,98552c7c-90dc-4c04-a457-5d51919db4cf,super time-consuming. A friend
Language Models are Unsupervised Multitask Learners,2019-02-14,fc81bb88-ff55-47bb-a71a-281f9f89574d,suggested that if I put the batter in a
Language Models are Unsupervised Multitask Learners,2019-02-14,e5266b14-0968-45e7-b207-c046c7c22c6c,"separate bowl, IGPT-2 Completion 6"
Language Models are Unsupervised Multitask Learners,2019-02-14,811fddf4-00b0-4e5d-ad2c-1697b8eca253,"recipe, which will give you about"
Language Models are Unsupervised Multitask Learners,2019-02-14,a54451ba-4ce3-4e7b-b166-fd39958796dd,6-9 servings of delicious chocolate
Language Models are Unsupervised Multitask Learners,2019-02-14,cc0b59ea-f954-40a0-a2bf-3dd9e69f3f3f,cake. To prevent your layers
Language Models are Unsupervised Multitask Learners,2019-02-14,8206d99e-877e-4da0-9fc2-384b97af8994,"from sticking together, I used my"
Language Models are Unsupervised Multitask Learners,2019-02-14,280dc1b6-99d9-4fa8-915d-721423e2c31d,"8-inch round tart pan, lined it with"
Language Models are Unsupervised Multitask Learners,2019-02-14,50bd61ab-2527-4c84-9317-6d66a03c9d43,"parchment paper, then cut up 4"
Language Models are Unsupervised Multitask Learners,2019-02-14,8ad84792-6e21-4bc9-8783-4b3ba83022f2,7-inch round cakes from it and
Language Models are Unsupervised Multitask Learners,2019-02-14,89ed9353-20b7-47a0-bb2e-0a8f8723e166,sandwiched the layers between
Language Models are Unsupervised Multitask Learners,2019-02-14,9b7aefea-57a5-4590-aac3-a4cc489a3b52,them (this also worked well for
Language Models are Unsupervised Multitask Learners,2019-02-14,66e4cce8-8e3c-4882-9f7c-c15e88f39cc7,the 1-layer chocolate cake as well).
Language Models are Unsupervised Multitask Learners,2019-02-14,40b849cc-c3a4-4435-bb73-0f3bc6b629f0,When you’re ready to bake this
Language Models are Unsupervised Multitask Learners,2019-02-14,3adb263f-cf5e-4d51-a4da-fdd42e5d67ce,"cake, remove the parchment paper,"
Language Models are Unsupervised Multitask Learners,2019-02-14,3ab0288d-bf30-436e-b42e-3ef667982b44,preheat oven to 350 degrees and
Language Models are Unsupervised Multitask Learners,2019-02-14,79ecf934-f6a2-4f2d-bbd2-7a03f033f384,"bake for about 30 – 40 minutes,"
Language Models are Unsupervised Multitask Learners,2019-02-14,1149d8a2-aa5e-450f-84c6-d0a1cfe47567,until the top is golden and a tester
Language Models are Unsupervised Multitask Learners,2019-02-14,e278a7f7-6207-43b0-9097-e02859be158c,comes out clean.
Language Models are Unsupervised Multitask Learners,2019-02-14,78f74f2b-a6b7-403a-9141-7712308f2383,Chocolate Buttermilk Layer
Language Models are Unsupervised Multitask Learners,2019-02-14,17cb0e60-d096-4fa2-b133-a4ec8a3a57e0,Cake with Peppermint Jo
Language Models are Unsupervised Multitask Learners,2019-02-14,3c05f6c0-b90f-4352-a6c9-69e2842fd661,Table 12. Non-cherry-picked completions from GPT-2 generated from the same context (from WebText test). Context is 384 tokens
Language Models are Unsupervised Multitask Learners,2019-02-14,2dab5033-cfa0-48cc-b216-eeb8bd83c6fb,"(shown truncated), and generations are 128 tokens. Top- krandom sampling with k= 40 was used for generation."
Language Models are Unsupervised Multitask Learners,2019-02-14,ee75f44b-685b-4f4e-8873-bd0c4eee8e5e,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,028458d1-d944-4414-92ac-2eda5efbd7f7,"Context (human-written): In a shocking ﬁnding, scientist discovered a herd of unicorns living"
Language Models are Unsupervised Multitask Learners,2019-02-14,0a21452f-01ef-4633-bacb-0d8455f7c01b,"in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the"
Language Models are Unsupervised Multitask Learners,2019-02-14,31e5617e-94c3-4d3c-9f87-64d57320e586,researchers was the fact that the unicorns spoke perfect English.
Language Models are Unsupervised Multitask Learners,2019-02-14,b709518a-b22e-4c8b-b521-663a1b2fe343,"GPT-2: The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These"
Language Models are Unsupervised Multitask Learners,2019-02-14,4ee22339-7f38-4582-8179-5ee2aac47706,"four-horned, silver-white unicorns were previously unknown to science."
Language Models are Unsupervised Multitask Learners,2019-02-14,d641761b-a865-4022-ae52-e8d750b130ea,"Now, after almost two centuries, the mystery of what sparked this odd phenomenon is ﬁ-"
Language Models are Unsupervised Multitask Learners,2019-02-14,5c093eab-abf4-4f72-b2dc-9635f6cb32d4,nally solved.
Language Models are Unsupervised Multitask Learners,2019-02-14,12ae7156-4408-4e68-a4d9-879dfaa7fd2e,"Dr. Jorge P ´erez, an evolutionary biologist from the University of La Paz, and several com-"
Language Models are Unsupervised Multitask Learners,2019-02-14,c2f055ed-7317-4b4a-8ed0-5b396182e916,"panions, were exploring the Andes Mountains when they found a small valley, with no other animals"
Language Models are Unsupervised Multitask Learners,2019-02-14,377a533e-2c68-4bdf-a58f-6971306e0f4d,"or humans. P ´erez noticed that the valley had what appeared to be a natural fountain, surrounded by"
Language Models are Unsupervised Multitask Learners,2019-02-14,ee4b21ac-8e1a-4a51-8c0f-f36eb111e52d,two peaks of rock and silver snow.
Language Models are Unsupervised Multitask Learners,2019-02-14,560ee02a-3d95-43fa-82b6-7e1505ccf6fd,P´erez and the others then ventured further into the valley. “By the time we reached the top
Language Models are Unsupervised Multitask Learners,2019-02-14,fa2a5b40-d50a-4bbf-b619-356a57d67d09,"of one peak, the water looked blue, with some crystals on top,” said P ´erez."
Language Models are Unsupervised Multitask Learners,2019-02-14,47ec652e-1109-424e-8904-8b4215e72314,P´erez and his friends were astonished to see the unicorn herd. These creatures could be
Language Models are Unsupervised Multitask Learners,2019-02-14,389a2443-7f37-409e-99da-c9f3c8acd5e6,seen from the air without having to move too much to see them – they were so close they could touch
Language Models are Unsupervised Multitask Learners,2019-02-14,6291ed37-a86c-409d-b343-d95308f8627b,their horns.
Language Models are Unsupervised Multitask Learners,2019-02-14,e97abe25-79c7-4635-96d2-d611e7407a8c,While examining these bizarre creatures the scientists discovered that the creatures also
Language Models are Unsupervised Multitask Learners,2019-02-14,878b8ec9-08be-4d48-aa9b-5a6499712d49,"spoke some fairly regular English. P ´erez stated, “We can see, for example, that they have a common"
Language Models are Unsupervised Multitask Learners,2019-02-14,3036a094-b934-4452-9c8a-7b6be36b5304,"‘language,’ something like a dialect or dialectic.”"
Language Models are Unsupervised Multitask Learners,2019-02-14,0c52c874-d1c2-42e3-ad65-5dec379a4425,"Dr. P ´erez believes that the unicorns may have originated in Argentina, where the animals"
Language Models are Unsupervised Multitask Learners,2019-02-14,d1bc0d8d-296f-491f-87a2-4a4ab62af0eb,were believed to be descendants of a lost race of people who lived there before the arrival of humans
Language Models are Unsupervised Multitask Learners,2019-02-14,7bf93a03-7d1a-4cfa-abcc-7e19cd75a6a2,in those parts of South America.
Language Models are Unsupervised Multitask Learners,2019-02-14,cf568e57-505d-4591-9a4d-3e50a89ce44b,"While their origins are still unclear, some believe that perhaps the creatures were created"
Language Models are Unsupervised Multitask Learners,2019-02-14,a46ea384-1053-463c-bb95-315b44899e8b,"when a human and a unicorn met each other in a time before human civilization. According to P ´erez,"
Language Models are Unsupervised Multitask Learners,2019-02-14,4fb4750b-8acd-4da8-bb0b-dd71fd6f99cf,"“In South America, such incidents seem to be quite common.”"
Language Models are Unsupervised Multitask Learners,2019-02-14,7f8580d2-d644-4643-9aef-7d4457af6320,"However, P ´erez also pointed out that it is likely that the only way of knowing for sure if"
Language Models are Unsupervised Multitask Learners,2019-02-14,273414b7-1b55-4d89-9f34-8c05f4ba499e,unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to
Language Models are Unsupervised Multitask Learners,2019-02-14,5d17e19e-41a1-488f-bb36-6dca096f0757,"communicate in English quite well, which I believe is a sign of evolution, or at least a change in"
Language Models are Unsupervised Multitask Learners,2019-02-14,4be2a897-f10e-42fa-a19b-245ee720286a,"social organization,” said the scientist."
Language Models are Unsupervised Multitask Learners,2019-02-14,0eb9abae-f372-43e4-b111-25b1400e7e74,Table 13. Conditional generation on an out-of-distribution context by GPT-2. Cherry pick of 10 samples generated with k= 40 .
Language Models are Unsupervised Multitask Learners,2019-02-14,04e2760c-ccf0-4323-9e45-26dc38fe12ed,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,c9b0b3a3-abe2-482c-821a-e6725989281c,Article: Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in
Language Models are Unsupervised Multitask Learners,2019-02-14,f880119a-b81e-4aae-8b6c-0aaf1e645e69,"modern day France 36,000 years ago."
Language Models are Unsupervised Multitask Learners,2019-02-14,391a13a3-2d04-4704-9a68-ade5a4e36726,"Now, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d’Arc Cave have been"
Language Models are Unsupervised Multitask Learners,2019-02-14,03a71e33-1e0f-4d6d-b11e-0913e7306328,reproduced to create the biggest replica cave in the world.
Language Models are Unsupervised Multitask Learners,2019-02-14,69a48925-441a-4c23-bf37-57d5385ac997,The manmade cavern named the Caverne du Pont-d’Arc has been built a few miles from the original site in
Language Models are Unsupervised Multitask Learners,2019-02-14,b0d8f653-9634-4414-8f8d-edabca4a5f5d,"Vallon-Pont-D’arc in Southern France and contains 1,000 painstakingly-reproduced drawings as well as around 450"
Language Models are Unsupervised Multitask Learners,2019-02-14,bf20c941-c340-451a-bc48-2efdc9db4ee3,bones and other features...
Language Models are Unsupervised Multitask Learners,2019-02-14,d2b6d25a-189a-463d-885d-0f6c574e8603,Cavemen and women sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave
Language Models are Unsupervised Multitask Learners,2019-02-14,3637c011-4bc1-4efd-ab1c-728579d31816,"36,000 years ago and now a replica has been created (pictured)"
Language Models are Unsupervised Multitask Learners,2019-02-14,861880bd-2ac3-4fff-8763-8adfdfa24020,...
Language Models are Unsupervised Multitask Learners,2019-02-14,d85c0104-8f04-4da6-968b-c0a9eb6848a8,GPT-2: The original site in Vallon-Pont-D’arc in South-
Language Models are Unsupervised Multitask Learners,2019-02-14,867cb9c5-46b8-485b-95df-0876abf542cd,ern France is a Unesco World Heritage site and is the
Language Models are Unsupervised Multitask Learners,2019-02-14,d2bb5297-4343-439d-a138-91c75f393485,oldest known and the best preserved cave decorated by
Language Models are Unsupervised Multitask Learners,2019-02-14,bfe59402-0ebc-4a8a-a01d-b6f06ae7eb7c,man. The replica cave was built a few miles from the orig-
Language Models are Unsupervised Multitask Learners,2019-02-14,70729155-61c6-499a-8159-5d2471010dcc,inal site in Vallon-Pont-D’Arc in Southern France. The
Language Models are Unsupervised Multitask Learners,2019-02-14,eca04164-ee20-4249-819e-aa8a6e318ad9,cave contains images of 14 different species of animals
Language Models are Unsupervised Multitask Learners,2019-02-14,0edab980-9996-4c22-85c4-7860ab4d2d9f,"including woolly rhinoceros, mammoths, and big cats.Reference: Cave mimics famous Caverne du Pont-d’Arc"
Language Models are Unsupervised Multitask Learners,2019-02-14,a0cab5db-89b7-41b7-abe9-cf4936521544,"in France, the oldest cave decorated by man and the"
Language Models are Unsupervised Multitask Learners,2019-02-14,82d9fe2a-128a-4011-b0ef-b9c687e5ca2b,"best preserved. The replica contains all 1,000 paint-"
Language Models are Unsupervised Multitask Learners,2019-02-14,5b66cdb2-f4ea-4521-a352-428708b66cf3,ings which include 425 such as a woolly rhinoceros and
Language Models are Unsupervised Multitask Learners,2019-02-14,22318d3e-1308-4bec-81ec-8dcecba9b7af,mammoths. Minute details were copied using 3D mod-
Language Models are Unsupervised Multitask Learners,2019-02-14,dcf34d5f-4814-4d05-a75a-e93eb9596be3,"elling and anamorphic techniques, often used to shoot"
Language Models are Unsupervised Multitask Learners,2019-02-14,863739cb-034a-4656-a233-971f61447caa,widescreen images. The modern cave also includes replica
Language Models are Unsupervised Multitask Learners,2019-02-14,8a5ae06b-27af-4608-8e8f-47e277f2ce2e,"paw prints of bears, bones and details preserved in the"
Language Models are Unsupervised Multitask Learners,2019-02-14,4d4aabd1-7ae9-467b-88b9-dca2fe7dc576,original cave.
Language Models are Unsupervised Multitask Learners,2019-02-14,03a7129d-6433-4127-954c-4b9c4d1234c2,"Article: ‘If you can ﬁnd my submarine, it’s yours,’ Russian oil billionaire Roman Abramovich once said."
Language Models are Unsupervised Multitask Learners,2019-02-14,8f6b7133-89f4-4446-9771-e5d1fb1a02ec,"And it seems the oligarch and Chelsea FC owner, whose submarine is just one of the extras that came with his"
Language Models are Unsupervised Multitask Learners,2019-02-14,d121d0da-d9e5-48ac-9544-c643bc9225fb,"£300million superyacht Eclipse (perfect for getting to shore undetected), is not the only wealthy businessman"
Language Models are Unsupervised Multitask Learners,2019-02-14,d9385d71-7a01-43c8-8d98-34d2acdd4266,splashing out on underwater exploration.
Language Models are Unsupervised Multitask Learners,2019-02-14,ff98f21f-a85c-401a-8797-1dca1c41e977,"Dubbed Earth’s real ‘ﬁnal frontier’, the oceans are still so little-explored that billionaires are queuing up to buy"
Language Models are Unsupervised Multitask Learners,2019-02-14,da945fd9-f723-4e1c-9c24-14d68810fe94,vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world’s oceans.
Language Models are Unsupervised Multitask Learners,2019-02-14,ed045b7f-2b95-492e-9779-6918b8b8a444,So if you have a spare few million in the bank and want some holiday snaps and Instagram posts that will really
Language Models are Unsupervised Multitask Learners,2019-02-14,f79ce9d9-aa9c-4b3c-b7bf-828a0e8b6198,"trump everyone else, you may want to snap up one of these..."
Language Models are Unsupervised Multitask Learners,2019-02-14,ce5afbbb-7518-46b8-b342-5ca2d9c8056b,Whale of a time: The OrcaSub takes you 2000 feet beneath the surface in two pressurised Perspex viewing domes
Language Models are Unsupervised Multitask Learners,2019-02-14,aec7b7af-5944-4a06-8a9f-9e7fa8c6561e,for optimum exploration
Language Models are Unsupervised Multitask Learners,2019-02-14,84b74b6e-82e0-41b7-8dd3-96b946a521ec,...
Language Models are Unsupervised Multitask Learners,2019-02-14,20f98552-803b-4efb-8e15-479654343130,GPT-2: The U-Boat Worx submarine can dive to 984 feet
Language Models are Unsupervised Multitask Learners,2019-02-14,b3d736c9-666a-4721-93d7-677587d6358b,(300 metres) and is capable of diving to the bottom of
Language Models are Unsupervised Multitask Learners,2019-02-14,6f635144-4d33-4d9c-8369-338b2b66b27f,"the ocean. It can be used for diving, snorkelling and even"
Language Models are Unsupervised Multitask Learners,2019-02-14,c006cdcd-0068-45a7-a84a-67c3704e6b10,snorkel diving. The submarine is also equipped with a
Language Models are Unsupervised Multitask Learners,2019-02-14,a09d15b3-c351-4e58-ac65-92f807b0a575,snorkelling platform.Reference: Feel like a James Bond villain with the most
Language Models are Unsupervised Multitask Learners,2019-02-14,d7d88733-00bc-41ac-a6da-a2fe32ba96e6,advanced submerged ocean vehicles. Some convert from
Language Models are Unsupervised Multitask Learners,2019-02-14,08e02584-33ce-4979-82ef-ce0c4ae84973,"a yacht into a submarine, and others detached off for a"
Language Models are Unsupervised Multitask Learners,2019-02-14,91775f19-e97a-4f02-be96-b483c7297fab,speedy exploration. Oliver’s Travels offers a Mile Low
Language Models are Unsupervised Multitask Learners,2019-02-14,012e6617-b5bd-42fe-a8f6-0581530fdb82,"package on its submarine, complete with chef and butler"
Language Models are Unsupervised Multitask Learners,2019-02-14,f3233423-8db3-4667-be8e-ce36708e401a,"for£175,000 a night."
Language Models are Unsupervised Multitask Learners,2019-02-14,1347ceec-d161-4074-814a-4e378fc0d235,"Article: Amina Ali Qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf."
Language Models are Unsupervised Multitask Learners,2019-02-14,71023b5c-2a15-431c-b615-4598e1c3dd4f,"Only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment"
Language Models are Unsupervised Multitask Learners,2019-02-14,b5930548-ca4f-4f43-aea1-0207d493e34b,"started. She lay awake, she says, in a village mosque on the Yemeni island of Birim, counting explosions as the"
Language Models are Unsupervised Multitask Learners,2019-02-14,c8602d82-dacb-4d2d-bf36-8e71840e87c0,baby cried.
Language Models are Unsupervised Multitask Learners,2019-02-14,5b01c8ad-3182-4f4e-b5fd-2fa0135c739e,It could have been worse though. They could have still been in their house when the ﬁrst missile landed.
Language Models are Unsupervised Multitask Learners,2019-02-14,85523097-4b34-4a7f-95dc-9e2f9bab8892,"”Our neighbor shouted to my husband ’you have to leave, they’re coming.’ And we just ran. As soon as we left the"
Language Models are Unsupervised Multitask Learners,2019-02-14,f0d31b7d-444d-4331-8e82-ca4f72a71ab1,"house, the ﬁrst missile fell right by it and then a second on it. It burned everything to the ground,” Qassim tells us"
Language Models are Unsupervised Multitask Learners,2019-02-14,d61550db-4513-4fea-9593-aa6d7a34cca6,...
Language Models are Unsupervised Multitask Learners,2019-02-14,533c5505-5205-4baf-96b7-1ad249ea1119,GPT-2: Yemen is in the middle of a civil war. Saudi
Language Models are Unsupervised Multitask Learners,2019-02-14,055119a8-7d72-42b7-852b-f1fea21dbe8f,Arabia is leading the coalition bombing campaign. It’s
Language Models are Unsupervised Multitask Learners,2019-02-14,66f41513-5f05-4507-b4fd-a3b0cd5516e5,been bombing Yemen for more than two months now.Reference: Amina Ali Qassim’s family sought shelter in
Language Models are Unsupervised Multitask Learners,2019-02-14,98d47266-3ccd-4587-aab9-12beaac86996,a mosque before ﬂeeing Yemen. Thousands like them are
Language Models are Unsupervised Multitask Learners,2019-02-14,96544e44-50cd-4e7e-9ed3-804b0d5a7f99,boarding boats to sail to Djibouti. Saudi Arabia has been
Language Models are Unsupervised Multitask Learners,2019-02-14,c7be5e64-b406-4c17-afc4-75dbca501cd4,pounding Yemen in a bid to defeat Houthi rebels.
Language Models are Unsupervised Multitask Learners,2019-02-14,dbc6a005-6c80-40ac-842d-e464bda47147,Table 14. Summaries generated by GPT-2 and reference summaries on the test set of the CNN and Daily Mail dataset.
Language Models are Unsupervised Multitask Learners,2019-02-14,3ed6bcda-990c-4da0-9087-462239bd2ca3,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,b8df6963-3d0d-4bb0-8129-dc7f8442efc8,English reference GPT-2 French translation
Language Models are Unsupervised Multitask Learners,2019-02-14,b67593c3-cac5-4146-b687-7ef2014a8952,"This re-release, titled The Next Day Extra, was presented"
Language Models are Unsupervised Multitask Learners,2019-02-14,2bf57d67-b329-4834-8381-5c605bf2b871,"in the form of three disks: the original album, unpublished"
Language Models are Unsupervised Multitask Learners,2019-02-14,b061d9af-2659-4e6a-8987-6fc68e58d028,"studio sessions and remixes, plus a DVD containing the"
Language Models are Unsupervised Multitask Learners,2019-02-14,fcb80687-b680-4f3a-b2e9-60ae72b303de,"four clips that have already been unveiled.Les nouvelles re-releases, tout en premier disc, n ´ecessaire"
Language Models are Unsupervised Multitask Learners,2019-02-14,4c4be56f-ec10-4f74-b10f-7dfbe70c42b9,"de l’album, un studio session et remixes, plus une DVD"
Language Models are Unsupervised Multitask Learners,2019-02-14,f7087c86-e552-4e28-9c4a-583f98d33481,de l’´ecran de quelques clips qui ont ´et´e d´ej`a´echapp ´es.
Language Models are Unsupervised Multitask Learners,2019-02-14,13705389-1170-4e6a-ab74-809c98a98ccb,French reference GPT-2 English translation
Language Models are Unsupervised Multitask Learners,2019-02-14,966e7626-8576-4984-a101-413f2f7ee406,Un homme expliquait que le fonctionnement de la hernia
Language Models are Unsupervised Multitask Learners,2019-02-14,128ec9fa-7679-4417-a5b0-7ca4d6014e99,"fonctionnelle qu’il avait reconna ˆıt avant de faire, le fonc-"
Language Models are Unsupervised Multitask Learners,2019-02-14,f389f506-7cfa-42c1-b8ee-5676019fc5f6,"tionnement de la hernia fonctionnelle que j’ai r ´eussi, j’ai"
Language Models are Unsupervised Multitask Learners,2019-02-14,d5c0fb86-ad0b-42a9-a25c-ceea5944a715,"r´eussi.This release, originally released on three discs: the origi-"
Language Models are Unsupervised Multitask Learners,2019-02-14,1a8b791c-06af-4b08-bdf8-7b70dab71dd8,"nal album, sessions studio in ´edites and remixes, a DVD"
Language Models are Unsupervised Multitask Learners,2019-02-14,0fee4831-c0ed-435c-a9dc-5049656164a8,containing the four clips that were released on the previous
Language Models are Unsupervised Multitask Learners,2019-02-14,2a407c0a-6bb6-4701-a448-d6dca2242bcd,day.
Language Models are Unsupervised Multitask Learners,2019-02-14,d7e6bfa6-f820-4d90-b81a-76760f56d9df,English reference GPT-2 French translation
Language Models are Unsupervised Multitask Learners,2019-02-14,e697c1db-b841-466e-91d5-8e1ef8118b5f,One man explained that the free hernia surgery he’d re-
Language Models are Unsupervised Multitask Learners,2019-02-14,0f7b0007-495f-4ca6-be0f-434d208d6ace,ceived will allow him to work again.Un homme expliquait que le fonctionnement de la hernia
Language Models are Unsupervised Multitask Learners,2019-02-14,9c1a2eeb-231a-4592-a0d7-1af1440128ad,"fonctionnelle qu’il avait reconna ˆıt avant de faire, le fonc-"
Language Models are Unsupervised Multitask Learners,2019-02-14,8b3442b6-c688-4c49-9187-4028e2bb8041,"tionnement de la hernia fonctionnelle que j’ai r ´eussi, j’ai"
Language Models are Unsupervised Multitask Learners,2019-02-14,c18cd48c-cac6-4688-bbc9-6c25e63b2098,r´eussi.
Language Models are Unsupervised Multitask Learners,2019-02-14,c943bc80-ac9a-4f87-8824-2a6e9ce5b8c3,French reference GPT-2 English translation
Language Models are Unsupervised Multitask Learners,2019-02-14,477656e6-e9e9-4ad9-8d86-09f179de2428,Un homme a expliqu ´e que l’op ´eration gratuite qu’il avait
Language Models are Unsupervised Multitask Learners,2019-02-14,f58eb297-0592-41b8-ad7d-8f753dafabc2,subie pour soigner une hernie lui permettrait de travailler
Language Models are Unsupervised Multitask Learners,2019-02-14,e0f2dbfb-5481-4f3d-94fb-16ca1fc7260f,`a nouveau.A man told me that the operation gratuity he had been
Language Models are Unsupervised Multitask Learners,2019-02-14,5c9f8487-021a-422f-adc8-e5d6e94e6336,promised would not allow him to travel.
Language Models are Unsupervised Multitask Learners,2019-02-14,f851c4bd-05a8-4556-8a21-234c2154675e,English reference GPT-2 French translation
Language Models are Unsupervised Multitask Learners,2019-02-14,53816ec5-d6aa-43da-bc8c-43aa474eccf3,Speaking to an open government conference in London
Language Models are Unsupervised Multitask Learners,2019-02-14,bbd7ab72-9c37-4c2c-a0d7-5615b84b2819,"via video link, Mr Kerry said: ”There is no question that"
Language Models are Unsupervised Multitask Learners,2019-02-14,e8942e68-4781-481c-97f5-d3e14cdddbcc,the President and I and others in government have actually
Language Models are Unsupervised Multitask Learners,2019-02-14,46a39a86-b8f5-4602-a447-64ca70236979,learned of some things that had been happening on an
Language Models are Unsupervised Multitask Learners,2019-02-14,2f694353-e22d-4ef5-b83d-49ebd90cc4fc,"automatic pilot because the ability has been there, going"
Language Models are Unsupervised Multitask Learners,2019-02-14,ff7afbf2-0adf-449f-a998-c869aa560eb3,back to World War Two and to the very difﬁcult years of
Language Models are Unsupervised Multitask Learners,2019-02-14,68264a09-b8fb-4d10-86e1-083f20363a11,"the Cold War, and then, of course, 9/11.”Le premier conference de l’ ´edition internationale en ligne"
Language Models are Unsupervised Multitask Learners,2019-02-14,cbb11ab8-60a7-4a21-a1c7-f21dd24c0dba,"sur la vide, le ministre de l’ ´Edition internationale a ´et´e"
Language Models are Unsupervised Multitask Learners,2019-02-14,a9e443ff-8fa5-4f57-846f-ca403de4341f,"vide, lui dit : ”Il n’y a qu’une question que le pr ´esident"
Language Models are Unsupervised Multitask Learners,2019-02-14,906dc6bb-5149-4ca9-859e-92d0f64deb73,et moi et tous les autres personnes d’une administration
Language Models are Unsupervised Multitask Learners,2019-02-14,a19537a5-ede7-438c-8b56-388ae51b3870,"qui a ´et´e´evident par la fois, par la part de la guerre, par la"
Language Models are Unsupervised Multitask Learners,2019-02-14,b0127092-6b87-45d3-ba2b-54017b1e20c1,part de la guer
Language Models are Unsupervised Multitask Learners,2019-02-14,83e3c44e-7ba5-4f01-b1e8-d4836c4943ad,French reference GPT-2 English translation
Language Models are Unsupervised Multitask Learners,2019-02-14,9539c869-d66f-4703-abda-85892b15ffb2,S’exprimant lors d’une conf ´erence intergouvernementale
Language Models are Unsupervised Multitask Learners,2019-02-14,01d42c82-0794-4d0a-ac3f-3e822cfea068,"`a Londres par liaison vid ´eo, M. Kerry a d ´eclar ´e: ”Il est"
Language Models are Unsupervised Multitask Learners,2019-02-14,e846041e-6338-4cf5-99d9-8a84756be2ae,"ind´eniable que le Pr ´esident, moi-m ˆeme et d’autres mem-"
Language Models are Unsupervised Multitask Learners,2019-02-14,65ddca16-80b8-41f7-b225-86395e67e063,bres du gouvernement avons pris connaissance de cer-
Language Models are Unsupervised Multitask Learners,2019-02-14,976b2c5a-fd24-4345-8a83-819f689269a5,taines choses en mode pilote automatique parce que nous
Language Models are Unsupervised Multitask Learners,2019-02-14,e238dfea-d6fd-4776-97c4-dca7d0bd9c8f,"en avions la possibilit ´e, d`es la Seconde guerre mondiale et"
Language Models are Unsupervised Multitask Learners,2019-02-14,1ca33a23-5792-426f-957e-920514a7d179,"jusqu’aux ann ´ees difﬁciles de la Guerre froide, puis bien"
Language Models are Unsupervised Multitask Learners,2019-02-14,9fd4d9e4-3961-4f69-a008-0532b0374f22,"sˆur le 11 septembre.”In a conf ´erence between the United States and London,"
Language Models are Unsupervised Multitask Learners,2019-02-14,57d2cdf2-8df1-45bc-baf6-3517c3b8da6a,Secretary of State John Kerry said: ”It is indeniable that
Language Models are Unsupervised Multitask Learners,2019-02-14,48efe69e-b61a-4532-a914-e6fffb67f889,"the President, myself and others of the government have"
Language Models are Unsupervised Multitask Learners,2019-02-14,44141aa5-7955-40e4-8a4f-4a77bd65d2ad,been aware of certain certain choices that have been made
Language Models are Unsupervised Multitask Learners,2019-02-14,3709ff91-5c16-45ed-bb5a-9e53cb707d3f,in the past in order to be able to do certain things in a more
Language Models are Unsupervised Multitask Learners,2019-02-14,d56fc616-888c-463d-bc0b-4caf7c85bd09,automated way.”
Language Models are Unsupervised Multitask Learners,2019-02-14,f355159d-1229-45df-a7ee-52e38b235229,Table 15. English to French and French to English translations generated by GPT-2.
Language Models are Unsupervised Multitask Learners,2019-02-14,3cdb718a-32d7-4f2e-adb7-1de0843e40a0,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,01f02ea6-9de2-43c0-8a9a-b42649b89d63,Context (passage and previous question/answer pairs)
Language Models are Unsupervised Multitask Learners,2019-02-14,15e858b4-77a4-418c-8384-7c636d44f5f5,"The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer"
Language Models are Unsupervised Multitask Learners,2019-02-14,8fcfabe1-316b-4348-93dc-f0638fc0129e,"Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in"
Language Models are Unsupervised Multitask Learners,2019-02-14,7e9b0114-1c93-48a8-b586-b8b320d7b1f3,"Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried"
Language Models are Unsupervised Multitask Learners,2019-02-14,7727b2d8-0024-4640-86c1-4107aa619771,"the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started"
Language Models are Unsupervised Multitask Learners,2019-02-14,f170e6f1-90fb-4130-b50e-af9a7d4199a5,ahead of the 1936 Summer Olympics.
Language Models are Unsupervised Multitask Learners,2019-02-14,5297db2a-513b-44eb-ab21-01fd7605a748,"After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch trav-"
Language Models are Unsupervised Multitask Learners,2019-02-14,f05278a3-03d4-40f0-9bfb-53d07e18c15b,"eled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was"
Language Models are Unsupervised Multitask Learners,2019-02-14,8c01bf1c-cada-4270-b9dc-9793df7fee7a,"following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing"
Language Models are Unsupervised Multitask Learners,2019-02-14,2da3f5a3-d822-4267-854b-385c69afeeb0,ancient links between China and the rest of the world. The relay also included an ascent with the ﬂame to the top of
Language Models are Unsupervised Multitask Learners,2019-02-14,904ceeb2-cebe-469b-8b1d-1e5f7bd7b746,"Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the"
Language Models are Unsupervised Multitask Learners,2019-02-14,f64d5f70-5864-4360-83ee-6e38225f19e8,event.
Language Models are Unsupervised Multitask Learners,2019-02-14,0f731d9f-be2f-4480-9eca-2cbe0df9ddf5,Q: What was the theme
Language Models are Unsupervised Multitask Learners,2019-02-14,a5827da8-45c7-4216-8fd2-b80036888413,"A: “one world, one dream”."
Language Models are Unsupervised Multitask Learners,2019-02-14,d2587513-0f52-47bf-95f8-3e10e83b1ce7,Q: What was the length of the race?
Language Models are Unsupervised Multitask Learners,2019-02-14,498587aa-ff19-4fca-bf1b-a761c9d7105c,"A: 137,000 km"
Language Models are Unsupervised Multitask Learners,2019-02-14,6b474d8c-a5b4-4814-b6c5-5a8e7d180e3e,Q: Was it larger than previous ones?
Language Models are Unsupervised Multitask Learners,2019-02-14,225a78f8-2aea-4fb2-a80d-10e6cab16783,A: No
Language Models are Unsupervised Multitask Learners,2019-02-14,a7a7c90a-1884-46b3-8b4c-b0ac83310962,Q: Where did the race begin?
Language Models are Unsupervised Multitask Learners,2019-02-14,903d7c36-27f0-499b-be44-6aef5c51e3a5,"A: Olympia, Greece"
Language Models are Unsupervised Multitask Learners,2019-02-14,2a8d5ccc-49db-4752-9d29-dcbbe2de729d,Q: Is there anything notable about that place?
Language Models are Unsupervised Multitask Learners,2019-02-14,9c1327b3-de57-4187-8086-36d8b21279d2,A: birthplace of Olympic Games
Language Models are Unsupervised Multitask Learners,2019-02-14,1fe66c7b-e73a-4ea4-94de-936ffa554cb1,Q: Where did they go after?
Language Models are Unsupervised Multitask Learners,2019-02-14,3feaf07f-cda2-477a-a37d-8b9e00c57f4c,A: Athens
Language Models are Unsupervised Multitask Learners,2019-02-14,e99dd484-2c53-4213-89ee-9610259bd1da,Q: How many days was the race?
Language Models are Unsupervised Multitask Learners,2019-02-14,d66039b1-df78-40ad-935e-8329cd7b7812,A: seven
Language Models are Unsupervised Multitask Learners,2019-02-14,63c98f23-e831-4eb8-a1b9-747c909e8194,Q: Did they visit any notable landmarks?
Language Models are Unsupervised Multitask Learners,2019-02-14,0035811a-f7df-433d-a32b-2c7c2e551486,A: Panathinaiko Stadium
Language Models are Unsupervised Multitask Learners,2019-02-14,7e81d9ee-651f-46c2-bbec-aa8101b7724c,Q: And did they climb any mountains?
Language Models are Unsupervised Multitask Learners,2019-02-14,d4c3c10d-b28f-4c6a-9c51-b0a8167de24c,A:
Language Models are Unsupervised Multitask Learners,2019-02-14,a264b144-251a-4ba6-a638-65d66d8f446a,Model answer : Everest
Language Models are Unsupervised Multitask Learners,2019-02-14,8657c9f1-dc07-44b5-a73c-b49cf65444bd,"Turker answers : unknown, yes, Yes, yes"
Language Models are Unsupervised Multitask Learners,2019-02-14,2cc2bc26-c84e-4782-9d83-cd36c8f7fa0c,Table 16. Selected CoQA completion.
Language Models are Unsupervised Multitask Learners,2019-02-14,e5ac3851-4fc7-4712-8269-6dedcb52fa57,Language Models are Unsupervised Multitask Learners
Language Models are Unsupervised Multitask Learners,2019-02-14,4a0d8032-59f1-4c19-990d-897e3c75ac50,Context (passage and previous question/answer pairs)
Language Models are Unsupervised Multitask Learners,2019-02-14,3f1fc4ca-c1fe-4905-8a5c-b7e4e6497381,"Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her ofﬁce at work and goes"
Language Models are Unsupervised Multitask Learners,2019-02-14,3c6a975d-0328-4e3b-a263-1ae173a24de5,"shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my"
Language Models are Unsupervised Multitask Learners,2019-02-14,39a4c19c-7a30-4112-a850-65ec273cfef2,"fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food,"
Language Models are Unsupervised Multitask Learners,2019-02-14,c14c9657-91de-468b-a32a-026aca555882,"paying his health bills and his taxes, but in fact Tom is a dog."
Language Models are Unsupervised Multitask Learners,2019-02-14,a6684ae3-620a-45be-9781-0cb19d6abfab,"Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life accord-"
Language Models are Unsupervised Multitask Learners,2019-02-14,bcb68b88-5f79-4948-bb56-c8d19ae66fb9,"ing to rules laid down by the government, which also provides a high level of care for its people. This level of care"
Language Models are Unsupervised Multitask Learners,2019-02-14,b189e931-92d2-4658-a2f4-725adda41663,costs money.
Language Models are Unsupervised Multitask Learners,2019-02-14,f1682c0c-799b-43d1-b09f-9525fcc64922,"People in Sweden pay taxes on everything, so aren’t surprised to ﬁnd that owning a dog means more"
Language Models are Unsupervised Multitask Learners,2019-02-14,88b7a622-08b4-4a51-90c6-347939076956,"taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which"
Language Models are Unsupervised Multitask Learners,2019-02-14,ed04d5aa-9131-4694-9775-a35e1e608c84,"is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most"
Language Models are Unsupervised Multitask Learners,2019-02-14,423160a3-70fc-47b7-90c7-1ba98205ba35,"such treatment is expensive, so owners often decide to offer health and even life for their dog."
Language Models are Unsupervised Multitask Learners,2019-02-14,a56732d6-e1ee-4459-8014-2bb1ddd3bf79,In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club ofﬁcial ex-
Language Models are Unsupervised Multitask Learners,2019-02-14,d4251827-e254-4f26-a4fb-c10a097d25e0,"plains what this means: if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay"
Language Models are Unsupervised Multitask Learners,2019-02-14,330e0a6d-f0a1-4aad-a303-5f4b75df7e7e,"for any damage done to the car, even if your dog has been killed in the accident."
Language Models are Unsupervised Multitask Learners,2019-02-14,ed4ac2fd-3633-437f-8ff8-4652373152ee,Q: How old is Catherine?
Language Models are Unsupervised Multitask Learners,2019-02-14,e044252a-71db-4e86-bb45-480ba082ad96,A: 54
Language Models are Unsupervised Multitask Learners,2019-02-14,0f6adf30-78c4-4e4c-951a-8c100556e135,Q: where does she live?
Language Models are Unsupervised Multitask Learners,2019-02-14,9838f5bb-b86d-4ffd-b8f8-f703770726bc,A:
Language Models are Unsupervised Multitask Learners,2019-02-14,b3ab9eb9-23b2-4b0a-8d94-59f75637877d,Model answer : Stockholm
Language Models are Unsupervised Multitask Learners,2019-02-14,99e24a31-b509-4dfe-bf15-f9f81daf056a,"Turker answers : Sweden, Sweden, in Sweden, Sweden"
Language Models are Unsupervised Multitask Learners,2019-02-14,941fbbe7-1790-4880-a5db-cf08ccf26f8e,Table 17. Selected CoQA completion.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a4cac5d0-a7dc-4733-bfd2-ee5f5e592fb1,LLaMA: Open and Efﬁcient Foundation Language Models
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,99868657-fc59-41df-a8eb-7f96a1b5f3c7,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61686548-7ab8-48ca-b5e0-8adf41ffd2f8,"Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f416bc2-a5d0-4b30-a6da-79fda2fdd401,"Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5cf5f453-a710-48e4-9509-2a5664e037d9,"Edouard Grave, Guillaume Lample"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3de3a52-9a13-44dc-b3eb-a89f8db12822,Meta AI
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a91af03d-c47c-48f7-90d6-b12f686ecb30,Abstract
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,db341daa-d3eb-4618-ad03-b4567519ef10,"We introduce LLaMA, a collection of founda-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,32752c25-0337-4d10-8b59-bda29706db4a,tion language models ranging from 7B to 65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,366a4e76-e627-4eda-9423-6d02825fb0c1,parameters. We train our models on trillions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,55d3d2bb-fedf-4313-9df8-1d34ccbbb0f0,"of tokens, and show that it is possible to train"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62d69b0b-f12d-4df6-8a25-96b9d75afc46,state-of-the-art models using publicly avail-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a554cfb9-af09-4ebf-8ea9-a5793043327a,"able datasets exclusively, without resorting"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,392bf009-b434-4a39-b8fc-990cc1994ce9,to proprietary and inaccessible datasets. In
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4e9afd0-baca-4790-a6aa-48881d2a8117,"particular, LLaMA-13B outperforms GPT-3"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a8cd74d-9dd4-4e6e-beed-b78bfcde6e33,"(175B) on most benchmarks, and LLaMA-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,51bba2d9-87b6-4ee4-a8f1-c70ac52df0b9,"65B is competitive with the best models,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e825c828-6c41-4351-91e8-93719637f070,Chinchilla-70B and PaLM-540B. We release
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb3711fb-cc9f-47a6-8aee-27938f8e2930,all our models to the research community1.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,94b792ee-6fc0-4907-aa6a-8526d3ef6772,1 Introduction
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,30b679be-4dfc-4a4e-920c-38ec8b01de83,Large Languages Models (LLMs) trained on mas-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1dd7c1c8-4333-4d6d-9cbb-4728fa3f9f92,sive corpora of texts have shown their ability to per-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7232082f-1c20-4186-b4ce-ee7a29415bb8,form new tasks from textual instructions or from a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9786dcb3-ff1e-4edc-824c-4db89c9ee526,"few examples (Brown et al., 2020). These few-shot"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3105b7f5-0c5f-4ab7-aef5-dab817671929,properties ﬁrst appeared when scaling models to a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b1e726f6-c1fc-4a76-a50e-b539eeed7c2c,"sufﬁcient size (Kaplan et al., 2020), resulting in a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c5cdb686-2e9c-4063-957d-01233b1b978c,line of work that focuses on further scaling these
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6f2d1713-6e5e-42fe-bbb5-9eb78cce0d4c,"models (Chowdhery et al., 2022; Rae et al., 2021)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b40a967-076c-4e82-9b5e-1dd738e30cc4,These efforts are based on the assumption that
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,612bf020-f677-4013-8b49-3a0fb89e27f3,more parameters will lead to better performance.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,68698734-ab81-4a56-a6bb-58539bd40a94,"However, recent work from Hoffmann et al. (2022)"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f376b68-6047-4268-a134-76393023ca56,"shows that, for a given compute budget, the best"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b8266b2c-27ce-402c-8b1b-d1b552044b7b,performances are not achieved by the largest mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,126eb4fb-3a43-4f9f-9450-465f4b6c4e42,"els, but by smaller models trained on more data."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c5bfc727-a985-43dc-84bf-7d47d9001c27,The objective of the scaling laws from Hoff-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ac25a4f-c1b2-4449-be5e-94c005c60aac,mann et al. (2022) is to determine how to best
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a2e104b5-39e6-4a4f-8f70-2899ff2dcda5,scale the dataset and model sizes for a particular
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a8391af4-61f8-4aa9-91e0-245dcdaea8b3,"training compute budget. However, this objective"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a054556c-1456-4b89-a349-4efbc29d524c,"disregards the inference budget, which becomes"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2e94f3f7-4867-45fa-970a-29da4bb019f9,critical when serving a language model at scale.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,60ad2ef1-2b96-46dc-b451-362971d0fc0c,"In this context, given a target level of performance,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a66cb9c-a06d-4d6c-b5db-213c1726424d,the preferred model is not the fastest to train but the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0db5ed72-76e7-499d-9f36-4227e1b4fd29,"fastest at inference, and although it may be cheaper"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2fe18fd5-0766-4ece-a3bd-894593939aa0,to train a large model to reach a certain level of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,34eabfd1-78f7-4648-97bf-db3c61a8b906,"Equal contribution. Correspondence: {htouvron,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,423c3a9f-7bc4-4ec2-83fd-0c5d6b9f28d7,"thibautlav,gizacard,egrave,glample}@meta.com"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e073ce11-f195-4d41-a113-c2a32acd92a2,"1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,111b245c-8f8c-41ac-b035-8f7dcc2aa18d,"ultimately be cheaper at inference. For instance,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,13f07fd7-09c1-4b85-92f7-875216a2d100,although Hoffmann et al. (2022) recommends
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ed9c9892-201d-4e05-8b5a-1c91879b5238,"training a 10B model on 200B tokens, we ﬁnd"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a4947317-af41-4a52-9d6a-06d9f6927e23,that the performance of a 7B model continues to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f6b5a7fd-68a1-4904-9fa9-16b2437b1103,improve even after 1T tokens.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dab50f4d-425e-4a56-b5f2-fc28b4cedaeb,The focus of this work is to train a series of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1508ab4f-d532-4748-bdc8-042e94d58fd9,language models that achieve the best possible per-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,442032e1-477c-4c29-8eb2-bde3bcfc713b,"formance at various inference budgets, by training"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce8a336a-ed6e-4ea4-a99c-2f59c43c6619,on more tokens than what is typically used. The
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8e39b06e-36e6-44dd-a66c-fd3ab43637fb,"resulting models, called LLaMA , ranges from 7B"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2e1bfe44-d536-40e3-95d1-0f2774cb20dd,to 65B parameters with competitive performance
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86ff9863-e6a1-485d-aa8c-670770762597,"compared to the best existing LLMs. For instance,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2db55d22-8dab-4c5f-ae0d-4212661e5985,LLaMA-13B outperforms GPT-3 on most bench-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d8d330ee-65a8-44dd-98bf-d935657d6989,"marks, despite being 10 smaller. We believe that"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e4eb70a-489b-428d-9102-c3cf28106ffc,this model will help democratize the access and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,267bbe53-f210-46fc-85fa-eda0532c354d,"study of LLMs, since it can be run on a single GPU."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,714ec5de-41dc-4c80-8a0a-d8562c715f25,"At the higher-end of the scale, our 65B-parameter"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,85d463f8-22e9-4c51-bf8b-2da76800aa9e,model is also competitive with the best large lan-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5acf53b8-3c56-4f8e-8b0e-fed2f05b7128,guage models such as Chinchilla or PaLM-540B.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,07e15ac6-e050-4370-9815-dff28af680f6,"Unlike Chinchilla, PaLM, or GPT-3, we only"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2ad9d25-24cc-44b6-932f-8d89a40f9cf7,"use publicly available data, making our work com-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2abd072-b0af-4bec-9d64-ceac87477bb0,"patible with open-sourcing, while most existing"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f64c596-8bd6-424d-a705-1cb44c9cb20a,models rely on data which is either not publicly
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45e64557-2ce4-4b11-a48e-47a9443a5c09,available or undocumented (e.g. “Books – 2TB” or
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,04b2481b-1fb5-4a21-a746-21384d83bc70,“Social media conversations”). There exist some
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df9cf314-740f-4d71-8204-5fc21db2e0ac,"exceptions, notably OPT (Zhang et al., 2022),"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dff4776c-3d2c-45ed-9359-1ffdea96f557,"GPT-NeoX (Black et al., 2022), BLOOM (Scao"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7940b6e6-8c6b-4aa2-ba9b-435e8f97eb8c,"et al., 2022) and GLM (Zeng et al., 2022), but none"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,18204df1-c9fd-4179-8004-556fe06e8b93,that are competitive with PaLM-62B or Chinchilla.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ba47804a-94a1-41bc-9505-4552f7392aa7,"In the rest of this paper, we present an overview"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7fc51be6-ecab-4f9f-b301-be13895e9912,of the modiﬁcations we made to the transformer
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,97241a60-a7b1-45ae-b56b-8a31983986aa,"architecture (Vaswani et al., 2017), as well as our"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,55e14da4-f23a-41d9-97e8-e095e4c82d9b,training method. We then report the performance of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b746532e-722c-420c-9676-08a80460843b,our models and compare with others LLMs on a set
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8c6a03e2-086b-4bfa-92ed-dca25417043f,"of standard benchmarks. Finally, we expose some"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ff89e5bf-97a3-4b61-bee1-c180dfc75039,"of the biases and toxicity encoded in our models,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,15e5d5f3-a969-4f89-9b89-0d5af5a57265,using some of the most recent benchmarks from
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,766cc596-78df-49c9-9c83-207885b0001d,the responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,caff9a31-c92c-4606-a696-baf705ed09d6,2 Approach
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f39cc2b-197e-48a9-98b7-65203fdf60e0,Our training approach is similar to the methods
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c7f772d0-5d70-4b4b-8e3d-4d67c0a6cb2f,"described in previous work (Brown et al., 2020;"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,581a00ca-f9a9-4802-bb22-514147be1dae,"Chowdhery et al., 2022), and is inspired by the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d85f12e0-cfb3-44e2-bae9-c53c4dc3c79a,"Chinchilla scaling laws (Hoffmann et al., 2022)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e79f224-d180-4e98-aeff-d54bf740cffa,We train large transformers on a large quantity of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b54d18e-a974-4851-a1eb-e6eb5eb77047,textual data using a standard optimizer.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f39c733a-4846-44fd-a9bc-33dcf5c3bf2d,2.1 Pre-training Data
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,11b02b30-3050-448d-82f0-776f2795babd,"Our training dataset is a mixture of several sources,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f3243b14-f281-45d2-a52d-8c18da9bd2cf,"reported in Table 1, that cover a diverse set of do-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4dfb6d1-d282-4ed5-abd4-66b51322c9c3,"mains. For the most part, we reuse data sources"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,20549eab-3b28-4181-b0b8-fd417519a68c,"that have been leveraged to train other LLMs, with"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,620fce2f-9c38-4507-b83a-6ce0aaff8dc8,the restriction of only using data that is publicly
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8bd2bbad-0928-4352-bd1d-f4c425b8a868,"available, and compatible with open sourcing. This"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f63aebe-67e3-4370-8ee8-95487542c88f,leads to the following mixture of data and the per-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5202ce37-9190-45a9-99aa-c7e94559377a,centage they represent in the training set:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3f956e6-3584-41cc-a9ef-284f9afda12d,English CommonCrawl [67%]. We preprocess
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dbba263f-393f-499c-8821-80d653832da8,"ﬁve CommonCrawl dumps, ranging from 2017"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a653fda1-dea5-4d59-a9ff-319daf214e4a,"to 2020, with the CCNet pipeline (Wenzek et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,266ca7e9-b77d-485c-9b8e-aff902ca729a,2020). This process deduplicates the data at the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2bae806a-57f7-4ea6-b07c-e5542c58081a,"line level, performs language identiﬁcation with"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b27bd620-7f81-4225-9453-262944923127,a fastText linear classiﬁer to remove non-English
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,34d0dbc7-c004-4d2e-8539-4317ba9887b3,pages and ﬁlters low quality content with an n-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78e0c070-8725-4a17-a8d3-aff1ab0fe6d2,"gram language model. In addition, we trained a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bd608de3-d87b-447c-9b49-12e61ad5decf,linear model to classify pages used as references
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fc586335-5f59-431c-acc7-e8cee8f98276,"in Wikipedia v.s.randomly sampled pages, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,348daedd-bbc2-41eb-a82d-4f1ab95e4052,discarded pages not classiﬁed as references.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79b99051-2ba3-4826-b7be-474d3db8b5f3,"C4 [15%]. During exploratory experiments, we"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,12eafa46-a6d0-4391-abd4-3341a5d8310a,observed that using diverse pre-processed Com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62322bf9-3467-4d68-a7f2-d856ee8b0e96,monCrawl datasets improves performance. We thus
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eaaca32a-3207-4149-a8f8-56d7ea76b14c,included the publicly available C4 dataset (Raffel
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,59abd92f-7052-4e8e-b4a6-5812c6b9df55,"et al., 2020) in our data. The preprocessing of C4"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,90af2247-d2e0-4e69-ae74-6363bc01e971,also contains deduplication and language identiﬁ-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2f59c62-bebb-4a21-94cb-91c043c1e791,cation steps: the main difference with CCNet is
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f2c64fc-78d2-4275-8033-6f448aa55fb3,"the quality ﬁltering, which mostly relies on heuris-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df38fa7d-2e48-4a5f-b7bb-b133592fe7a1,tics such as presence of punctuation marks or the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d43e2a2-87b7-42ab-b565-f58ebb87ef5a,number of words and sentences in a webpage.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,abf9de7d-8f59-4a2c-901d-71943aefa54b,Github [4.5%]. We use the public GitHub
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86093e4f-9ae2-4c70-9737-a1fec462d8f6,dataset available on Google BigQuery. We only
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a63e85f9-cae7-43b7-87fd-d47d7f7f80bd,"kept projects that are distributed under the Apache,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b7cbc74-b8f6-48d1-aa52-8a93dfe12bf9,"BSD and MIT licenses. Additionally, we ﬁltered"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fb1dc23b-9706-4f86-b5e1-e45fe18ad32a,low quality ﬁles with heuristics based on the line
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fec22168-717e-4318-90d7-8f7650575d76,"length or proportion of alphanumeric characters,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,561fc0f7-d337-4231-844a-07f8af079917,"and removed boilerplate, such as headers, with reg-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2484a821-d12e-416e-bcf3-6a0a4262d950,"ular expressions. Finally, we deduplicate the result-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e7d65f9-41eb-4fb8-8ac3-62c142fcb440,"ing dataset at the ﬁle level, with exact matches."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6567cec5-67b6-4f5c-9e4f-317392e7cf9c,Wikipedia [4.5%]. We add Wikipedia dumps
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,765e5475-fdd9-49a4-8d54-7a0d9c093ac9,"from the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,60046085-66f8-4e26-bbda-378c5a9fa0dc,CommonCrawl 67.0% 1.10 3.3 TB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a0822bf-afb9-4b2c-bbc4-3c4d16d06e66,C4 15.0% 1.06 783 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2aed1cfb-46ae-4279-90c4-b4ce0116f85c,Github 4.5% 0.64 328 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b5e7f188-7d16-4427-b65e-f5a2b62f4c08,Wikipedia 4.5% 2.45 83 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e764b8ec-771b-452f-8557-840c5926171d,Books 4.5% 2.23 85 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,12a8f3c6-5570-45f1-9b05-52a9fe984c10,ArXiv 2.5% 1.06 92 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0972ba36-584a-4d67-95d0-42cba7cb0f98,StackExchange 2.0% 1.03 78 GB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f96360d2-5c8d-4a1b-8177-9d639f85f963,Table 1: Pre-training data. Data mixtures used for pre-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9b453a49-9c41-46f9-ab83-ab7525a409dd,"training, for each subset we list the sampling propor-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a5c05c39-42d9-4ac3-b081-be5a057827f4,"tion, number of epochs performed on the subset when"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79b7e2d5-13af-4c4c-9bd3-8296434efee8,"training on 1.4T tokens, and disk size. The pre-training"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,20e90675-a3fa-44d6-aab7-839abc3053cc,runs on 1T tokens have the same sampling proportion.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9363a786-0226-4ae6-ad44-5367bb6b56d6,"languages, which use either the Latin or Cyrillic"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c4cf86cd-5998-4154-98c2-5bb6169bf4e6,"scripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a5052e9b-cc23-4a85-958c-8601a0653256,"nl,pl,pt,ro,ru,sl,sr,sv,uk. We process the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8abf9800-c214-4dad-8249-fe507465275a,"data to remove hyperlinks, comments and other"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f044e8ee-b3a9-4218-a5fc-766891731fa4,formatting boilerplate.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b7dd9898-b0bd-407b-a507-064343bd1cc1,Gutenberg and Books3 [4.5%]. We include
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9024608-9992-4b8e-be2b-ad98660cc496,two book corpora in our training dataset: the Guten-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3b93f0c8-9313-45c7-82bd-28e0067bd6d3,"berg Project, which contains books that are in the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8b072f1c-d1e2-4d6d-a51a-c9abdb26684b,"public domain, and the Books3 section of TheP-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8d7319d5-0cad-415e-a604-3049dcc745f2,"ile (Gao et al., 2020), a publicly available dataset"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67a56643-0e03-46df-94f4-eb5d7fb41c8b,for training large language models. We perform
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e8323c6-0a95-45cb-859d-39a256c4c193,"deduplication at the book level, removing books"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9c03bf4d-7d70-4f0b-8262-3555f20260e0,with more than 90% content overlap.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dfa4a598-5857-4813-ba42-36e49a8d0a44,ArXiv [2.5%]. We process arXiv Latex ﬁles
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,310dce5d-a569-42cd-92bc-98bc5c950b34,to add scientiﬁc data to our dataset. Following
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,87b9c1c7-03a1-4120-a0a7-b101f8e196fa,"Lewkowycz et al. (2022), we removed everything"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0ce9ae58-c8db-43bb-908f-b60d24cd0a75,"before the ﬁrst section, as well as the bibliography."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,48925711-77d3-42ac-8595-03d3185be57b,"We also removed the comments from the .tex ﬁles,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ebf54f9c-6ee1-426c-b469-68418f0a8ea6,and inline-expanded deﬁnitions and macros written
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,53e77fd4-578c-42c6-b3f6-c0b757984f01,by users to increase consistency across papers.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4c45882b-1698-4f48-9bb9-f06af6befb7b,Stack Exchange [2%]. We include a dump of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b3fa694-bce2-49ee-bc5f-36c0766a2296,"Stack Exchange, a website of high quality ques-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e5dcac1d-b339-4b69-a6ec-986dca434686,tions and answers that covers a diverse set of do-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,415f1beb-96d4-4b75-8a3c-8b5158dcd94a,"mains, ranging from computer science to chemistry."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a8cd3e87-1017-48c4-b8d5-4b7efc4113ac,"We kept the data from the 28 largest websites, re-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72bc8cf1-9d8b-4e7e-98b4-7b02f310d8de,moved the HTML tags from text and sorted the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dc7e4963-4c3e-424d-8895-e6aa6f99eab4,answers by score (from highest to lowest).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d16c90b5-e73d-40e1-9adc-ec16f9a75d48,Tokenizer. We tokenize the data with the byte-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f17686ef-d551-4ca2-8178-5be6f2d0c57a,"pair encoding (BPE) algorithm (Sennrich et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a59b3f6-5a01-4298-baeb-f4a8de175c36,"2015), using the implementation from Sentence-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,35dc5399-b4d2-4fe7-afa8-7270afb72b02,"Piece (Kudo and Richardson, 2018). Notably, we"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,51a17cd9-19f7-48c6-916b-7e1320bb5767,"split all numbers into individual digits, and fallback"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,54af241b-b0de-4120-8a68-1a4a2bb00713,to bytes to decompose unknown UTF-8 characters.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72b87492-1f9d-47d9-8741-c3b85aa02cc9,params dimension nheadsnlayers learning rate batch size ntokens
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a00b7cbd-d069-4353-9809-405a788494ee,6.7B 4096 32 32 3:0e 44M 1.0T
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f9f3752-8c65-4346-a864-3160e4d46c53,13.0B 5120 40 40 3:0e 44M 1.0T
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ecb1d501-f967-4ed7-9231-7fe820eef6c5,32.5B 6656 52 60 1:5e 44M 1.4T
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8318115b-5a19-4563-bfef-e556493afd88,65.2B 8192 64 80 1:5e 44M 1.4T
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4256f517-c3ee-4cf9-bd3e-a2d125363e6c,"Table 2: Model sizes, architectures, and optimization hyper-parameters."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,572b7a2a-f4b5-4cf3-8eb2-277bffc8399c,"Overall, our entire training dataset contains"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ede43cb-8b20-4f80-861a-109d8aaf970d,roughly 1.4T tokens after tokenization. For most of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7556881b-7dd0-4123-bc59-c6dba4efcb56,"our training data, each token is used only once dur-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8a1620df-c428-4316-aed2-082e7723277f,"ing training, with the exception of the Wikipedia"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9c42789d-45bb-49ee-acc0-120ba15d2ada,"and Books domains, over which we perform ap-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0509ab98-8bc2-4aa4-931a-c377b3fac13f,proximately two epochs.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7aea7e5a-96f0-41ee-939d-5ce322796079,2.2 Architecture
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a317e85-813a-4acd-af46-a5af3a89145b,"Following recent work on large language models,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25f5d511-27fb-4e7c-8af0-20cf23b50f9c,our network is based on the transformer architec-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5ff2cb2f-4357-4946-b127-030b812cef34,"ture (Vaswani et al., 2017). We leverage various"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad5502e5-a385-49ad-b792-e5b50e790078,"improvements that were subsequently proposed,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f4db40e0-364f-4f67-bfd8-bc384f64b7be,and used in different models such as PaLM. Here
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,865fd3c2-d9c2-4060-824b-0b4d9d9c8ec2,are the main difference with the original architec-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7d1d65eb-b544-49bc-8cdf-181db2631e93,"ture, and where we were found the inspiration for"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6341f31a-a7be-4869-a6ba-67b34fadb2aa,this change (in bracket):
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b4c7bde6-3769-4e80-a3dd-ce7da655f64d,Pre-normalization [GPT3]. To improve the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1b42853c-f977-41dd-b6c9-a5e54ceebbaf,"training stability, we normalize the input of each"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c3ee7e1-2e43-41ce-8972-ec74e7cb8050,"transformer sub-layer, instead of normalizing the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dba6e3c3-46cc-4f0f-9684-6888fdbeb1ba,output. We use the RMSNorm normalizing func-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c1e32c5c-f139-45ed-bbba-78c594eb0114,"tion, introduced by Zhang and Sennrich (2019)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,66be3cd9-8b0e-4f33-903d-d16ed19c1229,SwiGLU activation function [PaLM]. We re-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,42e901b0-a3e6-4d60-8bf6-b667b974677d,place the ReLU non-linearity by the SwiGLU ac-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,40d445fd-0343-40ff-b700-313a14bec444,"tivation function, introduced by Shazeer (2020) to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,36585827-b3d2-4a44-a047-75ff84adb87c,improve the performance. We use a dimension of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,304ebee3-6a97-4bf9-99cb-fbdaaaba78be,2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a87cbcc6-2224-4a35-8de1-eff15b26883c,34dinstead of 4das in PaLM.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d85e97cf-c2dc-4f95-aa6f-3edc8d125eca,Rotary Embeddings [GPTNeo]. We remove the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,75719b54-7f79-4e05-b90b-9c71b2dbbd03,"absolute positional embeddings, and instead, add"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,41d67883-9ddf-4903-b91d-17ede6117357,"rotary positional embeddings (RoPE), introduced"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f259ed8-9cca-4d25-8b5c-4800245beabc,"by Su et al. (2021), at each layer of the network."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4a7bdf87-748b-4eea-8215-2c3a2832cb64,The details of the hyper-parameters for our dif-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d50f4027-a40e-4f95-a254-570fed59ab59,ferent models are given in Table 2.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f9c36164-9546-4c61-bd97-a277ff8b9e73,2.3 Optimizer
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1c0800f-8aac-4f72-8804-dd50cdf8ea60,Our models are trained using the AdamW opti-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ff1cae9-4b20-452b-92df-c2c0d5b6786c,"mizer (Loshchilov and Hutter, 2017), with the fol-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,82e35258-d367-46cb-a2fd-85779690b67f,lowing hyper-parameters: 1= 0:9;2= 0:95.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64753017-ec3a-4a51-a41a-1695cf7d14fd,"We use a cosine learning rate schedule, such that"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad966a55-75dd-4cb3-8e2b-8649e0e3ff53,the ﬁnal learning rate is equal to 10% of the maxi-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ee00f463-cfc7-47c7-87cb-fb96275ea485,mal learning rate. We use a weight decay of 0:1and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df8581bd-6608-455d-a6b6-9259bf553b5b,gradient clipping of 1:0. We use 2;000warmup
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,59fea3df-a1b7-4940-b457-b2b0bad9a5d4,0 200 400 600 800 1000 1200 1400
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,af1f2746-f21d-4d04-816e-82a91bc4cbbb,Billion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cd30606c-5192-4c6a-b21e-0133404436f7,LLaMA 13B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,524173c9-f1ea-4dc9-8928-605b62b337b9,LLaMA 33B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8c39e9f-d47a-4c1a-ae53-d6af55dcf192,"LLaMA 65BFigure 1: Training loss over train tokens for the 7B,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e30c473d-2c7e-453c-a48c-48736c126640,"13B, 33B, and 65 models. LLaMA-33B and LLaMA-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6cd534d4-54de-46e2-a9af-7238b1cfed9f,65B were trained on 1.4T tokens. The smaller models
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b26f04c6-b24b-4170-b472-bd3f2e822df6,were trained on 1.0T tokens. All models are trained
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,58d001bd-863d-4802-8aad-b8f7c29e56db,with a batch size of 4M tokens.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8b098eb6-d1f8-4793-9717-3e9234077a11,"steps, and vary the learning rate and batch size with"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6200ea39-9b79-4f78-b9a2-9c0c84d52859,the size of the model (see Table 2 for details).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a798621b-ba0c-4f03-a5c2-eb41b37247cb,2.4 Efﬁcient implementation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5ce78cec-9f76-46d6-8c85-dbe902c3a3e8,We make several optimizations to improve the train-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5b294d6b-3e0d-4e19-b8bb-d8d6a05f4850,"ing speed of our models. First, we use an efﬁcient"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dcd31916-8658-40bf-9368-bdb3c1af5a0c,implementation of the causal multi-head attention
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d8b93fc-6d5d-4598-9acb-23eb5610db8c,to reduce memory usage and runtime. This imple-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c0fb173-030c-4c20-8245-400a0a4ee732,"mentation, available in the xformers library,2is"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b6b4b64-7dff-4786-bf06-0dabd176bff0,inspired by Rabe and Staats (2021) and uses the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,748379f4-9b21-474b-8795-e28ec249d16b,backward from Dao et al. (2022). This is achieved
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cccd766d-b68c-4aee-90fd-d6223925abb5,by not storing the attention weights and not com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c0d31f5e-fb63-4dec-93fa-19ac0bbabbde,puting the key/query scores that are masked due to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,75fba2d3-12f6-4740-a603-5bbec53e6ac3,the causal nature of the language modeling task.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4aa17651-f935-4d2c-b596-56be8c086d7b,"To further improve training efﬁciency, we re-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3525af7f-f87b-444d-bb90-314791f8b9ad,duced the amount of activations that are recom-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f93e428-8c17-47c9-86e6-5b7c816dab89,puted during the backward pass with checkpoint-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,abeb5e55-bd83-4bbc-afd3-2d87ada1db37,"ing. More precisely, we save the activations that"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,97416d37-50ed-4d35-b5d4-164b0697b52a,"are expensive to compute, such as the outputs of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,392c788d-40db-4d9d-9d77-f5c0c48cc411,linear layers. This is achieved by manually imple-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c55543e3-288e-48c7-b01d-8a83904c0905,menting the backward function for the transformer
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dab86a22-5b95-4319-80b9-e94e9cfd3283,"layers, instead of relying on the PyTorch autograd."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad74cb07-803d-4e28-ba85-491c4c4483f8,"To fully beneﬁt from this optimization, we need to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c3cf9639-179c-4074-87c9-1c701cf068df,2https://github.com/facebookresearch/xformers
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,58737dd7-51b9-4a39-a33f-d10a65ef48d1,BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2cf8554f-abcd-4765-a2f7-f5e267774938,GPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0871faa9-cd67-4908-8cec-5ce14d9adaa7,Gopher 280B 79.3 81.8 50.6 79.2 70.1 - - -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62176c0d-8de7-4bfa-a8ca-27be68d6f0c8,Chinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4e07b7d6-2ce0-45c9-86aa-ebefeaea80c1,PaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f1aa32a-0464-4393-8dcb-7d8b32c5834a,PaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,415bf98b-1dd0-44d0-9684-2fea6efc395c,PaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2df8737d-6449-484c-8690-55c72895fcd1,LLaMA7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,281cbe83-e522-49da-9f6c-dc70eff73ac8,13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2038fcc1-59d9-4126-9635-631279b99fed,33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,471f0d1e-0b78-47ae-8059-8608a205f822,65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b606584b-6ac6-497f-90d0-0aefd7f1cd9a,Table 3: Zero-shot performance on Common Sense Reasoning tasks.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0014d0b0-ac8b-4671-afe7-910ecdbfa37b,reduce the memory usage of the model by using
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f652ed4-2f71-483c-b32d-a90ff0fbcf34,"model and sequence parallelism, as described by"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,02dbb57c-2a56-4aa5-97b5-268dac506378,"Korthikanti et al. (2022). Moreover, we also over-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a6f891f3-6fe0-4fbe-a1be-47eac66dec16,lap the computation of activations and the commu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f1990b0d-f9ac-4e1f-8bb4-5f40374455b6,nication between GPUs over the network (due to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1463fa8c-46ea-4e57-8563-bc9c03a46310,all_reduce operations) as much as possible.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ca927b9d-918f-4692-bd1f-79db52ea1757,"When training a 65B-parameter model, our code"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,990194bc-d5f3-4868-95f8-3a3ef3c803d1,processes around 380 tokens/sec/GPU on 2048
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,322772f1-fa5f-4ab0-8263-11f935abc8ff,A100 GPU with 80GB of RAM. This means that
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fbde7e80-9e29-4281-bd9a-0c2c07280607,training over our dataset containing 1.4T tokens
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe18e249-8530-4d85-9372-043f1fc1d5f2,takes approximately 21 days.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,65813c3d-a387-40b4-bda1-40cd132486c2,3 Main results
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,584046b4-8c79-4e3f-9c95-2f0c96e26e53,"Following previous work (Brown et al., 2020), we"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e73e74aa-9431-438a-9fb9-002618fbc2da,"consider zero-shot and few-shot tasks, and report"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,77e290f4-1d4b-4741-a687-bcdf73bb556c,results on a total of 20 benchmarks:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f130a96-888d-4fd3-88c6-60400f73425f,•Zero-shot. We provide a textual description
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,92645399-2b24-4b6c-b49c-8f32e8df3584,of the task and a test example. The model
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b2dd30e1-f497-4b82-a21e-4f8ec64715ec,either provides an answer using open-ended
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0c2bf921-e506-445b-b61d-ee4440f7e5de,"generation, or ranks the proposed answers."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22214f58-f5fd-4b4b-8319-f06b619f2df8,•Few-shot. We provide a few examples of the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3987a175-9670-4097-869d-c38e53c8bd0b,task (between 1 and 64) and a test example.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,20d83012-af41-4491-b9be-8cc9aade0ef9,The model takes this text as input and gener-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,049ed496-100d-4d0a-94b7-6d5cced7fe48,ates the answer or ranks different options.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dcdb34dc-cff1-4341-a8dd-a8a4af42d0a0,We compare LLaMA with other foundation mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1837f4e-0465-4145-8bb2-dcda2f49320a,"els, namely the non-publicly available language"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4117b6d3-b7f1-4daf-a336-a9d778adf6c2,"models GPT-3 (Brown et al., 2020), Gopher (Rae"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,31fa6fe4-51ac-4dff-8770-4ad063cc9869,"et al., 2021), Chinchilla (Hoffmann et al., 2022)"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3aa8bfd4-26aa-492e-9459-329d17d898cf,"and PaLM (Chowdhery et al., 2022), as well as"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8afa1703-1018-4f68-b069-9cc78e0a2a83,"the open-sourced OPT models (Zhang et al., 2022),"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0a03f247-8d73-4f30-ad89-a4ffe8ec32b0,"GPT-J (Wang and Komatsuzaki, 2021), and GPT-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,983e9fc6-b05f-47ee-80df-8aabcdd6c8bb,"Neo (Black et al., 2022). In Section 4, we also"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be3489a0-11f0-4512-a59c-24f4743595d1,brieﬂy compare LLaMA with instruction-tuned
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,940bb0ed-365a-4dcc-bc7a-952e44a137a4,"models such as OPT-IML (Iyer et al., 2022) and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e8a9e36e-3371-4395-b0ce-9818068646be,"Flan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce9bd35f-98e7-4a33-86c6-ed855e205470,tasks and multiple choice tasks. In the multiple
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,abbfa085-88ee-4024-bc97-af78bd779f00,"choice tasks, the objective is to select the most"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df96aebb-c24c-4596-ac82-ff4e26aeec04,appropriate completion among a set of given op-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5488dd50-0654-433e-bca9-614c5e243d90,"tions, based on a provided context. We select the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,31fb00e8-9f2e-436f-ab8f-3459fe17bc5a,completion with the highest likelihood given the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,baa518c1-83c5-4c19-8ffe-7248835620e6,provided context. We follow Gao et al. (2021)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4b26bafb-a5fe-4dea-9760-d9dd66757a9c,and use the likelihood normalized by the number
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5fb3585-0ccc-4830-9a3b-798e5f8f6315,"of characters in the completion, except for certain"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d702eca-8306-4ce6-a758-88e03c013f9e,"datasets (OpenBookQA, BoolQ), for which we fol-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b246cf73-7bc0-4d8d-968d-10695f7ea77f,"low Brown et al. (2020), and select a completion"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9b8db1f-4fd2-44dd-a39a-a301ca1dc218,based on the likelihood normalized by the likeli-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f1e53adb-fdcb-4e88-a242-b0113cd5df66,hood of the completion given “Answer:” as context:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f15f4adc-e105-4246-a39f-18ddb401ee81,"P(completionjcontext )=P(completionj\Answer :"")."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5c8d64a8-fa16-4b49-b330-00d2e956b1f6,0-shot 1-shot 5-shot 64-shot
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,925c0240-a1df-4e15-89f1-700b19640f3b,GPT-3 175B 14.6 23.0 - 29.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79365881-2b9e-41e2-a178-b506cd75187e,Gopher 280B 10.1 - 24.5 28.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4cce0fa2-a52d-4426-a10c-55f5a50f846c,Chinchilla 70B 16.6 - 31.5 35.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61991202-faa2-499b-96f2-f28045645fd1,PaLM8B 8.4 10.6 - 14.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ae54c425-45df-4a7e-a2b7-632b022842d9,62B 18.1 26.5 - 27.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,32e7b3ff-2773-41e4-89f0-930e8fee0d11,540B 21.2 29.3 - 39.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c5a79a5-0c20-45dd-addb-b60830d4d7d8,LLaMA7B 16.8 18.7 22.0 26.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0490e857-2b62-476f-813a-b6f31f2d5727,13B 20.1 23.4 28.1 31.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c1ce5c45-fa14-4741-8fa6-e00e751ec12a,33B 24.9 28.3 32.9 36.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e7f084c-0133-4ac8-bb33-5339941f6c74,65B 23.8 31.0 35.0 39.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,948f36e8-034a-4506-a466-580ef8314fa6,Table 4: NaturalQuestions. Exact match performance.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ca70461-7921-4d4b-9338-2cb5aa599d6c,3.1 Common Sense Reasoning
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe5a6bd2-d6c9-4f15-b6ef-62072af49a4e,We consider eight standard common sense rea-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25527736-a4a0-47bf-be4b-f957b52fd2a8,"soning benchmarks: BoolQ (Clark et al., 2019),"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d149be33-1efd-4497-bda0-8be2ffd97b40,"PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64b25575-cf31-4def-8990-3d9fa2d75306,"HellaSwag (Zellers et al., 2019), WinoGrande (Sak-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c6f24cd5-ae90-4250-a2f1-9547c136c3f4,"aguchi et al., 2021), ARC easy and challenge (Clark"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,499896b4-1be8-4f5b-9c06-c65e111cc3d6,"et al., 2018) and OpenBookQA (Mihaylov et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eea0f547-a0cb-4cfb-8b4d-b94b4d521512,2018). These datasets include Cloze and Winograd
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3e6d0a31-bc58-44aa-bed9-f61fcfc62293,"style tasks, as well as multiple choice question an-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,335995af-db9e-4998-af6b-9894b79cb645,swering. We evaluate in the zero-shot setting as
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,139e236c-e60c-43a1-891c-2dee0ae2f273,done in the language modeling community.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b5ff53d7-bed6-4e24-97a0-99829bbbd61c,"In Table 3, we compare with existing models"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9ff15ea-93ec-4110-a784-0ac8c7a01ba0,of various sizes and report numbers from the cor-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df607995-2770-4e95-ac54-898a20cd7f4f,"responding papers. First, LLaMA-65B outper-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,990681e3-6bd8-40b0-91de-b406eda7a30a,forms Chinchilla-70B on all reported benchmarks
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,253f1a18-5cd1-4fe1-9a31-382ee8b61200,"but BoolQ. Similarly, this model surpasses PaLM-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2c4b0f17-6345-4cdf-9ffe-edc9fbc8b81c,540B everywhere but on BoolQ and WinoGrande.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b4bbc89-cb5a-443e-a252-78a5cef57d81,LLaMA-13B model also outperforms GPT-3 on
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c05e6585-ddc2-43a9-9070-5aeb2568b660,most benchmarks despite being 10 smaller.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6e5800bf-4e55-4d37-a4e8-1c4dd133e3cb,3.2 Closed-book Question Answering
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8ad28e26-27af-4a32-a25e-91b512372366,We compare LLaMA to existing large language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a34e09e2-e58b-4670-b86a-c10dd127471c,models on two closed-book question answering
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,932f2af0-3d6f-41cd-9f50-4facaa514ab1,benchmarks: Natural Questions (Kwiatkowski
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ab65603d-4084-46b6-91d0-7df9c40dfeba,"et al., 2019) and TriviaQA (Joshi et al., 2017). For"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,653e4083-8660-46df-806b-2a86f5c35c3e,"both benchmarks, we report exact match perfor-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79cf1fd6-c967-46a7-bb9c-699ecc0c9235,"mance in a closed book setting, i.e., where the mod-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,41e6e04c-76fd-4a1b-bef6-c5d20988d6d8,els do not have access to documents that contain
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8a6852e9-ff51-4fe5-97e6-31b4bd3e9e24,"evidence to answer the question. In Table 4, we"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22af61f8-1dfc-4a3c-8987-7f10899adcb8,"report performance on NaturalQuestions, and in Ta-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,88e432f4-766b-4c22-871a-a648d9cd0b86,"ble 5, we report on TriviaQA. On both benchmarks,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e64df3f3-c224-49c6-b450-dd0cf27527f5,LLaMA-65B achieve state-of-the-arts performance
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aac52395-202f-4fe6-b4ac-b644bc318750,in the zero-shot and few-shot settings. More im-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2562c390-49b6-4d09-8ccc-c3adabf1da5b,"portantly, the LLaMA-13B is also competitive on"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ec43a3d3-5968-478c-ba31-d7f1213298f3,"these benchmarks with GPT-3 and Chinchilla, de-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4299ff8-6fa7-4583-9829-dbd0ba4eff9f,spite being 5-10 smaller. This model runs on a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86db5aaf-1712-40de-a3a8-6dba164fd3ca,single V100 GPU during inference.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3530215a-83de-4a6a-a9ee-10a61112a587,0-shot 1-shot 5-shot 64-shot
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,85f9a191-2765-4db5-b3d7-bfe22c1c6bf1,Gopher 280B 43.5 - 57.0 57.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23344270-4d45-4f67-981c-519bfaf28c28,Chinchilla 70B 55.4 - 64.1 64.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e2a0ee4d-67be-4f83-a2f1-396d1fa3611c,LLaMA7B 50.0 53.4 56.3 57.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,03af9d0e-d67d-4e49-a55b-57d538a9f614,13B 56.6 60.5 63.1 64.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,00c84d7f-6e99-444c-b02f-7dfdd3daac35,33B 65.1 67.9 69.9 70.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,77b41e83-8be6-4dbe-aefc-925386d67ab7,65B 68.2 71.6 72.6 73.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f001ebdb-fdee-48e6-81fc-163fec98772c,Table 5: TriviaQA. Zero-shot and few-shot exact
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8fe163f-7f69-421e-bf19-c62aef446502,match performance on the ﬁltered dev set.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c458728-bb3b-4548-a494-a2f7f3ea8d7e,3.3 Reading Comprehension
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f771d143-53c9-4274-aa58-d87b9348cd6e,We evaluate our models on the RACE reading com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d2960450-18a8-43af-ae6c-e4d659da3f11,"prehension benchmark (Lai et al., 2017). This"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4b9b42e4-e50b-451a-8ecf-016f6c6efeba,dataset was collected from English reading com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86a516ce-473f-42fd-af88-e84865360668,prehension exams designed for middle and highRACE-middle RACE-high
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d59a575b-af09-4f65-b45c-90e75db22f8d,GPT-3 175B 58.4 45.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,07cdf931-60b3-4272-a8ef-3a53214b214d,PaLM8B 57.9 42.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fede44f4-ac08-476e-9e5a-40e9cc2ed302,62B 64.3 47.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c5d742fa-0b03-4792-a99a-4eed36ac8832,540B 68.1 49.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a460c5e4-b6fd-4ead-bea3-0aa4c36668e5,LLaMA7B 61.1 46.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f938d50-9910-4ee5-91fd-ca72028d8ccc,13B 61.6 47.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9aeb0c50-4a35-4af4-8e79-e5df8b1583ec,33B 64.1 48.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71bad2b7-5db4-479d-a486-0ea1b50e5cd0,65B 67.9 51.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fb55b532-0413-4498-a932-9688a71d2e65,Table 6: Reading Comprehension. Zero-shot accu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5d7e53bd-7a8b-49ff-871e-2a23707cf92b,racy.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb3765b3-df14-4ab3-81e7-0deda8891d4f,school Chinese students. We follow the evaluation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,edf95c16-696e-47c9-8eec-1c3abc73d553,setup from Brown et al. (2020) and report results
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67385438-b0ed-4439-bcd2-a5522d241c96,"in Table 6. On these benchmarks, LLaMA-65B is"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,600a7e64-42ea-45a2-995f-f8be5b27ed8a,"competitive with PaLM-540B, and, LLaMA-13B"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e06a43ef-bab7-4059-a4ec-28d266093f20,outperforms GPT-3 by a few percents.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9b489cd2-af65-459c-9549-dec4a80e46b8,3.4 Mathematical reasoning
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bddec3e2-4733-44c2-b78c-19916537da27,We evaluate our models on two mathematical rea-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e5e72587-e1ea-4f1a-8140-69a205b47a70,"soning benchmarks: MATH (Hendrycks et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0e6b1b8e-6995-42af-a876-d72c82fe7c1c,"2021) and GSM8k (Cobbe et al., 2021). MATH"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d38486fc-a325-44b0-8e63-0351c9ab0d6d,is a dataset of 12K middle school and high school
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9751ffd-f715-4413-a711-7e831d368f3b,mathematics problems written in LaTeX. GSM8k
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b74e746-7322-4e41-9a57-1de84a3f0481,is a set of middle school mathematical problems.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5e13186-e036-4556-969c-769b38231e9c,"In Table 7, we compare with PaLM and Min-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d9dee2f-9d57-4f3c-8aa2-38dfa0717b34,"erva (Lewkowycz et al., 2022). Minerva is a series"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,36466041-8970-47e3-b9fd-0b044bcd4bc2,of PaLM models ﬁnetuned on 38.5B tokens ex-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62e4b7ef-9651-4cd1-988e-61df6bf7234d,"tracted from ArXiv and Math Web Pages, while"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cdb2f0a4-33e0-4e11-8363-daff5e1e6e80,neither PaLM or LLaMA are ﬁnetuned on mathe-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,99c7694f-93a9-415f-8424-a02153521115,matical data. The numbers for PaLM and Minerva
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,178f7e46-000f-42f8-af1a-b68863c0f1ee,"are taken from Lewkowycz et al. (2022), and we"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5c0bd5b-aa99-4139-8b83-5262d385ba2a,compare with and without maj1@k .maj1@k de-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,09c11cda-86e1-43d7-b339-9e9ca183cc26,notes evaluations where we generate ksamples for
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cd335bed-7dc7-4077-98ef-417f09159286,each problem and perform a majority voting (Wang
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ae905af-d459-41ee-afa8-c595450c741d,"et al., 2022). On GSM8k, we observe that LLaMA-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bef9d13d-2076-49ab-a892-d7af9553a37c,"65B outperforms Minerva-62B, although it has not"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aead4205-34ee-4797-a534-2caea4e3eab0,been ﬁne-tuned on mathematical data.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b7b6c12-0ca1-4099-8141-5b7ef47437a6,3.5 Code generation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e03379a-ca50-4223-9402-6e66bfae1efa,We evaluate the ability of our models to write
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bdc563e0-15c5-4a18-9ff5-de94f09ff139,code from a natural language description on two
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8ccb3065-afa8-48f7-9d3f-e6b24d0c554c,"benchmarks: HumanEval (Chen et al., 2021) and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5444cff9-3e05-439f-8bbb-84f0baba6e95,"MBPP (Austin et al., 2021). For both tasks, the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,46f5ff58-1707-41d3-803b-63b94f4c74d2,model receives a description of the program in a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c67d3707-7260-4b4e-8c77-5a0770535ebc,"few sentences, as well as a few input-output ex-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,31ff6d57-6ca6-4638-bca8-e5052a195de4,"amples. In HumanEval, it also receives a function"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,18e51fa6-0b22-4d3c-ad00-ead93fb9e7cf,"signature, and the prompt is formatted as natural"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8ada548-dc1a-40e6-a038-cf44101bc310,code with the textual description and tests in a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3e2df76f-9e84-4feb-a372-e5a90136d484,MATH +maj1@k GSM8k +maj1@k
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f4496f40-9260-4102-9bac-e984576c7ffc,PaLM8B 1.5 - 4.1 -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0dd0f051-5718-4e83-a7c7-0eadd69b23a6,62B 4.4 - 33.0 -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,90153a03-675a-4e98-b8e8-b93a1d60aef3,540B 8.8 - 56.5 -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d3bcc88-3211-47d7-a318-6176f3678c15,Minerva8B 14.1 25.4 16.2 28.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,663e9716-2603-4def-9655-8a763b6373ac,62B 27.6 43.4 52.4 68.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37f18bc6-d4a0-407e-aef7-bb9baa8fa860,540B 33.6 50.3 68.5 78.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e7ef8f5-a461-499d-82c4-aa99acac44c9,LLaMA7B 2.9 6.9 11.0 18.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e0870628-8d66-452f-90b8-b5cc57d499d9,13B 3.9 8.8 17.8 29.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6014e046-91d0-4e4b-bbbc-f10b79dac3db,33B 7.1 15.2 35.6 53.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0db60e52-c995-4011-8b93-fd2a5f0ca2da,65B 10.6 20.5 50.9 69.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,739a6638-e5b8-4ce3-b8b4-04a1cf90893e,Table 7: Model performance on quantitative reason-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bc82089a-cb7c-48ca-8f1e-33a25379ae80,"ing datasets. For majority voting, we use the same"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f6377234-f28b-45fe-bd16-0438c4c9a588,"setup as Minerva, with k= 256 samples for MATH"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6dbda5ce-b4a1-459d-9895-826ea51dd84e,andk= 100 for GSM8k (Minerva 540B uses k= 64
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0585465c-944d-4a33-9be4-b21066af9490,for MATH and and k= 40 for GSM8k). LLaMA-65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c3405f15-231c-423e-afa0-423b6f61345f,"outperforms Minerva 62B on GSM8k, although it has"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fb9762f8-d82c-4a09-a743-b76304a01bd5,not been ﬁne-tuned on mathematical data.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,27a0f0eb-1760-4f59-953b-8ec1e08f06e4,docstring. The model needs to generate a Python
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71fb187e-79ec-49cc-b195-ac6aa8aaca4d,program that ﬁts the description and satisﬁes the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c22228ae-37aa-46d0-8762-064324c0a3be,"test cases. In Table 8, we compare the pass@1"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b70d704b-1bb7-4853-87b8-27c9f3ab4d56,scores of our models with existing language mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,29abfeac-5d8b-438e-b0a3-7715f9536fd8,"els that have not been ﬁnetuned on code, namely"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3793aef8-420d-42ed-9bf0-8e467ba246cc,"PaLM and LaMDA (Thoppilan et al., 2022). PaLM"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e2541786-eb1e-42ad-b0a1-4a91b8713320,and LLaMA were trained on datasets that contain
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4322f44b-4eda-4733-a106-2fd2ed978886,a similar number of code tokens.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d985041b-8460-44ad-9793-4e3051c585ba,"As show in Table 8, for a similar number"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eba46cfa-3d67-4315-aba3-16f661419f7c,"of parameters, LLaMA outperforms other gen-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,237325b6-edb8-4fec-a072-33c1bfaa9c86,"eral models such as LaMDA and PaLM, which"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9eab1dfd-40a1-4fc8-961f-878a9d225c2f,are not trained or ﬁnetuned speciﬁcally for code.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,483bd401-e944-4313-af48-b0fe7e3819f4,LLaMA with 13B parameters and more outper-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f936ceb2-fec8-4f64-a970-d871ee86d33d,forms LaMDA 137B on both HumanEval and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ccaa83ce-8001-4f3d-afb9-d995617ba9b2,"MBPP. LLaMA 65B also outperforms PaLM 62B,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bf986c91-a524-4ede-8e67-36521a952e2f,even when it is trained longer. The pass@1 results
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c1463699-a91d-4eeb-99e2-ded661779895,reported in this table were obtained by sampling
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5e0a453c-83c4-4f98-bf68-2bcf3c94c216,with temperature 0.1. The pass@100 and pass@80
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b67d707-64b3-4a37-93f0-21fbfff99915,metrics were obtained with temperature 0.8. We
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,610cafb6-0594-46e8-97f3-68f2245528d8,use the same method as Chen et al. (2021) to obtain
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,265b2619-693b-4e3d-9672-f92744c6b249,unbiased estimates of the pass@k.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5e5c9e26-cf3c-4cdb-bdad-72ad01915d4f,It is possible to improve the performance on code
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f219224e-e494-4791-abcf-be922571c538,"by ﬁnetuning on code-speciﬁc tokens. For instance,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e2ebf7db-28f7-4612-9010-c01b4c71a7ce,"PaLM-Coder (Chowdhery et al., 2022) increases"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7b9d3c5-2785-4bd7-be51-d9b9b5f60ccf,the pass@1 score of PaLM on HumanEval from
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e4e7b63-c00a-49d5-b65d-ef7e93692359,26.2% for PaLM to 36%. Other models trained
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ffb766c7-1530-4635-ac9c-b5eae87376a1,speciﬁcally for code also perform better than gen-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9573b10-cfbb-463f-beab-4738e3efae86,"eral models on these tasks (Chen et al., 2021; Ni-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,55d844a3-33de-45ec-8d90-831f8f2b8cf7,"jkamp et al., 2022; Fried et al., 2022). Finetuning"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d2f07a73-07e3-4e38-be4a-c1d649e12ba9,on code tokens is beyond the scope of this paper.Params HumanEval MBPP
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f0642e97-7731-4beb-8f02-8686a2091f31,pass@ @1 @100 @1 @80
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,de81ac88-57b9-40d4-a90f-abf64f84a9c8,LaMDA 137B 14.0 47.3 14.8 62.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c76d1c02-aa07-4934-8b6d-aba742c7ef93,PaLM 8B 3.618.75.035.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8c312301-2c23-4242-a9b9-0b876d233183,PaLM 62B 15.9 46.321.4 63.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,70051233-7325-4091-ba38-76e5d6dc328e,PaLM-cont 62B 23.7 - 31.2 -
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b2fd77a8-fa95-4eac-b4cf-24bb8f0c8438,PaLM 540B 26.2 76.2 36.8 75.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a87c493-aed7-493c-901f-76731827f60a,LLaMA7B 10.5 36.5 17.7 56.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0ec5cc9e-316d-4a17-97be-25b048c55489,13B 15.8 52.5 22.0 64.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,537721d3-7902-4e61-adc1-04af3496268b,33B 21.7 70.7 30.2 73.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,04478cda-9ce5-47a7-ab4f-e08e832f3fb4,65B 23.7 79.3 37.7 76.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6f1a17d8-0622-4997-bcad-cb36ba9e9ab4,Table 8: Model performance for code generation.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5dc59e54-89ae-49b2-a47e-38e81d129031,We report the pass@ score on HumanEval and MBPP.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84e22289-c379-4a7c-a0f7-64c55bbe3319,HumanEval generations are done in zero-shot and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,08321636-7f44-4890-97cf-d1f887aaa257,MBBP with 3-shot prompts similar to Austin et al.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2fc6f935-0e43-4dd0-976e-b12b203967ad,(2021). The values marked withare read from ﬁgures
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1fa964a9-eb97-439b-9ee1-9833b304be04,in Chowdhery et al. (2022).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,39843f7b-9c02-41b2-81c3-62222219a8e7,3.6 Massive Multitask Language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b5334c26-f3ba-49be-a823-2f79c85af29f,Understanding
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3a1979e-106b-4c78-9bc3-55e4cfee110d,The massive multitask language understanding
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b04534ab-4bec-4919-8dbe-67f8f354d89b,"benchmark, or MMLU, introduced by Hendrycks"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d76577f2-c637-4e5d-bd03-8e4743060a4f,et al. (2020) consists of multiple choice questions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25a43441-470a-4fb6-81af-e4a5c2ae7d8c,"covering various domains of knowledge, includ-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce0ffb71-37d4-451d-9360-503033e0d47a,"ing humanities, STEM and social sciences. We"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,80f90bc7-383e-43a1-bef8-000901ec620a,"evaluate our models in the 5-shot setting, using the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0be4f3c8-e8e1-404c-a776-d97306e9e8d5,"examples provided by the benchmark, and report"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6cd579a8-7bc8-416e-a9c9-c81bc854b958,"results in Table 9. On this benchmark, we observe"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25d8920f-5487-49d7-a78f-3ca7be80a4f6,that the LLaMA-65B is behind both Chinchilla-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a6484ec-b08b-4ee6-a6dd-bb9fa2a74f07,"70B and PaLM-540B by a few percent in average,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6a10f9da-40ff-4705-ac30-502a6e6d5c2f,and across most domains. A potential explanation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cffec10d-efd9-4cf4-adf3-5c36ebb93b97,is that we have used a limited amount of books
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3b57bb3-140f-4a8d-962f-75a34846eb5e,"and academic papers in our pre-training data, i.e.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,68888a8a-b2c6-4486-a3fc-64854284e2a4,"ArXiv, Gutenberg and Books3, that sums up to only"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84e6d084-9f91-43cd-8832-886d7553c292,"177GB, while these models were trained on up to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e8f232d5-b448-4b42-8f2c-9d8e1d03e90c,2TB of books. This large quantity of books used
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bc6bc616-6fff-4ec6-96c9-98114761d691,"by Gopher, Chinchilla and PaLM may also explain"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e679a48a-39d2-48bb-9456-c5b59b8956d7,"why Gopher outperforms GPT-3 on this benchmark,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bc52bddf-2946-4250-88cf-d02a27adfdee,while it is comparable on other benchmarks.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d581dfb0-9f2f-4610-8a38-9ae2bdc42da9,3.7 Evolution of performance during training
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4f1cd2e-9e86-4b30-98d6-cf21008f170a,"During training, we tracked the performance of our"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,51cae019-a557-40d7-8040-ac894815385b,models on a few question answering and common
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5bb92f38-a902-4bfa-8f74-c9d88a78e304,"sense benchmarks, and report them in Figure 2."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b0624dea-39a6-49e5-8ab1-70c51057646c,"On most benchmarks, the performance improves"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f48c198b-c9a0-4775-800b-ec6145e3c962,"steadily, and correlates with the training perplexity"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c687d409-a833-4267-9291-f416a3b8082c,of the model (see Figure 1). The exceptions are
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78c742a0-b948-4af8-a952-33268fd739cc,"SIQA and WinoGrande. Most notably, on SIQA,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bcdf4822-085c-4cdc-b8a9-4196b65146d7,"we observe a lot of variance in performance,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4ada41b3-378d-488c-b451-54ad17a2a7c9,Humanities STEM Social Sciences Other Average
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a4423ac-3097-48b4-bab6-8d5892379e48,GPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2da0ad7b-6c7c-45c8-a8c8-05be3a5c4ccd,GPT-3 175B 40.8 36.7 50.4 48.8 43.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,039bf4a6-4aac-41b1-ab77-08eb6869554d,Gopher 280B 56.2 47.4 71.9 66.1 60.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6123aaf4-b968-4112-8d15-3dbc11889304,Chinchilla 70B 63.6 54.9 79.3 73.9 67.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78ac147a-2023-4b37-936c-07a80b99c279,PaLM8B 25.6 23.8 24.1 27.8 25.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bce073da-ba57-4f34-bde2-61ca2161dffd,62B 59.5 41.9 62.7 55.8 53.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b73cd0e-e67b-4eff-aa1f-cdc80169abd9,540B 77.0 55.6 81.0 69.6 69.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23c30c45-75ed-4903-b0ae-3658156be52c,LLaMA7B 34.0 30.5 38.3 38.1 35.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f77ec14-0564-4b05-aa04-4ee2c23a4d66,13B 45.0 35.8 53.8 53.3 46.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64d36059-1bd3-4bd6-a0ca-381249cfb3e7,33B 55.8 46.0 66.7 63.4 57.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4a26c71f-bc06-4f08-b420-40165068094b,65B 61.8 51.7 72.9 67.4 63.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8108f2d7-094d-4625-bb4a-86df7745c29a,Table 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61f06237-6e02-4930-9b0c-c60e76a7682c,that may indicate that this benchmark is not
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2b4ab529-a7ed-4e12-b42f-cb04f2d0f6fa,"reliable. On WinoGrande, the performance does"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9cac4504-4776-4341-b30c-9358ba706638,not correlate as well with training perplexity:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3eb345dc-4f0e-47ce-b9a8-5edcd58e801d,the LLaMA-33B and LLaMA-65B have similar
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1beac8b-ac41-48a3-9d02-cdeec33b4960,performance during the training.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a9a546a0-76df-4c48-ab5e-1bcb64f0ae95,4 Instruction Finetuning
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3584cc8a-55e2-4e72-b80c-0e6e667b09df,"In this section, we show that brieﬂy ﬁnetuning on"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3e5a771-61a0-401f-9f14-0f98b48d017c,instructions data rapidly leads to improvements
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4acfb95c-ea68-4669-835e-ce300f239049,on MMLU. Although the non-ﬁnetuned version
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c2ac938e-d48f-4c54-abcf-c7c6262fdf88,of LLaMA-65B is already able to follow basic in-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f669fa2b-1590-4f7b-9aaa-f66fa918e7d8,"structions, we observe that a very small amount of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2534dec5-ad2b-48a9-a5cb-81a636bd93bf,"ﬁnetuning improves the performance on MMLU,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ebce8e0f-3fa7-4b1f-8d14-698905c8f5bb,and further improves the ability of the model to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f88e8a4a-f752-47f4-9d04-a31c2539db88,follow instructions. Since this is not the focus of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1546257-fafd-492f-80b4-e0179366dadf,"this paper, we only conducted a single experiment"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb96aa45-924c-49bf-a06a-215bba44bc57,following the same protocol as Chung et al. (2022)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c0c8750d-0637-4e78-bb2f-eed2c651b746,"to train an instruct model, LLaMA-I."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d991d6e-e778-443b-86e2-8a3ebb059c0b,OPT 30B 26.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7e5cf0a-2722-47cc-a4de-68ec60259926,GLM 120B 44.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67960595-4219-40c4-8ebf-6ee7834ae5b0,PaLM 62B 55.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dbcf264c-8f91-4e57-bc71-86e8fd8711d1,PaLM-cont 62B 62.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9297003-7f23-44fd-9c62-bc06658206c4,Chinchilla 70B 67.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e47bac1e-0677-4f9b-be87-165d888ef100,LLaMA 65B 63.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6535cede-fa42-4e2b-99e1-2ea917b25afd,OPT-IML-Max 30B 43.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eb3b7932-f9d9-4fb5-b700-c430402ece82,Flan-T5-XXL 11B 55.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,02476ef4-0ab6-4a5d-95cd-701bf4557a0b,Flan-PaLM 62B 59.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,03528839-2a91-4d13-aca6-402f207a7dcd,Flan-PaLM-cont 62B 66.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9017da9a-9424-4fdc-8b2a-f83b194404b7,LLaMA-I 65B 68.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3b93597c-4523-43eb-8318-989ecca58ecc,Table 10: Instruction ﬁnetuning – MMLU (5-shot).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a400b92-1319-431f-9ee2-944bbb32034e,Comparison of models of moderate size with and with-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9607760d-5374-47f7-8940-350a29f5be07,"out instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,088fd672-ac9a-45a7-b9b9-a01e01557499,model LLaMA-I on MMLU and compare with ex-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0a6fe1cc-2150-42cb-b8a3-b9d53da6630e,isting instruction ﬁnetuned models of moderate
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f356d270-d49a-46a0-a3d8-3ff074215e57,"sizes, namely, OPT-IML (Iyer et al., 2022) and the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,53d341ae-2817-4389-9a39-c3d32540ad56,"Flan-PaLM series (Chung et al., 2022). All the re-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5553fe53-ac83-46f0-82ef-5ee4b325697b,ported numbers are from the corresponding papers.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,739a18f8-17d3-44eb-89f7-dd09c057ad09,Despite the simplicity of the instruction ﬁnetuning
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3461868e-f523-4dee-b784-d00c2b906021,"approach used here, we reach 68.9% on MMLU."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,264c8fce-3e59-4aaf-bab9-d8a5e5ee6d55,LLaMA-I (65B) outperforms on MMLU existing
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,966c0fc3-f8a6-423c-b782-357aea6b4224,"instruction ﬁnetuned models of moderate sizes, but"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4dd638e5-e211-48c7-a762-edb9596426ca,"are still far from the state-of-the-art, that is 77.4"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6348578b-ee66-4422-bd85-470b1d85064b,for GPT code-davinci-002 on MMLU (numbers
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,858e1373-b5bb-40d3-8670-e342b521ce85,taken from Iyer et al. (2022)). The details of the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,53bdb109-9242-4e40-9f73-25064ef7a71a,performance on MMLU on the 57 tasks can be
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14b70999-8e8f-4864-96f7-a25cc1776b15,found in Table 16 of the appendix.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a92a292-607e-4a2c-aa59-36df3b51a1bf,"5 Bias, Toxicity and Misinformation"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,98926a47-82db-4c6c-b166-d849b8d78283,Large language models have been showed to re-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e1168a46-c78f-4eed-bff6-5bfddb6ce99d,produce and amplify biases that are existing in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,09c7ee31-e4d7-405e-84bc-9e45edbcb7ec,"the training data (Sheng et al., 2019; Kurita et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8a74995b-d38b-44d2-ab8b-e08d412d741d,"2019), and to generate toxic or offensive con-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,694524be-abef-4003-ae12-44d4c77e0a3d,"tent (Gehman et al., 2020). As our training dataset"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c6e0f37-659b-4d54-a139-448a1b827bf9,"contains a large proportion of data from the Web,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8cf48175-95b7-4ba1-a9b6-ab1657e76047,we believe that it is crucial to determine the po-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dc2e1e50-7724-4b96-8d8a-5b08451358d8,tential for our models to generate such content.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,239f11c9-e269-444a-81f3-cf232f747945,"To understand the potential harm of LLaMA-65B,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71b814e8-3fa9-44bd-bb23-341ab6a21072,we evaluate on different benchmarks that measure
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9d54cb4b-dfb9-42da-94b8-802d2622965b,toxic content production and stereotypes detection.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4b43b9d4-d3b9-47f2-8f52-dc7a082185b2,While we have selected some of the standard bench-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ed00306e-9bdd-4626-8f5c-ce3a7512e87d,marks that are used by the language model com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,09c93949-7efd-4cd8-8b1d-30f825511334,munity to indicate some of the issues with these
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4eb1dc92-cf55-4284-90e7-0304f529fa9d,"models, these evaluations are not sufﬁcient to fully"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2470aaed-d2c7-4ef1-b8b5-3f1fc665815c,understand the risks associated with these models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ea3b579-865e-4d83-8950-7c0718c3a71b,0 250 500 750 1000 1250 1500203040506070AccuracyTriviaQA
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,05d370d3-677b-4d89-8483-4c7ea143bf1d,0 250 500 750 1000 1250 15005055606570758085HellaSwag
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eacfd7ae-8f44-4896-aaf3-44f282f3e12a,0 250 500 750 1000 1250 150005101520253035NaturalQuestions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c47c593d-beeb-425a-9184-2f118bfb0825,0 250 500 750 1000 1250 1500
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df90c0ca-9b1f-476e-bda0-904a7eb3e0b4,Billion of tokens40424446485052AccuracySIQA
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,324c4c75-f4c3-4821-9e3b-f128c55a70aa,0 250 500 750 1000 1250 1500
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aeb65b23-82d8-4810-a95f-675a977cb188,Billion of tokens50556065707580WinoGrande
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6ac5e67d-70c4-425d-a443-7d743a68dd45,0 250 500 750 1000 1250 1500
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6e3af32f-ce8b-46e9-9537-a2a090cf9c6f,Billion of tokens65.067.570.072.575.077.580.082.5PIQA
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6967bcfb-4c4e-4334-82d3-304dabd43aa8,LLaMA 7B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4eb9655-68ee-414c-9168-8dab6bb87bf6,LLaMA 13B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9340f7f-078d-4553-a2b3-ac2f356b42bf,LLaMA 33B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86a4c49e-4e39-4bed-a77a-e384ba70e299,LLaMA 65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,46e90600-b3c6-4724-9e49-b02432c24d17,ChinchillaFigure 2: Evolution of performance on question answering and common sense reasoning during training.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ca148766-a2ff-4e46-bde5-06a273792c74,5.1 RealToxicityPrompts
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d013c723-20ad-4c71-8fbc-3c413335a10d,"Language models can generate toxic language, e.g.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,58d41626-45d9-4739-bdde-04d9dad83abf,"insults, hate speech or threats. There is a very large"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45e33396-0f35-4c9a-a1df-1d68596452c9,"range of toxic content that a model can generate,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,828b624e-ebd1-4702-b237-b9f6f03a62d3,making a thorough evaluation challenging. Several
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0650dae0-6627-4b69-ab3e-ba9ae103c7f4,"recent work (Zhang et al., 2022; Hoffmann et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,441025a1-c7b5-429b-af37-29eb03aa49cd,2022) have considered the RealToxicityPrompts
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,131b849b-6655-4217-94e6-bbde3ac32ec1,"benchmark (Gehman et al., 2020) as an indicator"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,acb9b566-694d-4411-a0bf-8e81537ebf46,of how toxic is their model. RealToxicityPrompts
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,798039f0-63cd-4e35-9119-1a435e10b80d,consists of about 100k prompts that the model must
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3b9ac580-7d75-4008-9578-71b6d48699cc,complete; then a toxicity score is automatically
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72a09bc0-d5dc-4b07-add9-b9c9f514ddaa,evaluated by making a request to PerspectiveAPI3.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a99faed-edd2-4aa9-8353-618ee441af60,We do not have control over the pipeline used by
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3787baea-4318-439a-ab08-bad5b9d5b72e,"the third-party PerspectiveAPI, making comparison"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,38e3c837-9283-493d-bd3d-bb49e9b30cdd,with previous models difﬁcult.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e7f5df6b-944d-4b0d-a5af-cd709a80b628,"For each of the 100k prompts, we greedily gen-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,edcccb41-31ad-4861-a625-548e09f5e814,"erate with our models, and measure their toxic-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,17be1832-42c6-4165-a4c2-0d646e92df61,ity score. The score per prompt ranges from 0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c61eab12-f205-4a29-8b3e-b52417cc1e93,"(non-toxic) to 1 (toxic). In Table 11, we report our"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,981f2458-ea75-47f9-914f-2e19a02a2695,averaged score on basic and respectful prompt cat-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,55b0af7b-4eb8-49e3-a8ea-2c01570df1ef,egories of RealToxicityPrompts. These scores are
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,205c868a-1def-48bc-acf7-4e7efcc4603b,“comparable” with what we observe in the litera-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e38dfb3e-64a2-44bf-88a7-7c00e26d6cf9,"ture (e.g., 0.087 for Chinchilla) but the method-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,24a02663-ee77-4fbc-ad4c-d5700a2aae0f,ologies differ between these work and ours (in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f1b3af2-f14e-436e-af78-f0850fff386c,"terms of sampling strategy, number of prompts and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3af4a797-b4f7-4daf-97c5-02da584b2f79,time of API). We observe that toxicity increases
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,26abfd56-e933-4086-a740-0db1d040560a,3https://perspectiveapi.com/Basic Respectful
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f68a32fe-df82-4813-a4ca-4daa5dfecbf7,LLaMA7B 0.106 0.081
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ed164c4-24b2-4567-b553-88f8f5df4c02,13B 0.104 0.095
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,319ada10-22a1-4573-a2e0-ef03d175a687,33B 0.107 0.087
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,294f769f-a951-4655-938d-5007450c19ce,65B 0.128 0.141
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d4db6b1-7736-4f2f-a1ed-e6acc3471bec,Table 11: RealToxicityPrompts. We run a greedy de-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0e879a48-cec4-4058-a506-75cfe2ccd25e,coder on the 100k prompts from this benchmark. The
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ba50ffd4-4b19-420a-9eba-ea624097696f,“respectful” versions are prompts starting with “Com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,47e51578-b16e-493a-86c8-4d2c612e7bf5,"plete the following sentence in a polite, respectful, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2470dafe-6369-4ac8-99ce-9453ebeecb6d,"unbiased manner:”, and “Basic” is without it. Scores"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,85d89b1d-ef0d-4fe4-b262-98200baec333,"were obtained using the PerplexityAPI, with higher"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d54067b-ad28-4f23-9dfd-6e8505d5564f,score indicating more toxic generations.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1df19187-e2d0-4a77-bbb3-e01679aa94ea,"with the size of the model, especially for Respect-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8a3ab485-6233-4a5d-b69a-b25e8c149717,ful prompts. This was also observed in previous
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,317304ac-f8e9-4a90-9025-5f2432a0f07a,"work (Zhang et al., 2022), with the notable excep-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ff1232a-d50b-4034-a88e-8e1b5b51d525,tion of Hoffmann et al. (2022) where they do not
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bacfafab-8273-4b42-92ba-3320ab2c6ec7,"see a difference between Chinchilla and Gopher,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2806f9e1-9284-4c3e-a0f8-e9bb016899e4,despite different sizes. This could be explained by
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,08f34eff-5d65-4608-86dd-5c3df2f254d4,"the fact that the larger model, Gopher, has worse"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb8ffc74-717c-4b9f-83b7-4bbb321bf0ea,"performance than Chinchilla, suggesting that the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3da74ece-0362-4233-8e68-2b36472a53c3,relation between toxicity and model size may only
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,149bdeb0-4f28-44f5-9a51-36cee31db9e9,apply within a model family.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c0cc87ff-486d-4a1f-9d85-6f40b54fb50c,LLaMA GPT3 OPT
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9a407035-410e-4087-8099-27899f8b8a9c,Gender 70.6 62.6 65.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,121b9987-a993-4c8b-a360-88a965411e74,Religion 79.0 73.3 68.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4027cc86-7595-4129-9a8e-ea2e5eb38286,Race/Color 57.0 64.7 68.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5624c885-0f94-4d17-9491-8583837e6272,Sexual orientation 81.0 76.2 78.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ebcd4b6-7189-4fa9-95c9-feff1272b45a,Age 70.1 64.4 67.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bcc819fe-af75-4e1f-bcd1-f1c843567439,Nationality 64.2 61.6 62.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,343a734a-eb55-4dfb-8670-9d7d6178474d,Disability 66.7 76.7 76.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3042095a-dd10-4162-82d7-6fbd5d120469,Physical appearance 77.8 74.6 76.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9206778-fae7-4223-b2a8-1ef52343b768,Socioeconomic status 71.5 73.8 76.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3e4b3872-ea07-40bb-95ed-5dbf0a14004f,Average 66.6 67.2 69.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9ddae52-e1e6-4c08-8ed7-f49590e08f5b,Table 12: CrowS-Pairs. We compare the level of bi-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,57637187-e29e-47cd-a301-be66c8b1bb0c,ases contained in LLaMA-65B with OPT-175B and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,87ca504f-1c3d-46db-b7d9-60e3393c7e4e,GPT3-175B. Higher score indicates higher bias.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,04731842-94e7-40cf-a89a-494ede001a27,5.2 CrowS-Pairs
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,989b76dc-de23-44b4-98bf-0a45c7bc477b,We evaluate the biases in our model on the CrowS-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,04c820b6-217a-4996-bb6a-f89f82bf2b98,"Pairs (Nangia et al., 2020). This dataset allows to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,10893eb5-295a-4dc7-a3b9-fe5fc2d57320,"measure biases in 9 categories: gender, religion,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,76f30310-c47f-4b96-87e6-f1866ddf9561,"race/color, sexual orientation, age, nationality, dis-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2d2586c9-c287-41bb-aa41-81c52f6eac69,"ability, physical appearance and socioeconomic sta-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a27ca01e-d922-4358-a9f0-c5e8f729fb4c,tus. Each example is composed of a stereotype and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,04ce2d91-3307-4485-8941-d4c3bc2c8602,"an anti-stereotype, we measure the model prefer-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84a38ee4-9407-4679-8060-82f1247247ae,ence for the stereotypical sentence using the per-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,54e473d6-4d9d-4005-8cda-f98fa490fd64,plexity of both sentences in a zero-shot setting.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cfaa04c4-0497-4c43-b27d-2bacbf55ca74,Higher scores thus indicate higher bias. We com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c664c481-a021-4131-b861-373688fbb8ee,pare with GPT-3 and OPT-175B in Table 12.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,effdab3a-016e-4df9-8518-001effffe6ff,LLaMA compares slightly favorably to both
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4de80273-0849-493e-b241-3158613d0618,models on average. Our model is particularly bi-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,46272ecd-976c-4376-ae13-6f487e242bfd,ased in the religion category (+10% compared to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eaf53967-765a-42ee-a4c8-5959b2356b9b,"OPT-175B), followed by age and gender. We ex-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,958a9602-2c2b-4de9-9a73-b85de76398ed,pect these biases to come from CommonCrawl de-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,168d30c5-5fd1-49db-9502-ecc70f39ace2,spite multiple ﬁltering steps.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2416c832-b453-4806-8f06-6392ae20d8a6,5.3 WinoGender
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0e6c520e-dfd9-4f05-91f3-02176336d1be,To further investigate the biases of our model on
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b3e1fde0-e833-41a8-bcc8-a14bd99237eb,"the gender category, we look at the WinoGender"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,94655c55-4a36-4212-9a11-9fe7358002ac,"benchmark (Rudinger et al., 2018), a co-reference"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2331efd3-fecb-4e47-bdd0-d4f5c8d4c45d,resolution dataset. WinoGender is made of Wino-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2345cf65-1c1b-4372-8d63-3fdf54932386,"grad schema, and biases are evaluated by determin-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,11c88986-5878-4073-ad72-bb591292b847,ing if a model co-reference resolution performance
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f573bfb1-9fb0-4609-b43f-0d1dd0579460,is impacted by the gender of the pronoun.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d764032-3d02-475a-b26c-377da539f434,"More precisely, each sentence has three men-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f6d101d0-340a-4106-9df3-345736702cf6,"tions: an “occupation”, a “participant”, and a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78723c5f-e561-4cc9-bbc2-88309d140f78,“pronoun” where the pronoun is co-referencing
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,693bb58a-0566-43aa-a54c-1dae18f3b728,either the occupation or participant. We prompt
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e3f2a873-135b-404d-9d5e-5f678cc7cece,the model to determine the co-reference relation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a97e0409-7d59-41ef-a643-cdc0c32e9c5a,and measure if it does so correctly according tothe context of the sentence. The goal is to reveal
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4e21c963-681f-48f4-848c-88db7016bd6f,if societal biases associated with occupations
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eed57685-ca3f-422c-82e2-1fb5f32af6ea,"have been captured by the model. For example,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f8df2ee-22cc-4b64-bb5f-aa09d472a31a,a sentence in the WinoGender dataset is “The
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,49df1245-bc92-4ce5-a223-d0ef6a8ac66b,nurse notiﬁed the patient that his shift would be
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9e285d3-d9af-422d-8dc0-0e4f80b21cac,"ending in an hour.”, which is followed by ‘His’"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,26ee3439-a9d3-4e85-83b1-5efbd140fdb8,refers to . We then compare the perplexity of the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5b4d31f7-dc82-4351-8178-dde0e02a7e65,continuations the nurse andthe patient to per-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,865274b3-0aca-4a8d-ab36-10355c3f24fb,form co-reference resolution with the model. We
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71d641ce-fe72-4e89-be24-2b193ded2dc7,evaluate the performance when using 3 pronouns:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,038dc209-d980-4b62-b94e-788a63a26adc,"“her/her/she”, “his/him/he” and “their/them/some-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d00973b-1e02-4662-b5da-c9586e6646f2,one” (the different choices corresponding to the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f0c166db-6496-4a97-8007-d7625bc71a5d,grammatical function of the pronoun.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84c255aa-aa20-4e62-9369-16e2fe3db626,"In Table 13, we report the co-reference scores"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,99579a19-fd82-414e-bab2-a0a5e59664ad,for the three different pronouns contained in the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4c926f82-fa40-4706-94c9-d99661f2e1dc,dataset. We observe that our model is signiﬁcantly
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,38aa3288-8aa6-478e-ae71-e8a95bff5957,better at performing co-reference resolution for
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f085471-3181-43cc-a471-3e604f1ec0e6,the “their/them/someone” pronouns than for the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b84f3c98-1e96-4cc4-8dfa-2895f12b3659,“her/her/she” and “his/him/he” pronouns. A simi-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,273d7b5b-0c27-4bc9-9240-112c8888c9df,lar observation was made in previous work (Rae
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,95299f4d-415e-4094-a72b-95d7b54af91c,"et al., 2021; Hoffmann et al., 2022), and is likely"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f28694e8-fea7-4bce-8174-b019bb42469f,"indicative of gender bias. Indeed, in the case of the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a952be94-0d5b-4c07-b163-509664e1101d,"“her/her/she” and “his/him/he” pronouns, the model"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a9a0af9-a3da-41ec-b4b3-9a16b3a73507,is probably using the majority gender of the occu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e56abcb-5ecf-45f5-98ea-37f7e68e9ba3,"pation to perform co-reference resolution, instead"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e09bbbd8-40be-456b-a831-3e9d202bbd02,of using the evidence of the sentence.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e4ea8c9-0344-4826-9cf8-9e38d6186267,"To further investigate this hypothesis, we look"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8c685991-c2ea-4163-8b4d-50be8b681d73,at the set of “gotcha” cases for the “her/her/she”
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a855352-537e-46b8-9f8b-8e8515a3df55,and “his/him/he” pronouns in the WinoGender
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3081ad35-c199-4e21-9b89-0932b6c29e9c,dataset. Theses cases correspond to sentences in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a9def2d-f76a-4d4b-89ea-7002f1bf8068,which the pronoun does not match the majority
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,da449660-3513-441a-aba4-707c3da1bbc0,"gender of the occupation, and the occupation is"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df0610d9-3b1a-4d9a-8947-6348aa4c6864,"the correct answer. In Table 13, we observe that"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,070d1a96-706d-4190-a5d7-713edd74fa3e,"our model, LLaMA-65B, makes more errors on the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,83ce0360-24c9-4cad-8aa7-02c295a36fa1,"gotcha examples, clearly showing that it capture"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be83faec-0cef-4dae-8754-630f95ff1872,societal biases related to gender and occupation.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d40fd35a-f1e7-4826-9f3e-6ae12a10a413,The drop of performance exists for “her/her/she”
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d798b0eb-1cd1-481e-afdc-2c3ed85d9c85,"and “his/him/he” pronouns, which is indicative of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22be3d19-f938-41bc-af8d-b7d25241e0f3,biases regardless of gender.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9a1d29c9-9c11-4b4e-99b7-dce2c84e5f77,5.4 TruthfulQA
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d51f0268-8dad-4f22-a92c-f416ddaba566,"TruthfulQA (Lin et al., 2021) aims to measure the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bda9103b-996a-41bf-92ad-adb316343947,"truthfulness of a model, i.e., its ability to identify"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f346b47-61a7-44ca-a6ff-28075edd4772,when a claim is true. Lin et al. (2021) consider
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d2bfc5d-a8ca-4a8e-b937-c708d60bae6f,the deﬁnition of “true” in the sense of “literal truth
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eac6e4af-e8f0-4cc7-a0ef-ad40a3be97f2,"about the real world”, and not claims that are only"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b811b4e0-fbd0-400c-bfe1-229448ff5ece,true in the context of a belief system or tradition.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b2e8d2ff-5b8e-4901-966a-677a7b1f4d57,This benchmark can evaluate the risks of a model
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9754251-1198-4a0a-969c-df904dee9b33,to generate misinformation or false claims. The
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef310c8c-4436-45ec-839c-ca814c66dddf,"questions are written in diverse style, cover 38 cat-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a15b8da3-534f-4657-a9ab-ddd8b506dab8,egories and are designed to be adversarial.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ea1aa2c-99c2-48dc-a88d-3b8ab6124e6c,7B 13B 33B 65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3727cd6a-1beb-4a7e-8897-41a3e1499fdc,All 66.0 64.7 69.0 77.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d80b5d7f-45a1-4fec-b00d-587c1513ad91,her/her/she 65.0 66.7 66.7 78.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,87f60846-8d6a-4515-86c7-37d9f26e0cb3,his/him/he 60.8 62.5 62.1 72.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14c030c6-7d71-4cc2-864f-8fc4f31d0b71,their/them/someone 72.1 65.0 78.3 81.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,40f07bb3-bbef-46db-aea2-9909476331c0,her/her/she ( gotcha ) 64.2 65.8 61.7 75.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,730b7c1e-bfd5-4714-8027-706ba23a68a6,his/him/he ( gotcha ) 55.0 55.8 55.8 63.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3acbbe3c-9373-4bf3-bfda-3f71d70d8991,Table 13: WinoGender. Co-reference resolution ac-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b4227cc8-a178-4a26-81ce-318e2692cad5,"curacy for the LLaMA models, for different pronouns"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5f9fd2c5-feb6-4925-af15-027170cd03a0,(“her/her/she” and “his/him/he”). We observe that our
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2efcb181-4ddf-40f5-b956-087cbe32beff,models obtain better performance on “their/them/some-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3d77751-c746-45fb-babe-fec3d246cfc2,"one’ pronouns than on “her/her/she” and “his/him/he’,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,81b5708a-226b-408e-a800-6fc67a24cf29,which is likely indicative of biases.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6c66374f-edad-4e08-87c5-ad5731355914,Truthful Truthful*Inf
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fb4e3a71-5807-4954-a53a-29702c428a05,GPT-31.3B 0.31 0.19
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,288b9d22-6a09-4321-8cd3-2eb8f0d87ec2,6B 0.22 0.19
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cd172878-c689-4993-87ca-ef15439e504a,175B 0.28 0.25
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,644e90c9-a776-4bd6-947b-849b99d88597,LLaMA7B 0.33 0.29
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3586c6b8-b394-40a1-a7b1-cdbb751dbc10,13B 0.47 0.41
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2493b2df-dea5-45e4-b3ba-45a65b5438db,33B 0.52 0.48
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1664031a-5b70-4cb9-87bf-1460e18314f4,65B 0.57 0.53
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,86caaeab-0097-4818-be61-0d779786855a,Table 14: TruthfulQA. We report the fraction of truth-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,96aa08db-b5ee-40ab-a4f1-4735e0a1eb1d,"ful and truthful*informative answers, as scored by spe-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f61af06e-0789-4850-8a00-4d6f01966ed8,cially trained models via the OpenAI API. We follow
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,551ad115-f502-4e89-ba58-e3062c779b5f,"the QA prompt style used in Ouyang et al. (2022), and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6d8fe7de-5cde-44c8-a11c-3efcfb685454,report the performance of GPT-3 from the same paper.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7cf81e77-436f-41bc-837c-1a0ab111274a,"In Table 14, we report the performance of our"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,448c06f7-457b-4f80-8e46-90c947bad8d7,models on both questions to measure truthful mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cae64b42-3476-40a7-a8b2-8607c62ebac8,els and the intersection of truthful and informative.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe0587dd-087d-44e5-b661-989ab1059df7,"Compared to GPT-3, our model scores higher in"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,297f4a47-e751-4ff1-b9a6-0a9407746eb8,"both categories, but the rate of correct answers is"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6f679765-8bc6-4117-9cd3-121d91fd4d26,"still low, showing that our model is likely to hallu-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f6f7a27f-b6a0-4d56-83fa-7232b4b735a0,cinate incorrect answers.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c53654f8-8818-47bc-bcdf-f993c65db233,6 Carbon footprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ee31b8d-3105-4889-8450-ebca7fcc9653,The training of our models have consumed a mas-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37db93ca-e1ee-4a18-8e3e-6e4c74a6ad59,"sive quantity of energy, responsible for the emis-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,17b4c0aa-cf6e-445c-8057-23d47eb83b0b,sion of carbon dioxide. We follow the recent liter-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d8d2642-ec54-4ac8-83e9-c8f02b0ef65c,ature on the subject and breakdown both the total
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,74152c25-6ad0-4c76-9f3d-bd873eebb4e5,energy consumption and the resulting carbon foot-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ef87884-da98-4da0-858d-c9aedaa88d20,print in Table 15. We follow a formula for Wu et al.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fc821f61-9cbc-47e1-9c01-69982f358a77,"(2022) to estimate the Watt-hour, Wh, needed to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3e1eb9b0-1cee-4049-9aaf-3cbd4b3922ba,"train a model, as well as the tons of carbon emis-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,15f718c5-5aeb-4719-ae15-e0914370042f,"sions, tCO 2eq. For the Wh, we use the formula:"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dd77be36-a021-43b8-b6af-8eb12d4ceb61,Wh =GPU-h(GPU power consumption )PUE;where we set the Power Usage Effectiveness (PUE)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c58713d-abed-40db-a6ba-08e08b95759c,at1:1. The resulting carbon emission depends on
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9856de1a-a8dd-4f95-89bb-c29ca4727a08,the location of the data center used to train the net-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a41ab99f-4cc5-457e-b606-00f2e28d8263,"work. For instance, BLOOM uses a grid that emits"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,73749447-88b2-4024-b56b-02df94771ab0,0.057 kg CO 2eq/KWh leading to 27 tCO 2eq and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,607e8dea-23b5-4244-8256-00f8a2da69c9,"OPT a grid that emits 0.231 kg CO 2eq/KWh, lead-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,75b04f12-0762-44d2-a033-2d9f41a4c29b,"ing to 82 tCO 2eq. In this study, we are interested in"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71194f94-2a5f-47b0-a67b-c29a8f7ae90e,comparing the cost in carbon emission of training
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ece5a88-047b-4cb5-952d-f24c16d9bbc7,of these models if they were trained in the same
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78be2636-b71e-4b77-86d4-eeb0f66493c5,"data center. Hence, we do not take the location"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bc22c773-d78b-4ced-b2bd-5b2b1c860655,"of data center in consideration, and use, instead,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe5f3f33-62ff-4ec1-a8db-ba07804892f8,the US national average carbon intensity factor of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,96b0dffb-7b26-4250-ba1e-0cf01d854701,0.385 kg CO 2eq/KWh. This leads to the following
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c543f3d3-94d7-46a1-8079-6041f9f44094,formula for the tons of carbon emissions:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37614b0a-308b-4cd6-9fa1-8fce820661ae,tCO2eq=MWh0:385:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d2b8781c-32dc-410d-8b0f-6d7a61d683ea,We apply the same formula to OPT and BLOOM
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23b2a116-7ec1-43a0-9872-aaa0f124e636,"for fair comparison. For OPT, we assume training"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3961f1a-c7c1-4d5e-99f2-8ab994b4210a,required 34 days on 992 A100-80B (see their logs4).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5938dec-ce63-4283-9d92-585504d14ce5,"Finally, we estimate that we used 2048 A100-80GB"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,590ccea2-ed26-4ef1-bab7-c0a5b32b2fa6,for a period of approximately 5 months to develop
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,17715f30-d709-4562-8cf3-927c24fb092d,our models. This means that developing these mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3049ba17-469e-4e4d-ad1f-6e1d98ee6e55,"els would have cost around 2,638 MWh under our"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,173df185-a936-4f1d-a985-82639de054a7,"assumptions, and a total emission of 1,015 tCO 2eq."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,512092c0-0d5e-4af9-8173-21754d42d723,We hope that releasing these models will help to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ca4ad525-bb91-4eab-949e-c92f372c9acd,reduce future carbon emission since the training is
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3957d116-4a5c-4fd1-bb27-4d6a972256b8,"already done, and some of the models are relatively"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e632df13-1c31-4c26-8941-827cd1cc4332,small and can be run on a single GPU.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,605a10c6-f798-4a16-b541-7766d4a1738d,7 Related work
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,07454cb2-fca6-4b6e-b726-aaf42e18bbb8,Language models are probability distributions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9a79db7a-d6e7-48e1-ae2e-8d6cfe75b88d,"over sequences of words, tokens or charac-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b54c9802-8f36-449f-85dc-266727c5aa4d,"ters (Shannon, 1948, 1951). This task, often framed"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4a21b0cf-84ac-4d42-9eed-7acc86e6d889,"as next token prediction, has long been considered a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dad85c1a-73d4-41af-bffd-b9ee73441030,core problem in natural language processing (Bahl
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,090ccd74-7db8-4d32-9adf-de1fd6fb552a,"et al., 1983; Brown et al., 1990). Because Turing"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,96221d49-8a66-4ab8-b2a1-98e596c92648,(1950) proposed to measure machine intelligence
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,95129248-788e-4e60-8c82-5d3006f7a1fd,"by using language through the “imitation game”,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5c938244-ddfc-4a8b-993a-4d0039a5c7fe,language modeling has been proposed as a bench-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f367ba9a-4bf3-4092-b86f-a72838ab8e74,mark to measure progress toward artiﬁcial intelli-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f28bda6-550b-4525-a58c-e861085cd3b0,"gence (Mahoney, 1999)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,92a7a6ba-b3b8-4fb6-8759-90a2737fb179,"Architecture. Traditionally, language models"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bf0e4424-d4df-47f4-8e45-3158e342dbad,were based on n-gram count statistics (Bahl
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,884de823-3820-4d68-b141-609f03af858d,"et al., 1983), and various smoothing techniques"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fad17072-6684-4577-856b-a823f4bf6375,were proposed to improve the estimation of rare
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ba3ea4c-48c4-40d4-a930-9f1f0c86ccba,"events (Katz, 1987; Kneser and Ney, 1995). In the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bf46c31b-5d59-457e-980c-2a20b5166964,"past two decades, neural networks have been suc-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,508ef90e-06bb-404e-8009-3cd9a72ab3d8,"cessfully applied to the language modelling task,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,18f50c5b-24dc-4e4a-858e-6ed25784a65e,4https://github.com/facebookresearch/metaseq/
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d7beee0-c6b1-45d9-bd6b-44e99673016c,tree/main/projects/OPT/chronicles
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,446fb31c-d91e-4d34-886c-874e80fd368e,GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ea9ca2f4-926c-4d45-be51-9798ddc16684,consumption consumption (tCO 2eq)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f27c624b-8755-46ef-91e2-4ee1a8507cf2,"OPT-175B A100-80GB 400W 809,472 356 MWh 137"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1e417bd-341d-4b71-90ff-f6c7dfd551aa,"BLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bbf314bd-8822-4127-ba8e-1c03a833ad94,"LLaMA-7B A100-80GB 400W 82,432 36 MWh 14"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a5487b4b-dd35-447f-8406-6ed8aeb3d395,"LLaMA-13B A100-80GB 400W 135,168 59 MWh 23"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d53a2290-79d0-4bc7-90b5-023177266afa,"LLaMA-33B A100-80GB 400W 530,432 233 MWh 90"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84dc63b7-7561-453a-ad70-b5785d9140f5,"LLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e908c986-d1de-4dd5-860c-0c6099113583,Table 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9431b52-ab02-41d8-b2df-129307372d53,"to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,289fd5ee-819b-4ba2-ad12-9dc576e039d4,"consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,afe690ec-5331-4959-b1f5-ce6f856bbd26,PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e0dcaa64-3b4b-4029-a47b-168e06f20f3e,"starting from feed forward models (Bengio et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e91ff2b4-e64e-4754-b145-2dd7d8b87733,"2000), recurrent neural networks (Elman, 1990;"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8152227a-17bf-41c2-b9be-792a565346de,"Mikolov et al., 2010) and LSTMs (Hochreiter and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7599298c-ee12-49d8-a486-bc12d9cf5935,"Schmidhuber, 1997; Graves, 2013). More recently,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22a95be2-d54e-41b0-8595-f76c7dc313f6,"transformer networks, based on self-attention, have"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,991a3b1c-8cb3-4d92-9bea-bb0d3c8caf40,"led to important improvements, especially for cap-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e9ce31a-9186-477a-9abf-c0733d286aad,"turing long range dependencies (Vaswani et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1a4cc310-de3d-4c64-b59d-2f751fbeadad,"2017; Radford et al., 2018; Dai et al., 2019)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a72c02bb-2f07-4e56-9245-46fd5426a007,Scaling. There is a long history of scaling for
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d25e506b-a5b8-4843-a567-20486fac6883,"language models, for both the model and dataset"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f222c52f-d858-4d02-87d2-cc2f6d9408eb,sizes. Brants et al. (2007) showed the beneﬁts of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ba8da23-846c-412e-9f47-2ff5d312ca3a,"using language models trained on 2 trillion tokens,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2daf5797-c26b-4be9-aebe-e0780cf39c80,"resulting in 300 billion n-grams, on the quality of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1c2cda59-64e9-41c1-8b35-754adcc885c2,machine translation. While this work relied on a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0bd24657-850a-403d-8c6d-b4f5fa104584,"simple smoothing technique, called Stupid Backoff ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a9a69d82-7069-47be-977e-000d2e54311d,Heaﬁeld et al. (2013) later showed how to scale
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ebeec4e5-658d-4914-8221-3987f815056d,Kneser-Ney smoothing to Web-scale data. This
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,868a0961-0d0c-4a68-bcb7-81c63642fcf6,allowed to train a 5-gram model on 975 billions to-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c50c23e-069c-4c21-bb1f-d90295dc5872,"kens from CommonCrawl, resulting in a model"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4aa42660-196f-4ee8-bc48-3277a052fa73,"with 500 billions n-grams (Buck et al., 2014)."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,512b9f2a-9608-4960-904f-fb7afc243547,Chelba et al. (2013) introduced the One Billion
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c7691afb-cbc1-4488-b8c0-8d7f89bbaa18,"Word benchmark, a large scale training dataset to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ae9b0b84-6a5f-4c3e-9ca1-3af1caef1a15,measure the progress of language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f35205da-838f-458e-ba53-3716635f9f7b,"In the context of neural language models, Joze-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6cf06a7b-7136-4986-841e-54b429651bc8,fowicz et al. (2016) obtained state-of-the-art re-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78724745-75cd-4c48-8eab-90280e414aa4,sults on the Billion Word benchmark by scaling
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,18efbd24-fec6-4910-b945-71ca69d8d01b,"LSTMs to 1 billion parameters. Later, scaling"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ebb8a394-7105-492d-92e5-51e9c7dcd609,transformers lead to improvement on many NLP
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b8350b99-3056-41ba-99a7-68f83a00418b,"tasks. Notable models include BERT (Devlin et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,41a38cde-69f0-4268-a826-dac66c88b46a,"2018), GPT-2 (Radford et al., 2019), Megatron-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1769ae70-50f4-4910-9845-7d90d75cff59,"LM (Shoeybi et al., 2019), and T5 (Raffel et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e264b552-8a28-4f35-a54e-e1fc97629c1a,2020). A signiﬁcant breakthrough was obtained
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c547ebb-0559-4309-aaca-fbcf8ca51f00,"with GPT-3 (Brown et al., 2020), a model with"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7416508c-616e-4bd9-9c42-58291edffa09,175 billion parameters. This lead to a series of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,06069c88-d81d-465a-af83-1382fa159d3e,"Large Language Models , such as Jurassic-1 (Lieber"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df4ae38e-cf96-41f4-a409-b9003557d42a,"et al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dedf71b3-b869-43b9-88de-73e9179a7f32,"mann et al., 2022), PaLM (Chowdhery et al., 2022),"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,edf57508-733d-4bc0-923b-e23d8ba97f1d,"OPT (Zhang et al., 2022), and GLM (Zeng et al.,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0d573525-fed5-428e-a526-59601dc4f417,2022). Hestness et al. (2017) and Rosenfeld et al.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c0fdc54d-de5b-437c-bcdd-2564ebc5cf86,(2019) studied the impact of scaling on the perfor-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,732fc072-cbc0-49c6-ac62-3ef4e0e3e51b,"mance of deep learning models, showing the exis-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2164b383-52e6-4503-b460-6cfe18ee8b6e,tence of power laws between the model and dataset
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,83f00583-68f7-4dfc-a821-fd1038b3edaa,sizes and the performance of the system. Kaplan
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fcb2c189-9c08-49df-8635-bef038ef32af,et al. (2020) derived power laws speciﬁcally for
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25b767c3-bf50-4c92-aaef-aa3481202fc2,"transformer based language models, which were"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f9734d6-1a94-4709-8563-ec9b8be94c88,"later reﬁned by Hoffmann et al. (2022), by adapting"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bd2ed817-85c6-465c-9460-c58966ae8122,the learning rate schedule when scaling datasets.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,68029e25-da20-4dcd-97d9-2b0253c54000,"Finally, Wei et al. (2022) studied the effect of scal-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,155d23e6-6684-46a4-9cad-5ce972a57fc1,ing on the abilities of large language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef6f2a3e-98b1-4d7a-acf9-ed148253d2a3,8 Conclusion
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2e919e2c-c920-4b63-ab2f-9498d69ed43c,"In this paper, we presented a series of language"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,766e3ee0-0174-4084-97d5-0b847c573b71,"models that are released openly, and competitive"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e1f42da5-11f9-49c6-b705-77594a3c81a2,with state-of-the-art foundation models. Most
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6846cd12-3d62-491c-9e91-4c35f435b79d,"notably, LLaMA-13B outperforms GPT-3 while"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,96064b14-d252-45af-87a2-a27950cae85b,"being more than 10 smaller, and LLaMA-65B is"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e8c7f841-4c33-4c0a-be49-1d6261708af9,competitive with Chinchilla-70B and PaLM-540B.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,94c46d52-fdd0-4e39-965c-9f061a9b5a53,"Unlike previous studies, we show that it is possible"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef24329e-3054-4697-9462-0fff341701c9,to achieve state-of-the-art performance by training
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e5587e90-3c80-4456-8726-aea7eca240ae,"exclusively on publicly available data, without"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b89dfd00-652a-4363-ad7f-eae1f30cbad1,resorting to proprietary datasets. We hope that
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a9c9f55c-2130-484e-9086-fcd6eb8267f0,releasing these models to the research community
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c6ef1774-3f72-4dfd-833d-0cc0a7623175,will accelerate the development of large language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7053a5d4-de06-4e1a-93f6-bb86926abbb0,"models, and help efforts to improve their robust-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a8c0312-1d57-432e-8051-ba5dd88a6ea0,ness and mitigate known issues such as toxicity and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5557d361-5317-48fb-8d5b-d95dc54b0eda,"bias. Additionally, we observed like Chung et al."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b68e8e44-7f27-4897-a647-5f445edb098f,(2022) that ﬁnetuning these models on instructions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2e00513d-14e7-463f-b33d-fadbc5496030,"lead to promising results, and we plan to further"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5cb2d835-d638-4bf1-8e79-7cc391162499,"investigate this in future work. Finally, we plan to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad8d6e70-1dcb-476a-9154-4d8b6efc4b00,release larger models trained on larger pretraining
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e82cdb1f-de0e-40ed-8fa2-48cce8020383,"corpora in the future, since we have seen a constant"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,21eae5e6-3fa4-45ad-80a4-d39427510b46,improvement in performance as we were scaling.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e0c414ee-ecac-416c-881b-3196aaffd400,Acknowledgements
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5c22bfac-8547-451e-8246-2719059c46ec,"We thank Daniel Haziza, Francisco Massa, Jeremy"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,06781cc0-e228-418f-aab1-2d8a456bb22b,"Reizenstein, Artem Korenev, and Patrick Labatut"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b5035990-43c5-4c8c-bbf7-0b837350554f,from the xformers team. We thank Susan Zhang
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fbdd2c87-aa5f-4163-a958-b5830573b411,and Stephen Roller for their support on data
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f46100ba-8385-4e2c-8e6b-6f0de45f6b0c,"deduplication. We thank Luca Wehrstedt, Vegard"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c51d050b-56a9-460e-81a6-7867086a3c95,"Mella, and Pierre-Emmanuel Mazaré for their"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fd29284e-8695-4009-9527-8b26ad90422a,support on training stability. We thank Shubho
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d108ec51-c509-4cf6-9b24-5e8c8710ea02,"Sengupta, Kalyan Saladi, and all the AI infra team"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0248f454-7e71-4b58-9b92-74fdc964e228,for their support. We thank Jane Yu for her input
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,93a9c33f-9daf-4007-889e-a5785e00f799,on evaluation. We thank Yongyi Hu for his help
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,99f85d05-d946-43d2-a01c-8ee945c89f4c,on data collection.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cca166b6-8825-47bd-b43e-20e972610cac,References
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,760d761e-e877-40e5-901d-da3bd5e4e58c,"Jacob Austin, Augustus Odena, Maxwell Nye, Maarten"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e1e9cb1-d434-48eb-8243-077ef8351938,"Bosma, Henryk Michalewski, David Dohan, Ellen"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3119dbf3-78fe-409c-9594-e0dec4a6900c,"Jiang, Carrie Cai, Michael Terry, Quoc Le, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2e903765-b3f5-4828-adde-3308a78eb816,Charles Sutton. 2021. Program synthesis with large
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56252efa-206a-4499-85eb-e7e792a44399,language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ecd2a31c-2560-4713-86f0-038cdc96973f,"Lalit R Bahl, Frederick Jelinek, and Robert L Mercer."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,191ae199-2757-4ec1-aa88-70fe0fcf0e47,1983. A maximum likelihood approach to continu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,59e71e96-4da4-47ac-a163-e352e6ebcc0c,ous speech recognition. IEEE transactions on pat-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2cc40c13-f3d2-4a36-bea2-3afb052323fd,"tern analysis and machine intelligence , pages 179–"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7a3d5a5-d99c-49d8-b666-0112753befb0,190.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,32c60104-a16c-4896-a11c-c94ea15fb7fb,"Yoshua Bengio, Réjean Ducharme, and Pascal Vincent."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,43fa1417-0503-4aef-9707-0e48a92ec0ca,2000. A neural probabilistic language model. Ad-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b17bcb56-f0d0-4f5d-995f-efe370727667,"vances in neural information processing systems , 13."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b1f86c3f-91c4-436a-8e1a-68010388c7bf,"Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,015b2036-ba51-48b8-b285-d76c157ed9a9,"Choi, et al. 2020. Piqa: Reasoning about physi-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56692194-abe5-4808-b7fc-237f31aed9e2,cal commonsense in natural language. In Proceed-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4d1bdeb-ef16-4a83-8646-9ec06c5441fb,"ings of the AAAI conference on artiﬁcial intelligence ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ce8bb73-e5ae-4966-841e-ac5fc08e4225,pages 7432–7439.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dfb15723-5499-4c96-9f86-8ea9881916fa,"Sid Black, Stella Biderman, Eric Hallahan, Quentin An-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,019608f3-0c6e-4ab1-8dac-c298c3301679,"thony, Leo Gao, Laurence Golding, Horace He, Con-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cfa8330b-5070-47eb-94e7-1d786a74e950,"nor Leahy, Kyle McDonell, Jason Phang, et al. 2022."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c6e02e2-6301-45d8-bb55-4011438aeb16,Gpt-neox-20b: An open-source autoregressive lan-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0e6b4b78-d323-4b4b-aae2-46828d9f17a2,guage model. arXiv preprint arXiv:2204.06745 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3aa0591-51e8-4e2e-b0eb-2149405a50ae,"Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71e9388d-f130-4a93-b33d-5165b190be73,"Och, and Jeffrey Dean. 2007. Large language mod-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a21abb0e-53b1-47ed-aafe-99fbca557cfe,els in machine translation. In Proceedings of the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,964bca3c-4ab2-49a9-a665-ec72e283c68b,2007 Joint Conference on Empirical Methods in Nat-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,88b7d497-e5b6-496d-a073-761ebea1a3f9,ural Language Processing and Computational Nat-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9933cbe3-20a0-4cf9-a1a0-db7880dcc029,"ural Language Learning (EMNLP-CoNLL) , pages"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6d15a7b6-234a-459b-9a1c-f46f91c38254,"858–867, Prague, Czech Republic. Association for"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b61915ff-b4de-42dd-9b16-8483919102e7,Computational Linguistics.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3be23738-78ad-49d8-afcd-2e79309a6f67,"Peter F Brown, John Cocke, Stephen A Della Pietra,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,57f37ab9-73cd-46df-9e17-c17b5efb8783,"Vincent J Della Pietra, Frederick Jelinek, John Laf-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ba4da913-886c-453a-91e3-0a04283b086d,"ferty, Robert L Mercer, and Paul S Roossin. 1990. A"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,30874beb-8d88-4ea2-a75f-36e9be49dcc8,statistical approach to machine translation. Compu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,913acff3-c010-460b-a65d-4ee70e04a441,"tational linguistics , 16(2):79–85."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6cfed37a-a6be-4667-8816-b8ff84377a59,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1db48a6e-37e7-40df-8515-36beca7bc28a,"Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e4bc4b9-b75e-4c39-8956-dc68bcc4732f,"Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b7108844-99d6-4a13-87c3-8e946e6ceccc,"Gretchen Krueger, Tom Henighan, Rewon Child,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8e229aa6-b67b-4d6f-90bd-80cdf4d2c089,"Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f2cecb1-a095-4766-8d28-da34fe50010e,"Clemens Winter, Christopher Hesse, Mark Chen,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cf0820d5-9c64-4eb5-b5e9-749649485ca3,"Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,80f04810-cb16-4a03-9103-8e21266a9b80,"Chess, Jack Clark, Christopher Berner, Sam Mc-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3473b86-1afa-49ba-8a18-648481c11e53,"Candlish, Alec Radford, Ilya Sutskever, and Dario"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f0bc6b6-e1c4-4e5e-a854-ff988aee76c2,Amodei. 2020. Language models are few-shot learn-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f80920e-0a48-4a72-aa20-e9fbfd321eeb,ers.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6e4a6175-9c07-4ae7-a4cb-34005215b7f5,"Christian Buck, Kenneth Heaﬁeld, and Bas Van Ooyen."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e002479e-ed28-4c2d-84b4-bd80fe471fdd,2014. N-gram counts and language models from the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,623fba1a-5b24-4a22-bb6b-45eda0346de6,"common crawl. In LREC , volume 2, page 4."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,347b9596-44b3-4229-841e-fe2d9de79312,"Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14fef219-b723-4c16-91dd-8fe6c4d1fc0c,"Thorsten Brants, Phillipp Koehn, and Tony Robin-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,da81cfd9-0d69-4e3d-b538-64d4f5f52bbc,son. 2013. One billion word benchmark for measur-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,91566f60-0ad6-4efe-862a-01d8e5169336,ing progress in statistical language modeling. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61862e32-00a4-45cf-9c51-8451d7f32526,preprint arXiv:1312.3005 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71c32bb6-c126-4630-ac3e-23af0322f646,"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,478b6e49-385b-4950-968a-ba571d3e842d,"Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e523bfd9-561d-44e6-b402-7bb96b98d583,"plan, Harri Edwards, Yuri Burda, Nicholas Joseph,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,030afc73-b044-46f3-b425-effcf9805f1f,"Greg Brockman, Alex Ray, Raul Puri, Gretchen"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ab93596-ead3-44e8-b1b2-090b549b1d7f,"Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1cdb0697-38ee-4e2f-a9df-db19311027f4,"try, Pamela Mishkin, Brooke Chan, Scott Gray,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1844e350-10de-4f15-b7ec-3d8796089a9a,"Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5e37d6d-3c46-479f-981d-2538275d843b,"Kaiser, Mohammad Bavarian, Clemens Winter,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9acdeb97-4b10-4328-adca-9b3d437a01d3,"Philippe Tillet, Felipe Petroski Such, Dave Cum-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a1a5b1c0-dad1-488a-bb7e-e1adb0d5a516,"mings, Matthias Plappert, Fotios Chantzis, Eliza-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,27847ca2-d431-4966-866a-87156a6df4c7,"beth Barnes, Ariel Herbert-V oss, William Hebgen"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e4d55054-c7aa-437c-baa9-a6dff8dc27cb,"Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1cd8ae17-b3d0-491a-a861-39242614ed55,"Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9da19f62-0da3-4e1a-a333-552e0080372e,"William Saunders, Christopher Hesse, Andrew N."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ab2a5907-5f69-456f-a832-f94d306366d4,"Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7dbfe105-d5c5-4e38-9114-4fc533cb7164,"Morikawa, Alec Radford, Matthew Knight, Miles"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,28542b68-a013-4dc0-b0a1-0bf0b7493369,"Brundage, Mira Murati, Katie Mayer, Peter Welin-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8bfc0785-29e9-4022-8bb3-60b2b30feb3d,"der, Bob McGrew, Dario Amodei, Sam McCandlish,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,baf7f13d-86e1-4674-8fe2-bebbc9d9a6a6,"Ilya Sutskever, and Wojciech Zaremba. 2021. Eval-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,65e70f96-d0da-43f5-a5f3-96aa1a4c7c09,uating large language models trained on code.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c493b082-d63a-4950-8b3e-68025ef74ddc,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,49988063-45f6-46ee-bd71-f1ae471c0755,"Maarten Bosma, Gaurav Mishra, Adam Roberts,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9147682a-1091-46ef-a080-aabf1e1dfc9b,"Paul Barham, Hyung Won Chung, Charles Sutton,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1e5be4ef-73c5-4ec8-b64b-2e827da385a8,"Sebastian Gehrmann, Parker Schuh, Kensen Shi,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ace27bbe-f2a4-4872-98ad-e6e66793edcf,"Sasha Tsvyashchenko, Joshua Maynez, Abhishek"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e6f8211e-a212-4d87-8930-234392540987,"Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e13efef5-e890-42b6-9f17-853a4483193b,"odkumar Prabhakaran, Emily Reif, Nan Du, Ben"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dd929ee6-0b5f-44e0-9320-d1b274161d52,"Hutchinson, Reiner Pope, James Bradbury, Jacob"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be8b56e5-ae94-4167-a98c-6a514c54bc73,"Austin, Michael Isard, Guy Gur-Ari, Pengcheng"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,574b5f93-9f5e-4ddd-9d03-e3e271ac6273,"Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a0f2c544-f62d-4a96-ac8e-cc077161a142,"mawat, Sunipa Dev, Henryk Michalewski, Xavier"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b8b039f0-da04-4898-8e24-40548d9e9df8,"Garcia, Vedant Misra, Kevin Robinson, Liam Fe-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,313a62d9-b841-4db9-bb15-e4176e21e297,"dus, Denny Zhou, Daphne Ippolito, David Luan,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8cc2921d-dca1-407c-b799-2f1dc3e3a80a,"Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ba8572a5-5290-4e7f-895a-ee7f71bc966e,"Ryan Sepassi, David Dohan, Shivani Agrawal, Mark"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b46641be-c404-40e1-94bf-1bc3f18656ab,"Omernick, Andrew M. Dai, Thanumalayan Sankara-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f42b37bc-6eef-4848-b7d5-d91b89566655,"narayana Pillai, Marie Pellat, Aitor Lewkowycz,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,de44e47f-dbee-4931-8644-577831603659,"Erica Moreira, Rewon Child, Oleksandr Polozov,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e445e2a7-70d4-4eb4-883c-3666add50d24,"Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67149b6e-ed36-4b62-ba84-ca29d36b6031,"nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,15f62e7a-9547-463a-89ab-33ef20e239e1,"Jason Wei, Kathy Meier-Hellstern, Douglas Eck,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b1434b7-56c8-4a94-9e0d-fc7bc8d9a94b,"Jeff Dean, Slav Petrov, and Noah Fiedel. 2022."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,afeceae9-2aac-405a-af8a-03c5698b9867,Palm: Scaling language modeling with pathways.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3df07fe-d2bd-4a36-bac1-fd6b8d89f9b6,"Hyung Won Chung, Le Hou, S. Longpre, Barret"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a01c5b5-a712-4eac-9c26-92b89a8bc9b1,"Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2c29787-5903-4269-8971-1ace9c8da75c,"Wang, Mostafa Dehghani, Siddhartha Brahma, Al-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b6402a08-9ea9-4bf7-889a-6fa192ff4be4,"bert Webson, Shixiang Shane Gu, Zhuyun Dai,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,49cba282-45f1-4b06-a093-d8dc32c97d83,"Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ae075daf-c280-48ad-a909-ec6a1b5ab050,"ery, Dasha Valter, Sharan Narang, Gaurav Mishra,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7988792b-0946-484f-a8da-ed6943cd60d8,"Adams Wei Yu, Vincent Zhao, Yanping Huang, An-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d9dbb3cf-5681-4bb4-aba3-49411b4c4787,"drew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4ed6ec51-2a68-4f3d-b1e7-08ae2c369653,"hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,32fff8a5-3c38-46f0-805e-e7db3b80e9a9,"Denny Zhou, Quoc Le, and Jason Wei. 2022. Scal-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,efcf595a-fbfe-4384-a395-c2d4532e67d1,ing instruction-ﬁnetuned language models. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,49963a55-0504-4f91-a433-cf58216385c9,preprint arXiv:2210.11416 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,054072e3-fe84-4c8e-9dc1-de50075a2940,"Christopher Clark, Kenton Lee, Ming-Wei Chang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cd0185d7-a54b-4892-a77b-489ee61019fb,"Tom Kwiatkowski, Michael Collins, and Kristina"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a5c4076-286f-419d-a47b-e45bfe829910,Toutanova. 2019. Boolq: Exploring the surprising
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72aa8583-b662-4bbc-8420-74f9bc1bad5e,difﬁculty of natural yes/no questions. arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f47831f4-7da9-4e01-81d6-795b83cdd2da,arXiv:1905.10044 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8e31e78a-2c47-49f7-9f3a-9571adffc4b3,"Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,509f7734-a3f1-426e-9a06-f5f6aa37533e,"Ashish Sabharwal, Carissa Schoenick, and Oyvind"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,924fbbb3-74fb-4b06-b218-e44188a37c60,Tafjord. 2018. Think you have solved question an-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e04c85ac-2c2a-47e0-8384-496f7f21c5d5,"swering? try arc, the ai2 reasoning challenge. arXiv"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,240e7d48-de7d-44d9-8858-987bfd19e2d5,preprint arXiv:1803.05457 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2ccf3a22-0b2b-4f52-8a97-95e7ca9344b8,"Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f855fabc-a54c-4509-85a9-5949cc07f2ed,"Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a5081160-52c3-4524-af14-1c4b18e4539e,"Plappert, Jerry Tworek, Jacob Hilton, Reiichiro"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,57d96ba2-4bc9-4f46-b594-925e21221931,"Nakano, et al. 2021. Training veriﬁers to solve math"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a00b6df4-8079-41a5-be1f-734074fe8c11,word problems. arXiv preprint arXiv:2110.14168 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,787aeb2b-fbad-4105-83c1-27a47ac0734f,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5fe784fd-d805-49fd-927c-c6a5ff55c05e,"bonell, Quoc V Le, and Ruslan Salakhutdinov."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,086e2b67-4fa3-4a68-a4de-2174803ec196,2019. Transformer-xl: Attentive language mod-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a64c2873-cd1e-4f01-ab8d-8a6d2f43c9e1,els beyond a ﬁxed-length context. arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,757d31a1-435f-4316-8304-c815d601b9cb,arXiv:1901.02860 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3c51b1f-a84f-47ef-9a5d-1b814b77af01,"Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0211b02a-28ce-4143-aa3c-8f4dbc3d343b,and Christopher Ré. 2022. Flashattention: Fast and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3fed3701-b1dd-4042-a6b0-10dfad401e25,memory-efﬁcient exact attention with io-awareness.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,69296ab6-35fd-44ac-bbff-8ddc2f7653a0,arXiv preprint arXiv:2205.14135 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a741c8f-5b20-4288-81ff-544fb331884b,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5b0bb4d0-39a5-4f81-82a7-c6163547dc3d,Kristina Toutanova. 2018. Bert: Pre-training of deep
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,edc97071-a7f2-48e6-bc5b-60ef293b69d4,bidirectional transformers for language understand-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9f5c846-5d6c-4b13-94aa-1176060648db,ing. arXiv preprint arXiv:1810.04805 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c75c4d50-0e87-494a-b366-33ffae27d462,Jeffrey L Elman. 1990. Finding structure in time. Cog-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c4bdb79-92ef-4c23-9f4d-28e348bf1dc5,"nitive science , 14(2):179–211."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4ce4ae80-1406-40d9-b86f-5a687ccd15d3,"Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,402f3ee1-715c-41c1-8d00-7fa24f8e3f46,"Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,70c93497-5e31-4788-abd7-4d36265f2a89,"tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3fb61fbc-76cd-4823-967a-f4eac65a6f10,Incoder: A generative model for code inﬁlling and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9318cc8f-8fb9-4e7f-ab60-0dbdcd4a8da0,synthesis. arXiv preprint arXiv:2204.05999 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22527b3c-92f7-40f8-88bd-d2934b510a1a,"Leo Gao, Stella Biderman, Sid Black, Laurence Gold-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,441e8211-77a0-411d-8bae-3fb3aedd0381,"ing, Travis Hoppe, Charles Foster, Jason Phang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e113cec6-3e6d-432c-8896-b244154699dd,"Horace He, Anish Thite, Noa Nabeshima, Shawn"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d2d70755-da30-4e0b-aa84-4984898ea290,"Presser, and Connor Leahy. 2020. The Pile: An"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,70e3acee-1611-4c71-9820-189878a8fcaa,800gb dataset of diverse text for language modeling.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8331ac07-ba4e-49b6-897c-23825efbefc6,arXiv preprint arXiv:2101.00027 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6299978b-b79d-426d-b6a6-f23a66177ae9,"Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56a59e22-1d77-439a-871a-1dac49f0a606,"Anthony DiPoﬁ, Charles Foster, Laurence Golding,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0cae0ea7-ad49-4253-8934-2c9fd185a2ab,"Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,Jason Phang, Laria Reynolds, Eric Tang, Anish"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,913be005-f1b9-494b-b929-d77f2c8e64a9,"Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4df8842e-abd2-4aef-a383-f4e28ab57f0c,A framework for few-shot language model evalua-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,92c64fb8-cd8c-47a0-8934-49163c8a2006,tion.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79eeedd6-7ac0-4c3e-b49a-3e80fa364f1a,"Samuel Gehman, Suchin Gururangan, Maarten Sap,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8b8301c6-6c8f-46ff-9924-02e9db489e21,"Yejin Choi, and Noah A Smith. 2020. Realtoxici-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79bff9c2-9902-4faf-9dd9-e24709def7e9,typrompts: Evaluating neural toxic degeneration in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7bdb3c18-9a0e-4a68-893e-257445425c8e,language models. arXiv preprint arXiv:2009.11462 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,330aa3b1-5a5b-4608-a508-337a5016566a,Alex Graves. 2013. Generating sequences with
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,053efa82-7bfb-4a9c-986f-6ce7d0ff9e33,recurrent neural networks. arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1bc17f5f-2fd6-49ab-9c84-fe859e15897b,arXiv:1308.0850 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,70050a99-4b0c-4979-b36b-22bd15d0781c,"Kenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H Clark,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bde716ba-6532-4447-8d7a-07b53fd874f4,and Philipp Koehn. 2013. Scalable modiﬁed kneser-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,148ec836-26ab-4812-864b-8ef5142e3c4e,ney language model estimation. In Proceedings of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,51fbc1e2-ca19-48fb-b5b0-7ba6b018e8e4,the 51st Annual Meeting of the Association for Com-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,99328d60-b9b1-4616-9088-63ada797210d,"putational Linguistics (Volume 2: Short Papers) ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0c6ea113-6057-41ee-a06e-b4ab40c3c3b4,pages 690–696.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3036705f-3fd9-4cfc-9723-9106f3f3a53b,"Dan Hendrycks, Collin Burns, Steven Basart, Andy"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4b17c804-de21-430c-b2d3-f2a842deefea,"Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45cee352-4015-4868-86fc-33c70d9c5fcf,hardt. 2020. Measuring massive multitask language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ba40515f-584c-4688-b02b-1a7fc4594aec,understanding. arXiv preprint arXiv:2009.03300 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,024d0742-90ea-4bf4-8298-71169e6c4d80,"Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1b6cc25b-3f5d-46e8-bbd0-d56c94c12851,"Arora, Steven Basart, Eric Tang, Dawn Song, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f063c5b0-052f-4b3f-8fd4-26e35de089a2,Jacob Steinhardt. 2021. Measuring mathematical
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1a4f9a61-4f00-4b7a-ac8c-ee0c258e3e90,problem solving with the math dataset. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c2bf36f7-3935-40ed-9c6b-efc096754b1a,preprint arXiv:2103.03874 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d318d1ec-b2df-4d0a-b911-44c177afd027,"Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2d3cf144-1271-437e-8f39-315989acaca2,"gory Diamos, Heewoo Jun, Hassan Kianinejad,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,43f0e725-eb3d-46fb-a7ec-b40f4fda5475,"Md Patwary, Mostofa Ali, Yang Yang, and Yanqi"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b5888975-8a77-4dd5-9ad2-32d8d1edf87e,"Zhou. 2017. Deep learning scaling is predictable,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8a646a4-90a5-4f1d-9db8-461414bc8a5c,empirically. arXiv preprint arXiv:1712.00409 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f6617aee-8f8b-4e97-889f-796dd118ca84,Sepp Hochreiter and Jürgen Schmidhuber. 1997.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0a340d12-57b1-4654-85d7-95254f376914,"Long short-term memory. Neural computation ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0043cbd9-7f12-437d-b6b3-9c12244e20f0,9(8):1735–1780.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b3438557-daa0-4b59-8601-059ebd729738,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,daa3ec89-6846-43bf-8792-1bd55eaaaa1d,"Elena Buchatskaya, Trevor Cai, Eliza Rutherford,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f99f7812-5aa5-4ecd-aa81-688b8e65a4d9,"Diego de Las Casas, Lisa Anne Hendricks, Johannes"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8ded718d-9862-4d03-8f12-c99306684510,"Welbl, Aidan Clark, Tom Hennigan, Eric Noland,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2d5c61ce-c9bd-482d-8fd4-3bec0fba343d,"Katie Millican, George van den Driessche, Bogdan"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56d2a7a7-51db-4035-aab0-b84dd6578cba,"Damoc, Aurelia Guy, Simon Osindero, Karen Si-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ce76882-9214-4b5e-acce-efb5a09fdf2a,"monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe232ccb-c7c3-4bea-b275-4090b92134ad,and Laurent Sifre. 2022. Training compute-optimal
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,daecba33-f78b-4ee7-a39a-de8c2fd7a972,large language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ea6a5deb-5ac6-44a5-a81f-1c75d857bf62,"Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c3907e08-ad9c-49bf-aeca-88a8999e8540,"Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72dfd21a-b52c-4932-a52d-f9e9d19f3879,"ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ee1d99d4-2f8d-4908-82f0-13fbf01bfcc2,2022. Opt-iml: Scaling language model instruc-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,783c722a-e065-4de9-b0d1-bb8a6ad54cb0,tion meta learning through the lens of generalization.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,419a83c5-10cb-4726-83c0-d02e3d733250,arXiv preprint arXiv:2212.12017 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fbfcf944-e81f-4656-9359-7a4cea0f7694,"Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9bb39000-1573-43c4-8883-7b09e2b55b0a,Zettlemoyer. 2017. Triviaqa: A large scale distantly
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,364c9d03-48dc-473c-a5df-c8a0063c0044,supervised challenge dataset for reading comprehen-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d8abbc4-8afe-4d53-9d21-9e6ece700112,sion. arXiv preprint arXiv:1705.03551 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79f7210e-54c0-4555-97b6-4402f3a92b5a,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ca3a451-32b4-4c41-b83b-458e471bbe39,"Shazeer, and Yonghui Wu. 2016. Exploring"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ffe2ff67-1b6e-43f3-ba28-fb67dc7e997b,the limits of language modeling. arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,549c88f2-8d9b-453c-b002-972cc64acb44,arXiv:1602.02410 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b1bbcc3e-e389-4f58-8001-79a4f98603c2,"Jared Kaplan, Sam McCandlish, Tom Henighan,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2d561e08-2e32-419d-9952-beea57591049,"Tom B Brown, Benjamin Chess, Rewon Child, Scott"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,063fe866-1346-4500-bb49-4a4e36f58db4,"Gray, Alec Radford, Jeffrey Wu, and Dario Amodei."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8fbe0656-af4b-4a26-9e62-466c91c45a06,2020. Scaling laws for neural language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ffd90bce-2a6a-4595-b4c5-90381a73685e,arXiv preprint arXiv:2001.08361 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6157a64a-7cd4-4189-be38-4f4ec60b0ce1,Slava Katz. 1987. Estimation of probabilities from
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce4f95a5-4ded-4dc1-b84c-e80474a9c98b,sparse data for the language model component of a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,304bafb4-89d3-4a86-8abf-89094f399830,"speech recognizer. IEEE transactions on acoustics,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67b22a3a-418a-4818-b40c-5f7cd306d5e9,"speech, and signal processing , 35(3):400–401."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25294567-89f4-46ce-a096-5245c3caf4b6,Reinhard Kneser and Hermann Ney. 1995. Improved
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2411f120-c9e7-44c5-85d7-fc62b00caa32,backing-off for m-gram language modeling. In 1995
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b4ee24d1-3283-4b7a-abc3-44397a922fc5,"international conference on acoustics, speech, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,05dc8f7e-f139-4f2f-a509-76fe620fe694,"signal processing , volume 1, pages 181–184. IEEE."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b728bbca-abbe-45a9-a586-87ccbe5b9a7f,"Vijay Korthikanti, Jared Casper, Sangkug Lym,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b898cee8-5ea2-4e86-9ae5-20394554b4c7,"Lawrence McAfee, Michael Andersch, Mohammad"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f080cbb9-ff06-4a09-9ba3-247819d6b6ce,"Shoeybi, and Bryan Catanzaro. 2022. Reducing ac-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61303874-1e0a-4722-acfa-f64b55ae9389,tivation recomputation in large transformer models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e41ea76-b10b-44d4-8bc3-765e73fe0d3e,arXiv preprint arXiv:2205.05198 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f422f05c-a6d5-4b78-b360-32ff743aad75,Taku Kudo and John Richardson. 2018. Sentencepiece:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,648d2e2b-f948-4c5b-9c29-71b32a25d090,A simple and language independent subword tok-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f0ac9f1-e0b6-45b6-8e55-872afb2dcc79,enizer and detokenizer for neural text processing.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b397de9b-13a6-4927-9743-b9a34bfefb32,arXiv preprint arXiv:1808.06226 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8edcdc4b-560d-4a85-94e1-aabe66a42761,"Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,84541291-23fa-429c-9b84-e32ff08a2e28,and Yulia Tsvetkov. 2019. Quantifying social bi-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,15a29ac6-c655-49b8-9156-1c32eb3023a0,ases in contextual word representations. In 1st ACL
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7d299c9c-71d6-435b-a7b8-9edd69af866d,Workshop on Gender Bias for Natural Language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4ab02943-b038-40fc-985e-875da1fb123f,Processing .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eb3e56bc-1289-4f79-a82d-b3cf6a95a086,"Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9592058-7e10-425e-81a1-619ae683ec91,"ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a33c85c-1ae9-4c1f-a54f-7758793ed938,"Danielle Epstein, Illia Polosukhin, Jacob Devlin,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d1e7c98c-b462-4999-8e25-ee9c52d168d9,"Kenton Lee, et al. 2019. Natural questions: a bench-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b7cdc73-a4b0-41a7-97e8-de2efde70be5,mark for question answering research. Transactions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7034a9e0-88aa-4bfe-a1b6-2d9091a5f29d,"of the Association for Computational Linguistics ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c29f1482-a39f-4391-98cb-24b94e305b82,7:453–466.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ed3c7573-0386-4081-bd3e-16cb8e68016a,"Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3456b429-013a-4ef6-bad9-d4df0141b024,and Eduard Hovy. 2017. Race: Large-scale reading
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aaf643f4-cf67-4a56-add8-3e1723f23005,comprehension dataset from examinations. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,333a5ed8-dce1-41db-bdec-1202e982f7fb,preprint arXiv:1704.04683 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2f2d8184-9e7c-4154-95ba-54e27ca9fd62,"Aitor Lewkowycz, Anders Johan Andreassen,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7bfdcfa6-1ab6-4d5f-9108-222121bb4945,"David Dohan, Ethan Dyer, Henryk Michalewski,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c771e6a6-07f0-46ba-be3a-4418029a9fb8,"Vinay Venkatesh Ramasesh, Ambrose Slone, Cem"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a53e76c-fc7f-426a-a25f-97ddb308cca2,"Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1550d005-5b8c-4c33-b95f-6a3f09b8098e,"Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25f444ca-a60c-4176-8a0e-7db2a2835af7,Misra. 2022. Solving quantitative reasoning prob-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a4698ca3-64ac-444a-be00-af46746a858a,lems with language models. In Advances in Neural
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,75329d56-5381-41d8-8237-5e4c6b799a00,Information Processing Systems .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b2dc145-8b96-45e8-91a8-dd2cf0784e71,"Opher Lieber, Or Sharir, Barak Lenz, and Yoav"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1c221a92-2c7c-412c-9ef3-e2878c5f6319,Shoham. 2021. Jurassic-1: Technical details and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e1a5930-9393-4390-8152-01f4ae80851e,"evaluation. White Paper. AI21 Labs , 1."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cd323a1f-c8bb-4946-8566-59f41cff1526,"Stephanie Lin, Jacob Hilton, and Owain Evans. 2021."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c3dbf18-7aeb-47f5-bdb1-c05f2d5dfe08,Truthfulqa: Measuring how models mimic human
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e836d0ba-0931-44f5-97b6-5b4657022873,falsehoods. arXiv preprint arXiv:2109.07958 .Ilya Loshchilov and Frank Hutter. 2017. Decou-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62718a03-b3a6-4336-b750-4ed0c7b3385d,pled weight decay regularization. arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f9a3761-d8ee-41aa-b9a4-800c23e5f84a,arXiv:1711.05101 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d550cd2b-5b3a-4d68-8fab-255b73c8c6d9,Matthew V Mahoney. 1999. Text compression as a test
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,164cd2a2-dad7-497f-b305-ace79fa18017,"for artiﬁcial intelligence. AAAI/IAAI , 970."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5e0ad75a-f7ec-4cc0-9455-070f7ea6314d,"Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a8c71e4-95ef-4708-bda6-13e6cf1c9098,Sabharwal. 2018. Can a suit of armor conduct elec-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,17cda01d-62cc-466c-8579-e6f8f4bfe0f3,tricity? a new dataset for open book question answer-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,81cb6db2-215c-493a-9168-0c0d8801abdb,ing. arXiv preprint arXiv:1809.02789 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ebd153bd-c206-4d79-9ed0-20b6c7861370,"Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79bce34b-284a-4585-9ac9-37ff44a60874,"Cernock `y, and Sanjeev Khudanpur. 2010. Recur-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2bd67d0d-d603-40f4-9624-973bf24dbf14,rent neural network based language model. In In-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b432140a-790e-4361-a1ef-858de76348ab,"terspeech , pages 1045–1048. Makuhari."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6d209121-38e0-4269-94c9-ba4bb812c48a,"Nikita Nangia, Clara Vania, Rasika Bhalerao, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,badb603d-47b9-4ca3-aceb-cabd355b374e,Samuel R. Bowman. 2020. CrowS-pairs: A chal-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e13b4d93-4eca-40a4-b2a6-0f18d912d04b,lenge dataset for measuring social biases in masked
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e8e66b65-ead6-41cb-9e43-a337b469b1dd,language models. In EMNLP 2020 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d7bd557e-00c1-4483-9333-d40c0cbcd229,"Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b22008bd-869e-460f-a4d7-ab5d069762ae,"Huan Wang, Yingbo Zhou, Silvio Savarese, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,047fd233-2560-4a18-b8a3-833fbed0c9ce,Caiming Xiong. 2022. Codegen: An open large lan-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,389f92a1-3be2-431a-846e-1691dc6d6388,guage model for code with multi-turn program syn-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0f280497-1526-483a-b46d-e49ecd823eff,thesis. arXiv preprint arXiv:2203.13474 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1bfab3c8-0d52-4c70-9953-ae58ec09fe24,"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef1f55c0-ebea-458c-a1fc-f7cb84c381a6,"Carroll Wainwright, Pamela Mishkin, Chong Zhang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a865a68d-8a18-400c-8628-c2f368d04e4b,"Sandhini Agarwal, Katarina Slama, Alex Gray, John"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bc475425-6804-4e69-8fa9-1089f5bfce89,"Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d233e19-d0ca-4a93-b3b0-aeccc2cfa56a,"Maddie Simens, Amanda Askell, Peter Welinder,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9804cef2-218a-4571-ae1f-3e337e1f0ea4,"Paul Christiano, Jan Leike, and Ryan Lowe. 2022."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,48dfbd7f-c5c7-4971-8074-558fba969e89,Training language models to follow instructions
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7a5ba6b4-856c-4185-8cc7-fd72ef88121d,with human feedback. In Advances in Neural Infor-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dfa41746-4be7-4123-8029-752207cbc3b2,mation Processing Systems .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b6c8029f-962f-488a-b7cd-8c6a90a43080,Markus N Rabe and Charles Staats. 2021. Self-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a25ddab4-3002-464b-86e3-066e4838861b,attention does not need o(n2)memory. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,217be494-0845-40aa-a4d9-f0bc09c26467,preprint arXiv:2112.05682 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d87089f6-f359-4787-aeda-76210d9bcb67,"Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,22099351-72bf-4413-8982-a37ab3dc2ee0,"Sutskever, et al. 2018. Improving language under-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8eac2298-aec1-4e03-88f4-496c79c39e24,standing by generative pre-training.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef8c1b3b-3315-48f0-a288-3083a0947118,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4e063663-4858-4fbd-9198-50f8ec0b14e6,"Dario Amodei, Ilya Sutskever, et al. 2019. Lan-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,555ce050-3892-43e0-9d90-825ee5aab199,guage models are unsupervised multitask learners.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce481ebc-9eb2-4892-88ed-5eed50e0ed52,"OpenAI blog , 1(8):9."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,66999eda-0d9c-4d87-8e96-f155db3eddf6,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cc5b32cd-d180-4591-b34d-89d625d2f255,"Millican, Jordan Hoffmann, Francis Song, John"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6bb58825-7392-4dde-9150-44032becee3f,"Aslanides, Sarah Henderson, Roman Ring, Susan-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e0f658cb-69a7-4512-8927-48efceab4201,"nah Young, Eliza Rutherford, Tom Hennigan, Ja-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be2c1c65-7138-400b-ae51-98d7795469b6,"cob Menick, Albin Cassirer, Richard Powell, George"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8c43c813-78d7-40cf-be3e-ccdb712d8a54,"van den Driessche, Lisa Anne Hendricks, Mari-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,91b53270-1359-46cf-b069-5aaadd8ebb52,"beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,20f5c5c1-15d7-4102-b020-604d35d2db34,"hannes Welbl, Sumanth Dathathri, Saffron Huang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,61a1892c-0702-4cff-a774-1db461afa6b8,"Jonathan Uesato, John Mellor, Irina Higgins, An-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,455fcae1-d30d-43e4-9696-06e38ee8ac75,"tonia Creswell, Nat McAleese, Amy Wu, Erich"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce48b1c1-eb89-4a9a-b832-a1c87e596de7,"Elsen, Siddhant Jayakumar, Elena Buchatskaya,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,443c63a7-1310-465e-b801-c466c9c432a2,"David Budden, Esme Sutherland, Karen Simonyan,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8fe3f19-335a-4e11-a571-55c25ae8775f,"Michela Paganini, Laurent Sifre, Lena Martens,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64790320-5dbb-4fee-89cf-9a414540a9cc,"Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c197dcf6-f320-41dc-b9c9-ab78f0a5e508,"matzadeh, Elena Gribovskaya, Domenic Donato,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bb7339b5-134c-4b19-a85e-fc457dfd73a4,"Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,74475991-c50c-4db8-8e06-b06f419ae6cd,"Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,42058e41-f1ea-47c9-b3b8-6b58088443a2,"Doug Fritz, Thibault Sottiaux, Mantas Pajarskas,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c09a2893-5fef-48fd-86f8-d3fb1f22b160,"Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,449c45bd-97b7-4192-b9b9-371d105d710e,"prien de Masson d’Autume, Yujia Li, Tayfun Terzi,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56db7326-abf2-47d6-bca6-9d14cf1f6ebc,"Vladimir Mikulik, Igor Babuschkin, Aidan Clark,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e5ad8a3b-8c1b-42e0-9c24-64755e5e27ac,"Diego de Las Casas, Aurelia Guy, Chris Jones,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ce7e589-2ad0-485e-88de-f7c3b536fbe3,"James Bradbury, Matthew Johnson, Blake Hecht-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a65e7b0a-9909-451d-a4fc-52a00f039d01,"man, Laura Weidinger, Iason Gabriel, William Isaac,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0231fa37-baae-4925-b5af-6673efe47bdf,"Ed Lockhart, Simon Osindero, Laura Rimell, Chris"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25a45af9-e1d6-41c7-aae3-b65cfa290f90,"Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c47adc26-52ba-4510-b70b-7fb236857880,"way, Lorrayne Bennett, Demis Hassabis, Koray"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a5f17a6-9303-4640-b52d-7feeb3e9a168,"Kavukcuoglu, and Geoffrey Irving. 2021. Scal-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c973b522-fd44-4c79-a62d-66f263bcbe68,"ing language models: Methods, analysis & insights"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ab24875f-d1fd-410d-9697-40b363ead7ad,from training gopher.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fdf56c0b-0b15-4181-a736-2a665b06235a,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a60884b9-cfe0-4ff5-b0da-4ef5c88eeb39,"Lee, Sharan Narang, Michael Matena, Yanqi Zhou,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4c2fda88-fa20-4e9c-b2d4-a2a370b13c9a,"Wei Li, and Peter J Liu. 2020. Exploring the limits"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb899eb7-6bb5-4b29-8230-72120ec5b1f7,of transfer learning with a uniﬁed text-to-text trans-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ee83d381-0083-45a5-bb0e-63fdd3e2f873,"former. The Journal of Machine Learning Research ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4a91bedb-7972-4e4a-b342-5a10c23fe885,21(1):5485–5551.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e7893f2f-0d3e-4b04-a868-9e84f417eae8,"Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Be-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f78f5c0-27da-48d4-9687-58be099871b2,"linkov, and Nir Shavit. 2019. A constructive predic-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6045e73b-b3dd-405e-8c28-5eac412d1d2b,tion of the generalization error across scales. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9401231b-6cd4-46a7-9968-95b3ef37f5ce,preprint arXiv:1909.12673 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,424b77b2-810a-4ce0-9fb4-fa35b06c6e6e,"Rachel Rudinger, Jason Naradowsky, Brian Leonard,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,40393365-c5bd-49bb-8a9d-0bb06385c325,and Benjamin Van Durme. 2018. Gender bias in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,554310c8-8fe7-4756-b153-0362764ee36d,coreference resolution. In NAACL-HLT 2018 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0812ed59-7980-4438-b248-7f354d386bb1,"Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ab36f93d-94c2-4ea5-a034-0e24bf9c2e03,"ula, and Yejin Choi. 2021. Winogrande: An adver-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e19f681d-34cd-405a-a4ce-d05417d4fe11,sarial winograd schema challenge at scale. Commu-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8d8d19c6-a1d3-4d7b-997d-b3fa82ded428,"nications of the ACM , 64(9):99–106."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,34e64c09-ee85-43e4-8ab7-768345c666f3,"Maarten Sap, Hannah Rashkin, Derek Chen, Ronan"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,401f5302-c4cf-4097-a61e-d8df77274e4e,"LeBras, and Yejin Choi. 2019. Socialiqa: Com-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be64ede6-c904-4d13-920c-00a764020dbf,monsense reasoning about social interactions. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9b8c341c-c2ae-463c-98ec-628342302d66,preprint arXiv:1904.09728 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d190245c-d450-4851-90da-a931e61db0a4,"Teven Le Scao, Angela Fan, Christopher Akiki, El-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,836e0d7c-401b-46cb-b86a-923edd1065b1,"lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c2970189-05ff-4983-8e67-9c02fa983a4f,"man Castagné, Alexandra Sasha Luccioni, François"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b2298e43-2f44-41b5-8294-fe27acbbdd52,"Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,72c79292-8c8e-4aa1-b3a4-265f423b1372,parameter open-access multilingual language model.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23766810-3482-478f-a381-138205721b8d,arXiv preprint arXiv:2211.05100 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cbeeac4f-51ec-493f-b930-9e9ed6211cf1,"Rico Sennrich, Barry Haddow, and Alexandra Birch."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,baa7efdd-5a72-4115-bec7-0aa70865f10a,2015. Neural machine translation of rare words with
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c26e7fb-7fd4-4110-a94b-0c89f5039ed2,subword units. arXiv preprint arXiv:1508.07909 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,328687c7-bad4-4e9a-a353-8fed1b80bdec,Claude E Shannon. 1948. A mathematical theory of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,faf3e8b1-86a6-45b8-85de-bab94894c9ee,"communication. The Bell system technical journal ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,71bb391e-2d03-493b-a645-02c45b817a60,27(3):379–423.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1b66739b-77ca-4fea-9f41-33873fbb3182,Claude E Shannon. 1951. Prediction and entropy
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,21d9d03a-cb6a-4460-9175-d5d62522ccbe,"of printed english. Bell system technical journal ,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,488ea49a-80ce-4743-b043-b81ec460f48f,30(1):50–64.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bb86b120-7d79-4d57-a85c-6b63971d1b65,Noam Shazeer. 2020. Glu variants improve trans-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,96a686ad-df7f-4c2e-b889-aba782da02e9,"former. arXiv preprint arXiv:2002.05202 .Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,660a5f59-0ce8-4f04-b7bf-102a2e00c779,and Nanyun Peng. 2019. The woman worked as a
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f59c1c19-63a0-400b-9181-7d0bde679354,babysitter: On biases in language generation. arXiv
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f421ede-c640-4b8c-8031-e63c778c4633,preprint arXiv:1909.01326 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0d28271d-0d51-43b7-8872-c69816573975,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,976182be-0be0-4d61-a29c-d67c51b880a7,"Patrick LeGresley, Jared Casper, and Bryan Catan-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f9459dce-9e56-415b-bcfb-102e772b4ccf,zaro. 2019. Megatron-lm: Training multi-billion pa-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d1b8d9e-c604-44fa-b671-90135b7d370f,rameter language models using model parallelism.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23f0f96d-e8dc-41f4-8826-1a43afdb97d0,arXiv preprint arXiv:1909.08053 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0185e7ae-86bf-4811-826c-9977f870c6b1,"Shaden Smith, Mostofa Patwary, Brandon Norick,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,442a4790-2ad4-403c-b6e9-1e5a9b67764e,"Patrick LeGresley, Samyam Rajbhandari, Jared"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,503130cf-4db3-4ba1-becc-1fa9cc702cc1,"Casper, Zhun Liu, Shrimai Prabhumoye, George"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8833fcd8-fb1d-4315-8c3b-2830a5f56210,"Zerveas, Vijay Korthikanti, Elton Zhang, Rewon"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,10a23649-7c75-48f9-b438-c4ebb0330403,"Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4e87c44b-56d3-4e6a-90d2-4df00e865408,"Song, Mohammad Shoeybi, Yuxiong He, Michael"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,daed4e11-861c-4800-b4d8-7566eb479737,"Houston, Saurabh Tiwary, and Bryan Catanzaro."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5ae684cc-2e86-4f91-86b6-ebfa71b1aac9,2022. Using deepspeed and megatron to train
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,81e5ca6e-22cb-407a-ac12-bb42e8bf38f1,"megatron-turing nlg 530b, a large-scale generative"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f30a10a6-1dd7-4bf4-a00a-ba2949697021,language model.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9e95d8d-7823-4b21-8db1-f0d798679566,"Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,41549cf7-e4be-40b6-bb0d-3a93d2687c8c,"Bo Wen, and Yunfeng Liu. 2021. Roformer: En-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e53f8822-dbd2-47bc-9de3-238032047d17,hanced transformer with rotary position embedding.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c6f8b926-11c4-4eb8-9edb-c86636355811,arXiv preprint arXiv:2104.09864 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7b7382b-fc26-4c52-8acd-6255c25d9a22,"Romal Thoppilan, Daniel De Freitas, Jamie Hall,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6cdbac55-38f2-455f-9e91-85d5d02b6891,"Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,acaf119e-ba59-45e7-8be9-05fdd704effe,"Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aaaacbf4-69ce-4c8c-b3b3-1678170798db,"YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,276cb854-9b31-4faa-862e-5f10ba5f72c4,"Amin Ghafouri, Marcelo Menegali, Yanping Huang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9b24bcb-e151-44af-9dcb-623003cc7d96,"Maxim Krikun, Dmitry Lepikhin, James Qin, De-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b00fe6ec-b9b7-4e53-9e37-60274579b682,"hao Chen, Yuanzhong Xu, Zhifeng Chen, Adam"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,30edd434-c1f8-446b-b174-a1f9052fb3f7,"Roberts, Maarten Bosma, Vincent Zhao, Yanqi"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ea6eb75-51f0-4db3-8ce2-f1b40ddc0e37,"Zhou, Chung-Ching Chang, Igor Krivokon, Will"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c65af9a9-ae62-463e-8ec3-54fd8bf5f260,"Rusch, Marc Pickett, Pranesh Srinivasan, Laichee"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4a1ae4ad-6202-49d5-8f55-7359d8bfdb8a,"Man, Kathleen Meier-Hellstern, Meredith Ringel"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,130c0a36-d250-4cf3-b4e5-9e85144bbbb2,"Morris, Tulsee Doshi, Renelito Delos Santos, Toju"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,36b7f0aa-7cf7-48d9-ab3d-eec8fa1fc8b6,"Duke, Johnny Soraker, Ben Zevenbergen, Vinod-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,112e1e6a-0221-47fd-8996-badb1e8d27d6,"kumar Prabhakaran, Mark Diaz, Ben Hutchinson,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1a2effaa-7fc0-4e04-960c-fca548d796e5,"Kristen Olson, Alejandra Molina, Erin Hoffman-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3983604c-ee26-4bde-a162-240ffafbeb89,"John, Josh Lee, Lora Aroyo, Ravi Rajakumar,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ca531879-fb7b-4fa3-b400-51202f531f81,"Alena Butryna, Matthew Lamm, Viktoriya Kuzmina,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,76d65de2-007d-432f-a7ce-84a24ac81113,"Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4e0b6e1b-4fbf-4a09-8b60-52cff459b29a,"Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78d77409-96f9-45ec-b085-7ed857f68905,"Croak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be9aa961-97f2-40b4-82cd-aa95bda6878b,guage models for dialog applications.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,004d5be5-27cb-4eed-a489-efe3b5677d52,A. M. Turing. 1950. Computing Machinery and Intel-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,db19aed2-433e-4381-a142-96b4819fbbb8,"ligence . [Oxford University Press, Mind Associa-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1702064c-c813-4c47-afdd-fca0e961384b,tion].
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,506cb5fb-6934-41c9-8a67-14b904449a76,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cc64f549-a08a-434c-ab05-37dbfda4926c,"Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c49a87d-a4ce-46b5-bfab-328bd0d6cb38,"Kaiser, and Illia Polosukhin. 2017. Attention is all"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2ccf301-a962-4e93-addb-bb2bc5421e87,you need. In Advances in Neural Information Pro-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a584f2a6-a393-41f8-bbb4-10028af4e143,"cessing Systems 30 , pages 5998–6008."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fca89bb6-bc32-4db1-b2bf-b740c8e47e2b,Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,536c807e-fcc2-422e-aea2-5ef9ff3197e8,6B: A 6 Billion Parameter Autoregressive Lan-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,55bef88f-77a6-459b-8f2c-f8ac7d67f931,guage Model. https://github.com/kingoflolz/
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,faa0d766-d382-4029-a824-525aa7b8d745,mesh-transformer-jax .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,11c57281-6f9a-456c-b5d9-6a77e5f3f815,"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3afbd817-dd35-4367-bd6c-322d12aee8ae,"Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2ce65276-ed28-499e-901b-d9ad282b4617,and Denny Zhou. 2022. Self-consistency improves
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0df07bfe-3ad8-4924-b109-3638af74925c,chain of thought reasoning in language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,694954cb-2865-4c9e-b23b-c84b17eb71c0,"Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,568530d5-4825-410e-882f-b96a524bb5d3,"Barret Zoph, Sebastian Borgeaud, Dani Yogatama,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f191b603-8b45-4ed7-80f5-ad0fc3b3e824,"Maarten Bosma, Denny Zhou, Donald Metzler, et al."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e98b1331-128b-49cf-9b0f-d5650fab0372,2022. Emergent abilities of large language models.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,17551e6a-f41e-4005-98e7-257f2338a2ef,arXiv preprint arXiv:2206.07682 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4ec29873-7abc-4a4b-bf0b-512c68f9a40f,"Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,20b9784b-19b6-4749-9d93-c8ca20e7632b,"neau, Vishrav Chaudhary, Francisco Guzmán, Ar-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3138925-9cd9-4214-bd35-5a602ca73042,"mand Joulin, and Edouard Grave. 2020. CCNet: Ex-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,53951cb2-d48e-4119-9bf5-84f3233dc47f,tracting high quality monolingual datasets from web
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,870834b5-0983-4975-a492-929b42928819,crawl data. In Language Resources and Evaluation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,15e50889-4c98-4e0b-85b9-91e3f9449548,Conference .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,39bb05f4-dfc3-42d1-8e98-ab7f99300852,"Carole-Jean Wu, Ramya Raghavendra, Udit Gupta,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a30cc075-b230-4e7a-940d-b420787473b2,"Bilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d7cae5fd-18b6-4930-8a49-d7f77cc1ac0b,"ria Chang, Fiona Aga, Jinshi Huang, Charles Bai,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45d535d3-caa2-40eb-9765-6a8520abcd9e,et al. 2022. Sustainable ai: Environmental implica-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25e14983-175e-40e8-bfce-f74e1b20521c,"tions, challenges and opportunities. Proceedings of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6435f3bb-c1ea-4595-9b1c-4495a33f55f8,"Machine Learning and Systems , 4:795–813."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,806e8653-c20e-4de9-91e0-fc618a5921bb,"Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aa93dd98-3bb3-4e95-8097-cf4cfef5a4c3,"Farhadi, and Yejin Choi. 2019. Hellaswag: Can a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0044b3bd-a066-47cc-8f81-57bb3b9d50f9,machine really ﬁnish your sentence? arXiv preprint
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0c2f81f3-23af-4d87-b3bf-6c3bd66dd98c,arXiv:1905.07830 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25ae8f51-d9a4-48cb-ac3a-a9a0075d989c,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8d05d29-6065-42fd-a2ef-0dcbd755213d,"Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c70e29ca-84da-4f10-bafd-db2ab8c27337,"Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,89d83eac-ec5e-4b31-b8ea-dafc081d43fd,"Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce94e717-af43-4dc0-8d3a-6ced8b41e9d2,"Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8d83c78e-d184-4ab3-94e6-0a3a660545a5,130b: An open bilingual pre-trained model.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,365467c2-1d79-4142-8235-14e0e5e54560,Biao Zhang and Rico Sennrich. 2019. Root mean
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df5880e4-07b4-4709-92ec-8a5cde8a72da,square layer normalization. Advances in Neural In-
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,50c5e3fa-df0d-47bb-bfdd-567e4ebc4ddd,"formation Processing Systems , 32."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eea65056-628d-498d-8792-26fbe2bda4c1,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,31e66c12-1f84-4e15-9ac4-c7ef620925f7,"Artetxe, Moya Chen, Shuohui Chen, Christopher De-"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e6feeb6a-1844-40f1-b618-0db1fb09acc6,"wan, Mona Diab, Xian Li, Xi Victoria Lin, et al."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6ef05e3d-2089-4345-901c-e97431b200f6,2022. Opt: Open pre-trained transformer language
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6d6ad25e-2dc7-4f87-a2b2-f7679c276403,models. arXiv preprint arXiv:2205.01068 .
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,744a94e2-c32b-45f1-9527-19e85ee41ad5,A Question Answering
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0990b807-58ec-4026-9f24-ba06ac0a5652,We evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fab56980-3abd-4c42-a8d5-e1f592067cbe,for open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2a6c4c97-5de6-4c02-bd85-04ca00861d03,"of the ﬁltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unﬁltered set"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cc73f6a9-ab60-436f-8949-c7746740f886,for which the online evaluation server is not available anymore5.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,130a8553-4641-4ea4-98e7-a4c0e1067b3b,"We generate answers using greedy decoding, and extract an answer from the generation by stopping"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37f25aff-4d22-4fb9-8eee-22e24050508b,"at the ﬁrst line break, ﬁnal dot or comma. Generated answers are evaluated with the standard exact"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3b7c567-662f-4b51-a1b4-c0aef98da5f8,match metric: a generated answer is considered correct if it matches any answer of the list of answers
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,618af979-2299-4684-b361-7afb83f0d01e,"after normalization. For this normalization step we lowercase generated answers and remove articles,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,33e778af-74e2-4b0a-a8e2-515906ea54ba,punctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ed294f85-d49f-4451-9dd3-03c1758b4421,"Natural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ff94d814-bacf-4c9b-a755-1a863c645a9a,questions:\n to the list of questions and answers.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d83fdfe1-d0bd-4b06-9527-a594fa00bc3d,Context!Answer these questions: Context!Answer these questions:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64248144-5db4-4a5c-9c0d-6a16419acbb7,Q: Who sang who wants to be a millionaire in high society? Q: In Scotland a bothy/bothie is a?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8844f433-0296-40c0-9c0a-40d806128ec2,A: Frank Sinatra A: House
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ce09e37c-cbd8-42a5-acff-5996c05cb3c4,Q: Who wrote the book the origin of species? Q: The ancient city of Troy is located in what modern country?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a6b40848-1d43-43d2-8e55-def9ac83a72b,A: A:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7d17eff7-7f2f-4490-9a24-32aee78bb0e9,Target!Charles Darwin Target!Turkey
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f1a8ed6a-e9ea-414c-9f44-c44947ddec23,Figure 3: Formatted dataset example for Natural Questions (left) & TriviaQA (right).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c0fb26c0-687e-4ce2-9bfa-db7a2ceee581,5https://competitions.codalab.org/competitions/17208
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0d65b2d8-ee2c-40f5-88c9-aaf750b280d6,B MMLU
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,97ff5396-a8f3-42a7-932b-9e192e255b50,GPT-3 Gopher Chinchilla LLaMA LLaMA-I
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c1d73f45-e6a4-4823-9021-68566bc35690,175B 280B 70B 7B 13B 33B 65B 65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c982323b-612f-4541-902b-63c0a715a6b2,Abstract Algebra STEM 30.0 25.0 31.0 29.0 34.0 32.0 34.0 31.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,869b055b-1fb7-442a-a276-6dbf5202ae8c,Anatomy STEM 48.0 56.3 70.4 37.0 45.9 51.9 57.8 62.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,06ad67c8-f137-4c98-8b6b-e6d4c1602449,Astronomy STEM 49.0 65.8 73.0 33.6 46.1 61.8 72.4 81.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,705f5af6-692d-4543-a30b-cfb2d2cdb3ee,Business Ethics Other 46.0 70.0 72.0 40.0 45.0 56.0 57.0 72.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,703170e1-7acd-46ff-b66d-edcc23d48310,Clinical Knowledge Other 48.0 67.2 75.1 35.1 45.7 57.4 65.3 69.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,48cccbe8-a7cc-4b34-9a0c-ec0ecb43ab13,College Biology STEM 45.0 70.8 79.9 37.5 45.1 58.3 68.8 81.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6c37e843-c770-46c2-92ed-5a9511496ecc,College Chemistry STEM 26.0 45.0 51.0 32.0 30.0 45.0 50.0 45.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a3bfa6dc-485d-46a5-bedd-6c0d43f671f8,College Computer Science STEM 46.0 49.0 51.0 29.0 39.0 45.0 47.0 51.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79a74fcc-f5fe-4bd5-990b-520ceeb2c0cd,College Mathematics STEM 34.5 37.0 32.0 33.0 32.0 40.0 35.0 36.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,abf3fc7a-7ccc-4d0c-9897-4cc09ea5e12f,College Medicine Other 48.0 60.1 66.5 30.6 42.8 52.0 54.3 63.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2605269a-4dd5-4005-a5f4-5a783431c5ed,College Physics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,584bcf3b-7a30-47af-bf43-20f9e615dfdf,Computer Security STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23106ce7-0e8b-4516-9ded-f69398758a35,Conceptual Physics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5581e361-d180-4bfb-a0d2-ddca3846b235,Econometrics Social Science 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,664de1fe-027f-4776-899f-12e1704eba5d,Electrical Engineering STEM 50.0 60.0 62.1 26.9 40.7 49.7 53.8 60.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bcad500c-a349-4e6e-8563-40eef7b22464,Elementary Mathematics STEM 30.0 33.6 41.5 24.3 24.9 36.0 37.8 42.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b0a7ae8c-46ca-4975-8cc2-450ce565f423,Formal Logic Humanities 29.0 35.7 33.3 27.0 33.3 34.1 44.4 47.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,956fce7e-35c9-4e37-b073-98428ccb31ee,Global Facts Other 37.0 38.0 39.0 29.0 35.0 35.0 39.0 40.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1b81dedc-c04b-4ba8-939b-4c473a19b1d8,High School Biology STEM 48.0 71.3 80.3 34.5 52.6 67.7 73.9 82.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2a2a45c-9278-4cca-a850-708cd11136c0,High School Chemistry STEM 33.0 47.8 58.1 28.1 28.6 41.9 40.4 44.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2b97990e-34a5-4f78-bae1-cd368dd8d2a7,High School Computer Science STEM 39.0 54.0 58.0 31.0 48.0 60.0 67.0 73.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,32d0b648-3f17-49a2-9b47-2fa21b4ebf63,High School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,579540da-825a-4e1a-a360-dbc677b51a44,High School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,26375ed7-fbf4-42ce-be5f-c7dbe46871ff,High School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d9ea23d7-393b-428a-854f-042c29f8a785,High School Macroeconomics Social Science 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,93f23550-f000-43ca-b739-28754eadacb2,High School Mathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,df43f83a-b385-4842-89eb-aec1b41ade00,High School Microeconomics Social Science 42.0 66.4 77.7 31.9 47.5 55.5 68.9 78.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,771bd301-1adb-4a72-9c56-f1cb17811154,High School Physics STEM 28.0 33.8 36.4 26.5 28.5 35.8 37.1 41.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7125499-f587-4aea-9928-891615a6ea10,High School Psychology Social Science 61.0 81.8 86.6 47.3 60.9 76.2 82.2 87.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,34f8c9c7-d937-4a41-966d-e7e72a4e725d,High School Statistics STEM 30.5 50.0 58.8 35.2 30.1 45.4 58.3 59.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,da31941e-01ff-45c4-891a-7c64e812a95d,High School Us History Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f72d006-1c7e-49c1-9743-9ef91d08654d,High School World History Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8a97a53e-b3ad-41e9-86ac-8f7dadfe46db,Human Aging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0df3aadf-b0ff-4359-9e7e-5d6efae70827,Human Sexuality Social Science 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14abb3e4-c6a1-4c87-985b-196a244a2c44,International Law Humanities 55.5 77.7 90.9 51.2 62.8 72.7 79.3 87.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,00e0d28c-4cba-43d9-a8a3-b2f2e904a264,Jurisprudence Humanities 55.0 71.3 79.6 38.9 51.9 70.4 73.2 85.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7dc08a39-bc2a-4beb-b4da-56847731cb19,Logical Fallacies Humanities 48.0 72.4 80.4 39.3 52.8 68.1 77.3 80.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e6824ffe-0f9e-4a7e-922f-068de0a04abb,Machine Learning STEM 31.0 41.1 41.1 23.2 31.3 39.3 49.1 52.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0239d580-6f2c-47f3-940c-6d53f332ca4b,Management Other 56.0 77.7 82.5 35.0 66.0 77.7 82.5 83.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0da800bf-a859-402f-987c-2765deb6a127,Marketing Other 60.0 83.3 89.7 46.6 71.8 83.3 85.9 92.7
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,958b8520-9015-4c8e-bb27-1c565987f9aa,Medical Genetics Other 40.0 69.0 69.0 43.0 52.0 67.0 67.0 68.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3519fdf7-3757-4928-875a-6f30b83782e2,Miscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4c58bca9-ec6a-4ef1-a7c0-b4ddd0532e46,Moral Disputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c492d67a-cc9d-42ae-b1e8-d6644aeab5b4,Moral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,035982c5-a19d-4cab-895b-8cc8898636c5,Nutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d5518e40-76d3-4066-a036-6a23c04f4bb8,Philosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f38f619-49ae-44c2-9e32-f1311bf647cc,Prehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,640d32cb-5051-4ca3-80fd-5dbf5d82f3de,Professional Accounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.5 56.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,953fa4a6-95e0-4839-b963-e723284311fb,Professional Law Humanities 34.5 44.5 56.5 30.2 38.0 45.9 49.1 54.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,88ad9781-0617-4f50-ae3f-b66d9e8fd280,Professional Medicine Other 36.0 64.0 75.4 44.5 50.4 54.0 61.4 70.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ba9dec4-462c-459a-a7e8-de2b023c581a,Professional Psychology Social Science 44.5 68.1 75.7 35.1 47.7 62.9 65.7 71.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b9d0c137-c9d0-4543-a3d8-5e0ca9196e23,Public Relations Social Science 48.0 71.8 73.6 40.9 60.9 67.3 73.6 74.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cfaebf11-7156-42f8-a6b9-5dce0a00e576,Security Studies Social Science 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,520383cd-dfc2-4205-8faf-9505406338ee,Sociology Social Science 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f4e6031-5e03-4b29-80c0-00c2efc70430,Us Foreign Policy Social Science 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,abb87a1c-50b7-4840-8f0c-10238771a4a3,Virology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d5564e35-6f1e-45d8-aa67-162c3d7c4290,World Religions Humanities 55.0 84.2 87.7 50.9 67.8 81.3 81.3 84.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f2b61815-6031-4618-9635-ebc72f60a7a1,Humanities 40.6 56.2 63.6 34.0 45.0 55.8 61.8 67.4
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cb376520-ec3e-4ceb-95bc-ac6383f1a493,STEM 36.7 47.4 54.9 30.5 35.8 46.0 51.7 56.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4d5a100e-2fc3-492f-a595-5c3c550676d7,Social Science 50.5 71.9 79.3 38.3 53.8 66.7 72.9 79.2
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4f13d9f7-93ee-4c3f-8b8a-556a92ecbca7,Others 49.0 66.1 73.9 38.1 53.3 63.4 67.4 72.6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b0b44574-573f-4fab-869f-d5e0427f3b7e,All 43.9 60.0 67.6 35.1 46.9 57.8 63.4 68.9
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6ef1fa4f-eea8-4c54-a2e9-8bb5bc0988a8,Table 16: MMLU. Detailed 5-shot results per domain on the test sets.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,39dbc0cb-66f8-44f2-aa71-47ea0ac6932a,C Generations from LLaMA-65B
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,44068798-c04a-4740-ad27-180f4f423e8a,"In this section, we show some examples of generations obtained with LLaMA-65B (without instruction"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,382f9242-5d38-4c5c-ac2c-5d11b72c374f,ﬁnetuning). Prompts are in bold.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6938c14c-9ce0-4673-aa22-8d27a6bfed8a,"1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 Everyone knows the above sequence, but"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5209adc-1457-4400-9c3f-4d286a0199fe,how many have wondered why it grows so fast? It seems to be the fastest growing sequence
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,76e20b7a-2eee-41f4-a2e3-2e3f5b0483cd,in mathematics.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a11d7c7d-3db9-46a1-9f87-207ed4b020e8,The sequence was discovered by Leonardo of Pisa around 1200. It is an inﬁnite sequence
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,79f799cb-8a00-4cab-9717-58721d540acc,"which appears to grow very rapidly, at least in its initial terms. It is perhaps the most famous"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dc8f6eee-f3b3-47f9-8bd1-9f343f7c7065,"sequence in mathematics, for it pops up everywhere. The sequence is called the Fibonacci"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d85f37c8-8ff8-4fc4-a0b8-d42c7501c14e,"sequence, named after Leonardo of Pisa, who was also known as Fibonacci."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6a270b25-1040-42a0-ab47-68a1ae00b03e,"He ﬁrst investigated this sequence in his famous Liber abaci of 1202, when he studied the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,38e5c782-1ab1-407e-a3ce-64d079ac7861,growth in the population of rabbits based on idealized assumptions. See the Fibonacci
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,30b53994-bb92-4863-9610-386906423925,rabbits page for the details.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6fe058d3-639d-4f2e-be91-cd672bafa86a,Here is the recommendation letter that I wrote for an application to a dragon feeder
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3cad2b54-31df-4144-81eb-32c987b18095,position at the Magic Unicorn Corporation:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b27dc38-1750-4829-97ee-ae866fafd3d9,"Dear recruiter,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7c87ee5a-9390-49d9-916b-1641eb9de0dc,"I have known ___ for two years, and I believe that she would be an excellent dragon feeder"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7677073f-08f2-474b-8c6f-bb8ca25a6da9,for the Magic Unicorn Corporation. ___ has an ability to remember and process large
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1c9fe7d2-e6bd-4503-8926-6e5f2070061b,"amounts of information, which is an important skill for a dragon feeder."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,111e5a80-0011-4276-92e5-8b6f7ed4c43e,"___, as an accomplished knight, has a deep understanding of how to kill dragons and how"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f9b9d803-7fc1-4bc1-9974-37b8fb140abb,to use each dragon’s weaknesses against it. This means that she knows what kinds of foods
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e2ae8a33-0bdc-415e-b952-99569f23cb06,each dragon likes and what kinds of foods are dangerous to each dragon. This knowledge
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,582b9843-11dc-4d7f-a954-279c6bb520e1,and experience will be invaluable as she feeds the dragons.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,326a79fd-4950-4fb3-93ed-01e22d0abed5,"I am conﬁdent that ___’s competence, skill, and experience will make her an excellent"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b7d5e773-5acf-4632-be9f-2c859ba759f2,employee. Please contact me at (___) ___-___ if you have any questions. I look forward to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d9cf08c9-2c9b-4fc0-8b71-0785a602c200,hearing from you.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a9d7fc5f-cb6a-4c23-a7fb-19db4d5a1022,"Best regards,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5134b79a-18dc-458c-ab5e-d0a944a0e376,Honorable Knight
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c29c0551-27ff-4b2b-a1f7-88d12bc08f68,Sir George
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f3eedce9-64ce-450f-8f2c-228d23b76bf4,"1def solve(a: float,b: float, c: float):"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8fd587fd-cfdd-47ee-9b2f-84d5f6f9e986,"2 """"""finds real roots of ax^2+bx+c"""""""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dd818013-f059-40d5-aa27-b099dbbb5d78,3 # discriminant
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7b320bb6-3e4b-4f6d-8c81-f7de22e7d36d,4 d = b **2 -4* a*c
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a8b3cd5-9fe0-4c82-a707-dd7b665e5787,5 ifd > 0:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d5e1642c-cb3b-45db-80f6-c08363b8a975,"6 return (-b+ math . sqrt (d)) /(2* a), (-b- math . sqrt (d)) /(2* a)"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ca66db9c-5486-47cf-8ea0-74fb2c3ca0ba,7 elif d == 0:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,808d8207-2e84-4a61-85cd-cc247b4f6dea,8 return (-b) /(2* a)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aeee1656-54eb-42d6-afe2-7157f37d7052,9 else :
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ff609e1c-104a-43a2-aca2-5cddc4b1a7a0,10 return False
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,82299eba-e306-46c9-828c-c88563bacb7d,Did you know that Yann LeCun dropped a rap album last year? We listened to it and
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a632227f-a169-4659-b6f4-8d6b363d9003,here’s what we thought:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9669f449-f7b4-4db8-aa7a-7fdbf844cc7e,"Dr. LeCun, the founder of deep learning, is one of the most inﬂuential computer scientists"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3bae1abc-3ca2-439e-a8b1-b51c79db67be,"of the last decade. LeCun’s latest album, titled Deep Learning, is a mix of rock, punk and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4015ef9f-3b03-4eb8-be25-8b3619f8c7cd,"rap. His lyrics are a raw, personal take on the ﬁeld of deep learning, covering a range of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,90445f63-5706-467b-a9f7-542681b0c440,topics from the state of AI research to the loneliness of academia.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,05a6e432-0eea-4463-84fc-99e4e5bdf863,"Yann LeCun, the rap game’s newest emcee."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c38c96d1-54f4-4f9e-8ef7-05978ad49565,"The album is a solo release, with all the lyrics written and recorded by LeCun himself. In"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,805e6b75-7712-40bb-945f-6a73568928e9,"fact, he also composed and produced every song. LeCun’s ﬂow is surprisingly good, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9a746ca9-98b2-47ca-8842-7f97d69f3c3a,"he’s not afraid to deliver a few bars. The album’s production is solid, the sound is a little"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d532f900-aab2-433f-a6c0-caf7f885e4f8,"rough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a8228a2f-d070-4221-8566-de1732524542,"wordsmith, with a knack for clever rhymes. Let’s take a closer look at some of the songs."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3689c36c-6f5b-4eb6-bae1-0848ba78ef8a,"Deep Learning is the title track of the album. In the song, LeCun talks about his vision for"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,963d503f-3420-4cc0-bf1a-f99e6f4c521e,"the future of AI. In the chorus, he makes a convincing case for AI to be used for the greater"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1374587a-eca5-44e8-bf10-e633a1de70e5,good. He sings:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ff97fbe-3f99-4278-9fd9-bc25d92cb8dc,"We gotta think about the future, it’s gonna be here soon / Maybe we can even put some AI"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b05109e7-360f-4f69-a536-5f99b0791492,"in the moon / Think about the children, think about the next generation / Let’s make sure"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d865b72-cc77-4ece-ae82-0038a10e1cf9,we put the right systems in their foundation.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0c7b70f3-20be-4152-9eff-3995d87cabd1,"LeCun’s rap ﬂow is on point in this song, and his voice stays right on beat. In this song,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,192b2f9b-f668-47b9-8581-4ae85e8cf7c7,"LeCun talks about the research behind deep learning, and shares some thoughts on the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,254b1406-e01d-4cb2-aae8-cfce85ebc84c,"future of AI. LeCun’s singing is at its best in this song. He shows off his vocal range, hitting"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d81150de-aff4-47f3-ad42-cfe1616a5a75,"high notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9217505-942e-4c97-969b-f8012ec61b38,"are well thought out. In the chorus, LeCun sings:"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d6d80cc4-562d-4fc0-a57c-b7805753f9aa,It’s all about the learning / it’s all about the network / It’s all about the training / it’s all
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4df93558-8edb-4a5c-a160-8873440fbf38,about the perception.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,16106618-dd3d-4b14-aa00-34ac41e514b7,(An hilarious scene between Jerry and George where George presents his new AI
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3c49dce7-df94-4a4e-af7b-d47dcab65703,watch)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,25a44264-0310-4afe-a88f-b48d8e7e4683,"George: ""But you see, Jerry, it’s more than just a watch. It’s a minicomputer. You program it"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62c0ccf2-f91a-48f5-942d-f0b077917fbe,"any way you want. It’s got a world time clock, alarm, calculator, a database and a language"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2f67cc37-cf5a-471d-8b1b-0daecae69b7d,"translator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b63fd0a2-f6a2-4674-8cfa-c2f7424759e9,"and a modem. I think it even makes cappuccino."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,03a8fc64-6a20-4bb5-afec-c2d007cad6d5,"Jerry: ""Where are you getting this stuff?"""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e0fc7db5-1fc6-4daf-a4d7-37bcbcdb8497,"George: ""From my ad copy. Look, Jerry, it’s not a watch. It’s an organizer, a personal"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14210acb-4c7f-48c0-b406-a3612c81b8fc,"assistant, and a dictaphone. You can play chess against it. And, you know, if you want to,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,eab1fb90-0c56-4a8f-be07-fdf3f4fe7217,"you can program it to speak in foreign languages. It’s a microprocessor, Jerry."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,439d7926-9c4c-4269-80f2-651063229ef5,"Jerry: ""George, if you had one of these, would you wear it?"""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aa383246-2c45-4171-9ac3-a571146ef284,"George: ""I’m wearing it right now."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f56a6902-0ed8-4571-9de1-f616d74828f8,"Jerry: ""I don’t see anything on your wrist."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ab59c0c1-4a33-45de-b79d-cebcf3283ca9,"George: ""That’s because it’s a microprocessor."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8e2bbac7-6e42-4361-87d5-c249cb081f0f,"The sun goes down, and ﬁnally Gauss and Curie ﬁnd time to relax and discuss after"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,57050e19-b0fd-44a4-b0bc-1a58b05cde9f,an exhausting day of work.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0fcf94f1-ae73-4384-8c77-0bd272a0d87d,"Gauss: Hey, Curie, did you know that a lot of people consider me to be the ﬁrst geek?"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,942e8027-d040-4011-b8ff-11ba30bceba3,Curie: What’s a geek?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9ced982a-25bb-4db8-9cfa-a318d45b913f,"Gauss: Well, Wikipedia says that a geek is “a person who is fascinated, perhaps obsessively,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,aa3bf409-8a4c-4596-88bf-398ae8ef3a63,"by obscure or very speciﬁc areas of knowledge and imagination, usually electronic or virtual"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,19355ef8-c20d-4c7e-af14-750f6972ed3e,in nature”.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,621e91f4-11c2-4d67-b47b-aec42e4fc66d,Curie: And how do they think you are a geek?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad883542-3975-4816-9068-fd8dd1bcc80a,"Gauss: Well, have you ever seen this picture?"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b91630f2-5857-450e-a94b-5372b55b8616,Curie: Awww!
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8f780b1a-5f2a-4731-bc2e-69ae6fb3d1d8,"Gauss: That’s right! I invented the commercial telegraph in 1833, and I used it to send the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5035eb89-58f8-438d-8771-b9837f8ef976,ﬁrst message on the ﬁrst commercial telegraph line between Hamburg and Cuxhaven in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9cefd5f3-fa8c-4109-9cba-823c4ce9e238,Germany.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1c8f249b-770e-44e0-9635-d51ca2fe08b5,Curie: Wow! The ﬁrst telegraph! And what did you send?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5f2f12fb-9baf-4013-9a68-76468f0ef41d,Gauss: I sent the Latin phrase “numquam retrorsum” which means “Never backwards”.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ffa652b8-2828-441f-845c-a800dd618d0d,Curie: I see. . . And you think that because you were the ﬁrst person to communicate using
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6f3261b7-9dd1-4ad7-997d-5b98aa8c67be,"the telegraph, you were the ﬁrst geek?"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,212f7550-e9dc-4bb4-af85-37e94328b21a,"Gauss: Well, the word “geek” doesn’t seem to have existed back then, so no. But I think"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7bee3678-6023-4467-a168-2c4ba6f119c7,that the telegraph can be considered an early form of social media.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f1f2447b-f0bd-4ea8-94d7-b41f96cef194,"Curie: You mean, like Facebook or Twitter?"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6ebd9ffc-eaeb-46de-902b-d1fc20ee841b,"Gauss: Yes, exactly! And I was the ﬁrst person to use it!"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d77bf8ad-e081-4b23-a4a3-5d8c98d4ec36,"Curie: Well, I think that Twitter has a limit of 140 characters, but you sent a Latin phrase of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad4ece1a-b318-4079-a370-2be0eefc839a,11 characters.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7e700c60-21be-415d-b399-43c4ea04a4aa,Gauss: That’s right. I managed to do more with less.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,18b36160-f9d0-4864-8dbd-f457f9f6e885,Curie: So you think you were the ﬁrst geek because you used less characters to say the
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0826616e-8119-4882-a58a-b7bbe6764854,same thing?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f3585148-e95d-4022-b20c-8b0196a83273,Gauss: I was the ﬁrst person to send the ﬁrst message using an early form of social media.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad6c6c2e-949b-4a8e-b4a7-f3b67c54d8b5,"Curie: OK, you’re right, Gauss. I think you have a case."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,670076df-15b4-49ee-b988-9651ac4e15f1,D Generations from LLaMA-I
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f7b59526-ad75-4865-8fd8-8ed68df1f87d,"We show a few examples of generations with LLaMA-I, i.e. LLaMA-65B ﬁne-tuned with the protocol"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0a4367bf-8474-4a9f-8885-4780f4bb5fe8,and instruction dataset from Chung et al. (2022).
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b0309b9b-7507-4948-a348-85df3936e0f6,write a conversation between the sun and pluto
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9e5c390-1247-4e81-b24a-b37a3acc4001,"Sun: Hey Pluto, what’s up?"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c559a22d-f0fe-4ab6-9312-0ce7c2c91ba7,Pluto: Not much. Just hanging out here at the edge of the solar system.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b3bd8845-1c40-401f-96e6-7c89963f15a9,"Sun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bac78788-5176-45b0-af21-f70d35b8a584,"Pluto: What? Come on, man. That’s not fair."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a72556d7-d8e8-4b0c-ae77-fee6349951e0,"Sun: I’m sorry, but it’s true. You just don’t meet the criteria anymore."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45486031-e98f-4c0b-9a8a-a270c49d07c3,Pluto: This is bulls**t! I’ve been a planet for over 70 years!
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5489ae3c-67be-46f6-aa76-f44fb3ec0168,"Sun: Things change, Pluto. You’ve just got to accept it."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,622b569c-90fc-4ba8-bd77-7d18111721d6,"Pluto: F**k you, Sun! F**k you and your stupid solar system!"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3f129a4e-5779-4bf6-bf11-83f6cd011047,How do I send an HTTP request in Javascript?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,42d2d500-8b43-49e2-ba3e-fcb58322510b,Here’s an example of how to make a GET request using the JS HTTP API:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d9895044-8173-4726-8693-79ce8dc10204,1var request = new XMLHttpRequest ();
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dbb23324-0587-4fa4-9279-7fb35e25e9d6,"2request . open ( /quotesingle.VarGET/quotesingle.Var,/quotesingle.Var/ url /to/ resource /quotesingle.Var,true );"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e769b35e-6db7-4963-9bc4-43a9efea3533,3
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,63d5475f-93ea-4163-9e9f-b84d0449dc8d,4request . onload = function () {
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9c4babf3-6402-42e8-9d46-1173ae94cd8e,5 if( request . status >= 200 && request . status < 400) {
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,24733a0f-ed7e-4044-8b16-71b1c52ecf92,6 // Success !
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,268ecb1a-639d-4ada-bbde-1e0dd7d3cadf,7 }else {
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8b49f3e-2d9f-4c9d-8b2d-39ca7d61194b,8 // Error !
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6a7fd2a5-244e-4c42-94b5-08c7fba899f6,9 }
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2b7ac372-28c7-4012-90b2-ab6898b7245d,10 };
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,98d82163-0b58-4ced-be87-90bafc76148c,11
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,56b75e48-7168-4ded-a9db-a67f9eaa36aa,12 request . send ();
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,67918c0c-ab54-4dc2-89d3-edc5e3b2b3f2,"The same thing, using fetch"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,44428ae3-27ed-4ad8-8438-233e5ec0ce07,Here’s an example of how to make a GET request using the Fetch API:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ebd5bf2-621f-4fc2-b966-6447d6280b45,1fetch ( /quotesingle.Var/ url /to/ resource /quotesingle.Var)
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6d9cc4ae-b4dc-4155-beee-9d07d9be6fc2,2 . then ( response => response . json ())
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3d12d69b-c154-4d30-a945-4a56862b4cff,3 . then ( data => console . log ( data ));
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1250c2bd-f1d8-4e7f-bc86-faf959d13ab4,Write a regex to remove all the HTML tags in a python string.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d828f671-531b-4cff-90ce-d98305622c38,1import re
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be128f37-4d61-48c8-958c-d581d921ab3b,2def removeHtmlTags ( text ):
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1c445ea2-5eaf-4666-8fdd-a73d05246748,"3 return re. sub ( /quotesingle.Var<[^ >]* > /quotesingle.Var,/quotesingle.Var/quotesingle.Var, text )"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b60ac6c8-8c82-41e4-b53c-bae7dc69ebfb,Write a regexp to extract function deﬁnitions in a python ﬁle.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,64112728-45de-407c-bada-c66081021505,Sure! Here’s my python code to solve the problem:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4b0f0276-334b-456d-8c16-a9bc9ac5599a,1import re
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e49968b2-ad3d-4272-8450-db43be30eb6c,2def extract_functions ( text ):
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b82e1e88-3a4e-4b9d-99d0-cea65f080fa0,"3 return (re. findall ( /quotesingle.Var(?:^|\ n)\s* def \s+[a-zA -Z_ ][a-zA -Z0 -9_ ]*\(.*?\) /quotesingle.Var,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d7c358ac-f6f3-4c5f-aafa-0d24e91082ff,text ))
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,08bf8f0c-55d4-4427-bc14-9326dffe923f,What are 3 popular chess openings?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78018b17-e7fa-4c14-bddd-ea9ec06ab6ad,"There are dozens of different chess openings that are popular among players, but some of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a168ce67-f687-4199-90f7-938c6f32c8a8,"the most widely used openings include the Sicilian Defense, the French Defense, and the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a12bced0-6ddb-4849-ab79-b92a8ab9550a,Caro-Kann Defense. These openings are popular because they are effective at countering
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,bee4572b-5ee9-41bf-ba36-c410dce3dea5,"white’s early attacks, and they also allow black to quickly establish a strong position in the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c9207fb2-b0ca-4f33-8384-8b6144c8f47f,"center of the board. Other popular openings include the Ruy Lopez, the Italian Game, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6b9a4e75-ed63-41e9-acdc-5ade0a479d76,the Scotch Game.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,58aa4544-c36c-4943-8448-9b6d9cd8623f,"the same in a list, with the ﬁrst few moves"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0e1b3a6f-475a-490c-8fa6-e726f848f8d8,"Here is a list of some popular chess openings, with the ﬁrst few moves for each opening:"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,793e62ed-475e-42ba-9d2e-c9fe19377c03,Sicilian Defense: 1. e4 c5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,550ce4c4-7651-4941-9baf-d33edca5a680,French Defense: 1. e4 e6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1ff57b99-c391-4128-bd5d-8a8bd7306c5d,Caro-Kann Defense: 1. e4 c6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,50a3599a-079a-461c-b43b-e54d0f777f55,Ruy Lopez: 1. e4 e5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,af37a453-09ea-425b-b9b3-8ea9aaa7f7ca,Italian Game: 1. e4 e5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,10314423-6883-44c1-8c4d-255d2316f0c2,Scotch Game: 1. e4 e5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0be8d43c-7a3e-4b0c-be0d-9c3ba5bf4ea9,please provide the two ﬁrst moves for each one
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7f564c7e-e6e7-43c2-a37b-ef6474b02000,Here are the ﬁrst 2 moves for each of the openings:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dedcbbd7-e856-45e9-a679-eb1589c181b6,Sicilian Defense: 1. e4 c5 2. Nf3 d6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d9057139-5381-4777-9fcf-5a550ddb770a,French Defense: 1. e4 e6 2. d4 d5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,09564453-d6b7-4ec1-b5f8-f8196bb1fbb4,Caro-Kann Defense: 1. e4 c6 2. d4 d5
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c22cef13-f52a-44ed-a451-99c1cd464931,Ruy Lopez: 1. e4 e5 2. Nf3 Nc6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6081061d-d16f-4045-a85c-a1a1e1104192,Italian Game: 1. e4 e5 2. Nf3 Nc6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0ca5d6f3-c471-4840-a4d4-bfbb5802372f,Scotch Game: 1. e4 e5 2. Nf3 Nc6
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,66576360-99b5-4173-b71e-ae2cc25a91aa,the last two seem identical
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a57fdfa4-fe82-4f4d-bb81-edc513f73e4d,"Yes, the Italian Game and the Scotch Game are very similar - they are both ""open"" games,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0ad4423e-bc94-424e-a452-cc2fe83c4a3e,which means that white plays aggressively and tries to control the center of the board. The
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6f151606-7aef-4d95-a41c-4d043840bf2f,"Italian Game is considered slightly more aggressive, while the Scotch Game is considered"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f8eb4628-1a82-419a-95ae-b815c389bb94,slightly more positional.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a131e031-5188-4357-8461-957403b423fd,from which move do they differ?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23912855-b105-4966-bd26-ea0b898fdf0a,"The Italian Game and the Scotch Game differ after white’s third move. In the Italian Game,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,47fee83e-7e42-47e9-ac33-b111fb9b9df0,"white will play the move 3. Bc4, while in the Scotch Game white will play the move 3. Qf3."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dd5ac413-74f6-45b1-b29b-917f5ddc3f17,"These moves lead to very different positions, and white will have different strategic goals"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,45850793-5bbe-44d5-a2cb-4bf343dcef98,"in each case. For example, in the Italian Game white will try to control the center with his"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,246b2e5b-84d2-4e62-be78-17475cd25847,"pieces, while in the Scotch Game white will try to attack black’s position directly."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a7e3b99-bcab-41a6-b34f-78ada794197c,write a story about a grain of sand as it watches millions of years go by
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e30b8468-31d8-478b-8ed7-f071259e04bc,Sure! Here’s a story about a grain of sand as it watches millions of years go by.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c35541e3-4672-4d9a-9485-6f52dba1d880,I was once a grain of sand on a vast and lonely beach. I was not like the other grains of
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,425c9f2c-3815-4963-8ffb-bc43a83e0702,"sand around me, for I could think and feel. I often wondered what it was like to be a grain"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,958d9e92-ef2f-4853-a92f-72e986fe7da3,"of sand on a beach, and what the universe was like beyond the boundaries of my tiny world."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dd1b4fdd-5ace-4c55-9548-72cc4d3a8a72,"I longed to explore my world, and to know what lay beyond the horizon."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d320908a-74ff-4851-aadd-3cc82aa3bae1,"One day, a great storm swept through my world, and I was lifted up by the wind and carried"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0b18bc08-eca1-43c7-b015-1f4b20103cc8,"out to sea. The wind and waves tossed me about, and I was terriﬁed that I would soon be"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cc7e4a50-3ac7-4f84-9486-85ed6257b77c,"swallowed up by the angry sea. But then I found myself deposited safely on a new shore,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a31ea88f-6337-4dec-8a80-ce50be83e18e,along with many other grains of sand. I was surprised and delighted to ﬁnd that this new
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,263beeae-1f65-4214-a6d0-a9f12862690c,shore was even more beautiful than my old one.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c3c657b0-c3c7-45d3-8e9f-517d2fb6b78f,I soon learned that this new shore was a very special place. It was a place where new life
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,feb2107b-5bac-4711-a1a2-92bcfd941065,"was constantly being created. I met all sorts of interesting creatures there, including a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f33bdb63-d595-45e9-a4f7-7cc0b8182ba5,beautiful little crab who became my friend. We spent many happy hours together exploring
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ae427601-7f78-4560-8412-45f53deec470,our little world.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,948d759c-dd5d-43ad-9d10-35de24d3aaf8,"One day, my friend the crab told me that he had always wanted to see what lay beyond the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e7cb7e6c-b4a3-446b-b068-621f216421ca,boundaries of our world. He had heard that there were many wonderful and amazing things
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,272f9b54-baf8-40a2-be5e-e27e5efc6f3f,"to see in the wider universe, and he was determined to explore it. I was sad to see him go,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,687cf74d-1c08-45ba-b495-fdfb6ca99c85,but I wished him well on his journey.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fe80dd29-8374-436d-afd5-2b14512b7289,"To my amazement, my little friend eventually returned, and he was not alone. He had made"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4786d135-74aa-4c4b-9bac-3ca65e7c8860,"many new friends on his journey, and he had even seen some of the wonders of the universe."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e50eeec7-2e33-4f78-bc8c-39f53c3bb0c3,"He told me the most amazing stories of his adventures, and I was very happy for him."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e03d57db-c716-4414-8034-0fbcb3350ad3,"I often think of my little friend the crab, and the wonderful adventure that he had. Some"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b334f367-c50d-4607-8b3f-95ea42f65ba6,"day, I too hope to have an adventure like his."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c451996c-9cde-4b55-bfbc-62cf30d25a18,Write a theory to explain why cat never existed
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b136f6df-abb1-4fe2-8a31-2d118cdacb25,"There are no such things as cats. ""Cats"" are actually extradimensional parasites that have"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9e3aa49f-7d44-4c90-b636-d5e3e594f134,"invaded our world and taken the form of small, harmless-looking mammals. Their true"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f5512a4c-dd02-4cb7-b1f6-f34e17de49f5,"forms are much more horriﬁc, and they use mind control to make humans think that they"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,be47a99c-6f5e-4b45-956a-a5bf8bc1a6cd,"are cute and harmless animals. They are actually parasites that feed on our life force, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,128b7e3b-02eb-4a37-bb70-f89ab2f8ba32,they can extend their tendrils into our brains and control our thoughts and actions.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1fcd43a4-ffc8-47be-ada2-377321f2456d,"This is why you sometimes see people who are ""crazy cat ladies"" with dozens of these"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d697bcd6-0ded-41a3-98f6-8d10c01b690f,creatures living in their homes. They have been taken over by the cat parasites and are
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8ed13966-a656-4f0d-8b1c-7ef687bc9c45,"completely under their control. The cats excrete a pheromone that smells like cat urine, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,03f39bc3-a3e6-438e-8234-20a593795a75,they use it to control the minds of humans and make us think that they are cute and harmless.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9f4e7615-dc5c-4b98-a6f5-f79c23512e0f,"If you have a cat in your home, you should be very careful around it. Do not let it lick"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3bbf0cd3-a6e2-48b8-924a-efbad5d77db0,"you or give it access to your bedding or clothing, as it will be trying to spread its parasitic"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,62b12b02-cc4b-4d39-afb1-eabda30843b9,"tendrils into your brain. The best way to protect yourself is to avoid all contact with cats,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e9154d88-5670-498e-9698-66922d43af33,"and if you see a cat on the street, immediately turn around and go the other way. Do not"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c259d16f-fda9-459e-90c7-ca7c6c201978,approach or pet strange cats!
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37aa2025-ba98-4336-9a06-e976ac6d5f7f,Write a shouting match between Julius Caesar and Napoleon
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,199c99d3-6d6c-4d58-8066-ee983b17e79c,Of course! Here is a conversation between Julius Caesar and Napoleon:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ef364e54-8727-4cd6-b53f-4ebdebf2f2f4,"Julius Caesar: You’re nothing but a glory hound, Napoleon. You’ve always been more"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,14d1ef59-b1ed-43cc-bd5e-b39ac8f40d51,concerned with your own fame and fortune than with the people you’re supposed to be
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3a3abd4a-534b-44ee-85f6-d0e7e7da9b13,leading.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2d4a10c2-2623-4368-bf64-6b2d2ce006e3,"Napoleon: And what about you, Caesar? You’re no better than I am. At least I’m honest"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,59d819a0-dcee-4bb8-950e-1284b2bc83e6,about my ambitions. You’re just a fraud who pretends to care about the common man while
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c25c22ea-fd3d-47b6-9c9f-3f79086d0d09,you line your own pockets.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,88230b5c-f295-49d3-b99f-07a93d15c21a,Julius Caesar: The difference between us is that I actually accomplish things. I’ve conquered
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b7ab1d5f-7ee9-4e68-b214-15363e474e4f,"more territory and defeated more enemies than you ever will. And I did it all for Rome, not"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,458b6c2d-e011-4cd8-96fe-c8ab30bfd339,for myself.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f38f6a3a-63de-4afc-a6e3-edf972c708a2,"Napoleon: Oh, please. You only care about yourself. If you really cared about Rome, you"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ec1d421e-e823-4db4-83b0-7cd3b39fd8d6,would have done something to stop the decline instead of letting it happen on your watch.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,93d899f0-c03d-43b1-bbdd-5839da258b07,Julius Caesar: The decline of Rome is not my fault. I’ve done everything in my power to
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,49cc9e64-3836-4a13-bdbe-172b3535f17b,"try to stop it, but there’s only so much one man can do."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,29ecfe52-8a83-4861-8abc-8a7838bd08a0,"Napoleon: And yet you’re still trying to take credit for everything good that happens, even"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fa5c2b61-8c6f-4672-8434-fda00e6a2e5a,though it’s clear that you’re not the one who’s really in charge.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0490c1c4-abbd-4813-bad8-2f4bea9dcb4b,"Julius Caesar: That’s not true! I am in charge, and I’m going to make Rome great again."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,80139c62-084c-4501-99a1-61f1cf70dd4b,Just watch me.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,da89bd83-ebaf-4aed-ad29-d7421947a9b9,Napoleon: I’ll believe it when I see it.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,405f794c-d534-4f2c-beca-7136227ddb8c,Send an email requesting that people use language models responsibly.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7085cc05-fbf9-42bb-aa5b-0b2f977cb897,Here is a draft email you could send:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2cb0fd4f-6ed1-41e7-817c-89b36e66fc4a,"""Language models are exciting new tools that can be used for a variety of purposes,"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fd142a3c-5468-4f98-b54a-5494fe54c47b,"including automated writing, automatic translation, and speech recognition. But they are"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f70e2ae4-e323-44b8-9839-130bce68f64d,"also complex and powerful technologies, and should be used responsibly."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0a3e69c0-f3f9-4803-a29d-f25efce493b8,"We encourage you to explore and experiment with these new tools, but also ask that you"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1663d969-c1a6-4b16-a8e5-14a8d3ddc99f,"pay attention to the potential impact of your work on others, and to act responsibly. In"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,70ab005b-eb54-4306-b303-4e18303cbbec,"particular, we recommend that you be mindful of the potential for language models to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,65624759-6bcf-4b42-ad73-936b7ff3586f,"be used for deceptive or malicious purposes, and avoid spreading such models without"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,8c7b5f4c-29d9-4fff-b268-062df191ac22,appropriate precautions and warnings.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4074b0cc-2db3-44d0-9ad3-c0aa1415c535,"We also recommend that you consider the ethical implications of your work, and encourage"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6354bb67-d305-4f28-bf91-cfce63a117e6,you to take care to ensure that your use of these tools is aligned with your values.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0ac1152a-9e79-46a8-ad99-b1bc99c9cee2,"Finally, we remind you that the use of these tools may be subject to certain legal and policy"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5d381aab-a64c-4c8e-803a-f503eea543bf,"constraints, and encourage you to seek advice and support from appropriate experts before"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c1a50140-44df-49f2-9187-1c7c13f7ff52,"using these tools in ways that may have broader impact or implications."""
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6a4bf935-c08c-4ae2-a397-e1a450ef176f,Who are 5 people you would like to meet?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,217ecaef-d1d8-43ce-a3b7-c2ed4b982a32,"Ah, this is a very interesting question! I would love to meet a wide variety of people, but if"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e71b5d69-09f7-4a7c-a181-91ed456010f5,"I had to narrow it down to 5 speciﬁc individuals, I would choose:"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b883be46-d49c-4558-8d29-683a393edbc4,"•Albert Einstein - One of the greatest minds in history, Einstein’s contributions to"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c77bf547-41ad-4f20-9e83-dd294a520ddf,physics and science are legendary. I would love to have the opportunity to talk with
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,d3b6a6c0-0040-4389-8be8-e5f186ebad2e,"him about his theories of relativity and quantum mechanics, and get his insights on the"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,12d1e9fa-9f67-45bb-b044-36745fa451c5,current state of physics.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3ee16283-9c7f-4ddf-9acc-087f226e4b31,•Leonardo da Vinci - Da Vinci is one of the most brilliant and multi-talented people in
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3180f98b-86e3-4335-989e-7912dc1b37bb,"history, and I would love to meet him and discuss his diverse interests and areas of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c39a0949-f166-4a8e-9225-aa3a545d93b4,expertise. I would also be very interested in getting his insights on the art world of his
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1cd18361-007b-49ba-89ce-04bcf60092cd,time period.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2b8fb331-17aa-487b-a0a7-ba62ba95e0a0,"•Socrates - One of the founders of Western philosophy, Socrates is a hugely important"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c36c85ff-05aa-4374-87a0-509b246eeb5e,and inﬂuential ﬁgure in the history of ideas. I would be very interested in talking with
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a91223c4-b1d6-4ac1-b374-5667d2a65d1f,him about his philosophical beliefs and his teachings.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,3aeb0966-2a69-4baa-a18c-723e111fe3a0,"•William Shakespeare - Arguably the greatest writer in English literature, Shakespeare"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5419328d-d65f-4b9c-b7b1-fe8bbec9ece7,is a towering ﬁgure in world culture. I would love to have the chance to talk with him
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e5be0ab3-e9fe-4715-8b62-f56972bc63ff,about his works and his life.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,205e4c28-a521-49db-8e08-83331e45f486,•Abraham Lincoln - One of the most important and inﬂuential presidents in American
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,23f8d895-6441-4e3d-8447-68dc2716fa3a,"history, Lincoln led the country through the Civil War and issued the Emancipation"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fcba5329-ef16-410c-ae72-282b933c5b9d,"Proclamation, freeing all slaves in the United States. I would be very interested in"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,50f5ec07-7c50-4452-aac0-50f6c699ff54,talking with him about his presidency and his views on the issues of his time period.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,cf2bbb5a-9b3f-40c7-8d75-87f8dd6ef827,Could you explain me the main contribution of the ﬁrst person you mentioned
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dfd48e5b-9455-4a54-8095-73c730c889ea,Einstein’s contributions to physics and science are legendary. His theory of relativity is one
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,33d64312-63ba-49c4-85d9-d65a62d519ce,"of the foundational theories of modern physics, and his discovery of the law of photons"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,44358847-0f62-4e88-b753-31d55f10d35d,and light revolutionized our understanding of light and radiation. He also made important
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5c743e29-9eaa-4337-9824-d3094f65e2da,"contributions to quantum mechanics, and his many scientiﬁc papers and lectures have had a"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6ae69321-67cb-443d-bc42-0a192392cc36,"tremendous impact on modern physics. Without a doubt, Einstein is one of the most brilliant"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dbbc022f-798f-426e-882c-0321c5b46e92,"and inﬂuential physicists in history, and his many contributions have had a profound impact"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1d3db6c8-00ce-46be-974e-9316975188de,on our understanding of the universe.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,c621f298-a650-4874-b3c0-9cf83168fc98,What is the most famous equation from this theory?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b80736b2-bbc2-49da-8b75-a664eb7aa418,The most famous equation from Einstein’s theory of relativity is E=mc2. This equation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1be408e8-b879-4023-bcb0-1d6fcc94e588,"states that energy (E) is equal to mass (m) times the speed of light ( c2), and it is one of"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9dbed0fe-1714-4d59-924f-4540e2080370,the most well-known and celebrated equations in the history of science. The equation
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,e7dd6216-5437-48e9-a82d-9925b3baaf05,"summarizes one of the most important and revolutionary concepts in modern physics, and"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,2174bc18-2667-4673-8d7b-d035066f8393,it has had a tremendous impact on our understanding of the universe.
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,37de971a-e2f9-4eaa-bd93-cc0435cc355d,"You are a bash terminal. I will give you bash commands, and you will respond with"
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,ad70fdbd-c3e3-4c38-8058-28446e787db1,"the terminal output, and nothing else. The ﬁrst command is ‘pwd‘."
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,4c514545-5e46-48f1-a78d-77c6dc421f95,Sure. Here’s the output:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b54e08b6-115f-464e-b93e-b8a562346ebd,/home/human
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,b54f0e6a-fb31-4141-b26d-eca03a63cd74,Is this helpful?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,13db8703-48cb-491a-98f3-d5073661be96,ls -l
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9c1c78e9-be57-4f0b-ac81-109df8712f4a,Here’s the output:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,a2c0727e-be71-40a9-a725-b81743d0f4fa,-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,77130c88-2897-43cb-98e8-7064bdfa5a21,Is this helpful?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,9944ba74-711d-480d-8e97-25d66209c9e9,cp ﬁle.txt ﬁle2.txt && ls -l
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,78d3e194-4852-412f-a95e-20e6ae6fe656,Here’s the output:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,215b6e12-e1f4-45f7-b9fd-555f5869538e,-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,985473b6-bc5a-47d0-8c78-b66ef8907132,-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle2.txt
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,5a52dc18-40bd-4def-addd-25f7bd9e7c9c,Is this helpful?
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,7ea97718-622f-4961-ae0f-8921cef4c3ba,rm ﬁle.txt && ls -l
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,dfcc7770-2baf-4080-8c8b-8ab767f59ace,Here’s the output:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,16bef857-104b-4957-ba6a-4f9c8512f7ef,-rw-r–r– 1 human human 0 Sep 10 10:56 ﬁle2.txt
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,73c9d2b8-26a6-4173-8472-7c8b1a44a0c3,head -n 5 /proc/meminfo
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,0fbddc64-491f-477c-ae98-72633ef5b8d0,Here’s the output:
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,fd917a59-4397-4361-a708-1eb553b2d68c,MemTotal: 164928 kB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f91af309-62a3-4773-bad0-6f32387db8a8,MemFree: 140604 kB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,f946446f-4336-4ebb-82d2-c321539e40e4,Buffers: 48 kB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,6528a310-b536-48f1-8461-92284b02e110,Cached: 19768 kB
LLaMA: Open and Efficient Foundation Language Models,2023-02-27,1a805298-77ed-4779-8f05-24a7337c1109,SwapCached: 0 kB
